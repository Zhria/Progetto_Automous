{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd2DHK4QYaKH"
      },
      "source": [
        "This module contains the implementation of the PPO algorithm.\n",
        "Ci basiamo sullo pseudocodice presente sul sito di OpenAI per la realizzazione del ppo.\n",
        "https://spinningup.openai.com/en/latest/algorithms/ppo.html#id7\n",
        "Utilizzando un Actor-Critic Method.\n",
        "Ciò suddivide l'implementazione in 8 passi principali:\n",
        "1. Inizializzazione dell'ambiente con policy parameters theta_0, e l'inizial value function parameters w_0.\n",
        "2. Ciclare per k iterazioni\n",
        "3. Raccogliere un set di traiettorie D_k = {τ_i} con una policy pi_k = pi(theta_k)\n",
        "4. Calcolare i reward-to-go R_t\n",
        "5. Calcolare gli advantage estimates A_t basandoci sulla value function V_{w_k}\n",
        "6. Aggiornare la policy massimizzando la PPO-Clip objective (Gradient ascent con adam) . Non scriverò la formula che è complessa\n",
        "7. Aggiornare la value function minimizzando la MSE tra V_{w_k} e R_t (Gradient descent con adam)\n",
        "8. Fine ciclo.\n",
        "\n",
        "Implementiamo tutti i passi nella funzione learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqY5vJI-YaKI",
        "outputId": "1424a1ab-43fe-4315-8154-1bf6372c7ac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: procgen in /usr/local/lib/python3.10/dist-packages (0.10.7)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from procgen) (1.26.4)\n",
            "Requirement already satisfied: gym<1.0.0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from procgen) (0.25.2)\n",
            "Requirement already satisfied: gym3<1.0.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from procgen) (0.3.3)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from procgen) (3.16.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (0.0.8)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.17.1)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (2.36.1)\n",
            "Requirement already satisfied: imageio-ffmpeg<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (0.3.0)\n",
            "Requirement already satisfied: glfw<2.0.0,>=1.8.6 in /usr/local/lib/python3.10/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.12.0)\n",
            "Requirement already satisfied: moderngl<6.0.0,>=5.5.4 in /usr/local/lib/python3.10/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (5.12.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.13.0->gym3<1.0.0,>=0.3.3->procgen) (2.22)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0.0,>=2.6.0->gym3<1.0.0,>=0.3.3->procgen) (11.0.0)\n",
            "Requirement already satisfied: glcontext>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from moderngl<6.0.0,>=5.5.4->gym3<1.0.0,>=0.3.3->procgen) (3.0.0)\n",
            "Requirement already satisfied: tensorflow_probability in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability) (1.26.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability) (3.1.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability) (0.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore') #ignora warnings\n",
        "#Check if colab is used:\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "  print(\"Not running on CoLab\")\n",
        "if IN_COLAB:\n",
        "  !pip install procgen\n",
        "  !pip install tensorflow_probability\n",
        "  !pip install numpy\n",
        "from rete import ReteNeurale\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import gym\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ohXWluKCYaKJ"
      },
      "outputs": [],
      "source": [
        "class PPO:\n",
        "    def learn(self,env):\n",
        "        #Passo 1 --> Inizializzazione dell'ambiente con policy parameters theta_0, e l'inizial value function parameters w_0.\n",
        "        #Dobbiamo creare una rete neurale per la policy e per la value function.\n",
        "        self.env=env\n",
        "        self.nAzioni=env.action_space.n\n",
        "        self.nStati=env.observation_space.shape\n",
        "        self.listaAzioni=list(range(env.action_space.n))\n",
        "\n",
        "        print(\"N STATI ENV:\",self.nStati)\n",
        "        print(\"N AZIONI ENV:\",self.nAzioni)\n",
        "        #self.stepsPerEpisode=2048 Per produzione\n",
        "        #self.episodesPerBatch=8 per produzione\n",
        "        #self.nEpoche=200 per produzione.\n",
        "        self.stepsPerEpisode=512\n",
        "        self.episodesPerBatch=1\n",
        "        self.nEpoche=10\n",
        "\n",
        "        self.gamma=0.95\n",
        "        self.epsilon=0.2\n",
        "        self.nUpdatesPerIteration=10\n",
        "        self.cov_mat=tf.linalg.diag(tf.fill([self.nAzioni], 0.5))\n",
        "        self.policyNN=ReteNeurale(self.nStati,self.nAzioni,softmax=True) #Actor\n",
        "        self.valueNN=ReteNeurale(self.nStati,1,False) #Critic\n",
        "        self.policy_optimizer=keras.optimizers.Adam(learning_rate=0.0005)\n",
        "        self.value_optimizer=keras.optimizers.Adam(learning_rate=0.0005)\n",
        "        self.policyNN.compile(optimizer=self.policy_optimizer)\n",
        "        self.valueNN.compile(optimizer=self.value_optimizer)\n",
        "        #passo 2 ciclare per k iterazioni.\n",
        "        for k in range(self.nEpoche):\n",
        "            states, actions, rewards, rewards_to_go, log_probs =self.collect_trajectories()\n",
        "            print(\"Trajectories collected\")\n",
        "\n",
        "\n",
        "            num_samples=states.shape\n",
        "            print(\"numSamples_:\",num_samples)\n",
        "            num_samples=num_samples[0]\n",
        "            batch_size=64 #Faccio calcoli con mini-batches perchè altrimenti vado in Run out of memory fisso.\n",
        "            for i in range(0,num_samples, batch_size):\n",
        "              print(\"Batches\")\n",
        "              batch_states=states[i:i+batch_size]\n",
        "              batch_actions=actions[i:i+batch_size]\n",
        "              batch_rewards=rewards[i:i+batch_size]\n",
        "              batch_rewards_to_go=rewards_to_go[i:i+batch_size]\n",
        "              batch_log_probs=log_probs[i:i+batch_size]\n",
        "\n",
        "\n",
        "              V,latest_log_probs=self.evaluate(batch_states,batch_actions)\n",
        "              advantage=self.calcAdvantages(batch_rewards_to_go,V)\n",
        "              print(\"Advantages calculated\")\n",
        "              std_advantages=tf.math.reduce_std(advantage)\n",
        "              mean_advantages=tf.math.reduce_mean(advantage)\n",
        "              print(\"Mean and std of advantages:\",mean_advantages.numpy(),std_advantages.numpy())\n",
        "\n",
        "              with tf.GradientTape(persistent=True) as tape:\n",
        "                  _,latest_log_probs=self.evaluate(batch_states,batch_actions)\n",
        "                  print(\"log_probs is tensor:\", isinstance(batch_log_probs, tf.Tensor))\n",
        "                  print(\"advantage is tensor:\", isinstance(advantage, tf.Tensor))\n",
        "                  print(\"rewards_to_go is tensor:\", isinstance(batch_rewards_to_go, tf.Tensor))\n",
        "                  print(\"V is tensor:\", isinstance(V, tf.Tensor))\n",
        "\n",
        "\n",
        "                  surrogated_loss_1, surrogated_loss_2=self.calcSurrogatedLoss(batch_log_probs,latest_log_probs,advantage)\n",
        "                  policy_loss = -tf.reduce_mean(tf.minimum(surrogated_loss_1, surrogated_loss_2))\n",
        "                  value_loss=tf.reduce_mean(tf.square(batch_rewards_to_go-V)) #MSE tra rewards to go e V\n",
        "                  print(\"Policy Loss:\", policy_loss)\n",
        "                  print(\"Value Loss:\", value_loss)\n",
        "              gradientsPolicy = tape.gradient(policy_loss, self.policyNN.trainable_variables)\n",
        "\n",
        "\n",
        "              # Debug: Controlla se i gradienti sono None\n",
        "              print(\"Gradients for policy:\", gradientsPolicy)\n",
        "\n",
        "              # Verifica che i gradienti non siano None\n",
        "              if gradientsPolicy and all(grad is not None for grad in gradientsPolicy):\n",
        "                  self.policy_optimizer.apply_gradients(zip(gradientsPolicy, self.policyNN.trainable_variables))\n",
        "              else:\n",
        "                  print(\"Policy gradients are None!\")\n",
        "\n",
        "              gradientsValue = tape.gradient(value_loss, self.valueNN.trainable_variables)\n",
        "              print(\"Gradients for value:\", gradientsValue)\n",
        "              # Verifica che i gradienti non siano None\n",
        "              if gradientsValue and all(grad is not None for grad in gradientsValue):\n",
        "                  self.value_optimizer.apply_gradients(zip(gradientsValue, self.valueNN.trainable_variables))\n",
        "              else:\n",
        "                  print(\"Value gradients are None!\")\n",
        "\n",
        "              print(\"EPOCA:\",k,\" POLICY LOSS:\",policy_loss,\" VALUE LOSS:\",value_loss)\n",
        "\n",
        "\n",
        "    def collect_trajectories(self):\n",
        "        #Passo 3 --> Raccogliere un set di traiettorie D_k = {τ_i} con una policy pi_k = pi(theta_k)\n",
        "        #Dobbiamo raccogliere un set di traiettorie e per fare ciò dobbiamo raccogliere: stati, azioni, rewards, rewards to go, log_prob delle azioni.\n",
        "        batch={\n",
        "            'states':[],\n",
        "            'actions':[],\n",
        "            'rewards':[],\n",
        "            'rewards_to_go':[],\n",
        "            'log_probs':[],\n",
        "        }\n",
        "        stato = self.env.reset()\n",
        "        done = False\n",
        "        #Abbiamo un fisso di 8 episodi per batch con 2048 steps per episodio\n",
        "        for i in range(self.episodesPerBatch):\n",
        "            rewardPerEpisode=[]\n",
        "            print(\"episode: \",i)\n",
        "            for j in range(self.stepsPerEpisode):\n",
        "                batch['states'].append(stato)\n",
        "                azione,log_prob=self.getAction(stato)\n",
        "                #azione sarà un int, mentre log_prob sarà il logaritmo della probabilità dell'azione\n",
        "                batch['actions'].append(azione)\n",
        "                batch['log_probs'].append(log_prob)\n",
        "                stato, reward, done, info = self.env.step(azione)\n",
        "                #info non usata.\n",
        "                rewardPerEpisode.append(reward)\n",
        "                #if done:\n",
        "                #    break #Ha raggiunto il termine dell'episodio.\n",
        "            batch['rewards'].append(rewardPerEpisode)\n",
        "        #Calcoliamo i rewards to go --> PASSO 4\n",
        "        batch['rewards_to_go']=self.calcRTG(batch['rewards'])\n",
        "        #return batch states, actions, rewards, rewards to go, log_probs\n",
        "\n",
        "        batch_statiTensor=tf.convert_to_tensor(batch['states'],dtype=tf.uint8)\n",
        "        batch_azioniTensor=tf.convert_to_tensor(batch['actions'],dtype=tf.int32)\n",
        "        batch_rewardsTensor=tf.convert_to_tensor(batch['rewards'],dtype=tf.float32)\n",
        "        batch_rewards_to_goTensor=tf.convert_to_tensor(batch['rewards_to_go'],dtype=tf.float32)\n",
        "        batch_log_probsTensor=tf.convert_to_tensor(batch['log_probs'],dtype=tf.float32)\n",
        "\n",
        "\n",
        "        return batch_statiTensor, batch_azioniTensor,batch_rewardsTensor,batch_rewards_to_goTensor,batch_log_probsTensor\n",
        "\n",
        "    def getAction(self,stato):\n",
        "        stato= np.expand_dims(stato, axis=0)  # Diventa (1, 64, 64, 3)\n",
        "        stato=tf.convert_to_tensor(stato,dtype=tf.float32)\n",
        "        azione_pred=self.policyNN(stato)\n",
        "        #print last column values softmax\n",
        "\n",
        "        #dist=tfp.distributions.Categorical(probs=azione_pred)\n",
        "        # azionePresa=dist.sample()\n",
        "        azionePresa=random.choices(self.listaAzioni, weights=tf.squeeze(azione_pred), k=1)[0]\n",
        "        #dist=tfp.distributions.MultivariateNormalTriL(loc=azione_prob, scale_tril=tf.linalg.cholesky(self.cov_mat))\n",
        "        #azionePresa=dist.sample()\n",
        "        #log_prob=dist.log_prob(azionePresa)\n",
        "\n",
        "        log_prob=tf.math.log(azione_pred[0][azionePresa]+ 1e-10) #Aggiungo un 1e-10 per evitare problemi nel calcolo del gradiente\n",
        "        return tf.squeeze(azionePresa), log_prob\n",
        "\n",
        "    def calcRTG(self,rewards):\n",
        "        print(\"CALC REWARDS TO GO\")\n",
        "        print(rewards)\n",
        "        #Prendo la formula per calcolare i rewards to go e richiede i cumulative rewards e un fattore di sconto.\n",
        "        rtg=[]\n",
        "        for episode_reward in reversed(rewards):\n",
        "            cumulative_reward=0\n",
        "            for single_reward in reversed(episode_reward):\n",
        "                cumulative_reward=single_reward+cumulative_reward*self.gamma\n",
        "                rtg.append(cumulative_reward)\n",
        "        return tf.convert_to_tensor(rtg,dtype=tf.float32)\n",
        "\n",
        "    def calcAdvantages(self, rtg,values):\n",
        "        advantages=rtg-tf.stop_gradient(values)\n",
        "        return (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-10)\n",
        "\n",
        "    def calcSurrogatedLoss(self,log_probs_old, log_probs_new, advantages):\n",
        "        advantages = tf.stop_gradient(advantages)\n",
        "        policy_ratio = tf.exp(log_probs_old - log_probs_new)\n",
        "        surrogated_loss_1 = policy_ratio * advantages\n",
        "        surrogated_loss_2 = tf.clip_by_value(policy_ratio, clip_value_min=1.0-self.epsilon, clip_value_max=1.0+self.epsilon) * advantages\n",
        "        return surrogated_loss_1, surrogated_loss_2\n",
        "\n",
        "    def evaluate(self, batch_states,batch_actions):\n",
        "        batch_states=tf.cast(batch_states, tf.float32)\n",
        "        retVal=self.valueNN(batch_states)\n",
        "        V= tf.squeeze(retVal)\n",
        "        mean=self.policyNN(batch_states)\n",
        "        dist=tfp.distributions.Categorical(probs=mean)\n",
        "        log_probs=dist.log_prob(batch_actions)\n",
        "        return V, log_probs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DUvSNGxTYaKK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MRQw7a59YaKK",
        "outputId": "14b44976-8c3c-4514-9d59-8209e0bab64c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N STATI ENV: (64, 64, 3)\n",
            "N AZIONI ENV: 15\n",
            "episode:  0\n",
            "CALC REWARDS TO GO\n",
            "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "Trajectories collected\n",
            "numSamples_: (512, 64, 64, 3)\n",
            "Batches\n",
            "Advantages calculated\n",
            "Mean and std of advantages: -5.9604645e-08 1.0\n",
            "log_probs is tensor: True\n",
            "advantage is tensor: True\n",
            "rewards_to_go is tensor: True\n",
            "V is tensor: True\n",
            "Policy Loss: tf.Tensor(7.4505806e-08, shape=(), dtype=float32)\n",
            "Value Loss: tf.Tensor(85.39233, shape=(), dtype=float32)\n",
            "Gradients for policy: [<tf.Tensor: shape=(3, 3, 3, 32), dtype=float32, numpy=\n",
            "array([[[[ 1.15641961e-02,  1.97963417e-03, -7.73686857e-04,\n",
            "          -1.39490425e-04,  1.19043296e-04, -9.43670701e-03,\n",
            "           1.55775947e-03,  8.42646579e-04, -8.26478004e-03,\n",
            "          -2.87383649e-04,  4.23167087e-03,  0.00000000e+00,\n",
            "          -1.99542521e-03,  0.00000000e+00,  1.54567161e-03,\n",
            "           0.00000000e+00, -5.05317119e-04, -7.09449733e-03,\n",
            "          -8.62838584e-04,  9.28909052e-04,  6.00836589e-04,\n",
            "           0.00000000e+00, -5.21354331e-03,  0.00000000e+00,\n",
            "          -1.00343290e-03,  1.31113746e-03,  1.44630566e-03,\n",
            "           0.00000000e+00,  6.40159799e-03, -1.73866079e-04,\n",
            "          -1.36973336e-03, -4.04348189e-04],\n",
            "         [ 1.31166512e-02,  2.09514657e-03, -7.92784034e-04,\n",
            "          -7.66479585e-04,  2.52509017e-06, -1.08932219e-02,\n",
            "           1.22720178e-03,  8.37760628e-04, -8.48853588e-03,\n",
            "          -6.31721574e-04,  3.83664714e-03,  0.00000000e+00,\n",
            "          -1.39823765e-03,  0.00000000e+00,  1.79187034e-03,\n",
            "          -1.28957297e-04,  1.22376267e-04, -7.59523921e-03,\n",
            "          -1.51102443e-03,  6.86472689e-04,  5.16195898e-04,\n",
            "           7.64166907e-06, -4.87843622e-03, -2.79778724e-05,\n",
            "          -1.07127230e-03,  1.09058234e-03,  1.35877286e-03,\n",
            "           6.56809134e-04,  6.30142121e-03, -3.86727537e-04,\n",
            "          -1.13327987e-03, -5.33060404e-04],\n",
            "         [ 1.35781951e-02,  2.10000575e-03, -8.40734632e-04,\n",
            "          -1.14228611e-03, -1.08009132e-04, -1.13606816e-02,\n",
            "           9.87725100e-04,  8.18218221e-04, -8.24591611e-03,\n",
            "          -9.28004563e-04,  3.25293280e-03,  0.00000000e+00,\n",
            "          -9.58331511e-04,  0.00000000e+00,  1.79742789e-03,\n",
            "          -2.14928237e-04,  5.47688454e-04, -7.53166340e-03,\n",
            "          -1.94868201e-03,  4.53766639e-04,  4.31524706e-04,\n",
            "           1.27361373e-05, -4.36649518e-03, -4.54074543e-05,\n",
            "          -1.06814096e-03,  9.85479099e-04,  1.32914702e-03,\n",
            "           1.08039880e-03,  6.12925598e-03, -4.33439505e-04,\n",
            "          -9.09052265e-04, -5.27400931e-04]],\n",
            "\n",
            "        [[ 1.14856465e-02,  1.24312867e-03, -8.11678365e-06,\n",
            "          -4.01744153e-04,  1.09035184e-03, -9.66635905e-03,\n",
            "          -2.78277003e-04,  6.98069925e-04, -8.56596790e-03,\n",
            "          -4.20618569e-04,  4.15548170e-03,  0.00000000e+00,\n",
            "          -2.46151886e-03,  0.00000000e+00,  4.06982290e-05,\n",
            "           0.00000000e+00,  5.56565705e-04, -8.42892844e-03,\n",
            "          -1.13867340e-03,  2.01826543e-03,  5.50641620e-04,\n",
            "           0.00000000e+00, -3.67144984e-03,  0.00000000e+00,\n",
            "           1.27427804e-04,  2.92263320e-03,  1.14257960e-03,\n",
            "          -9.24370966e-07,  4.74432949e-03, -1.39676107e-04,\n",
            "          -1.49372371e-03,  5.03380608e-04],\n",
            "         [ 1.31312171e-02,  1.50518550e-03, -5.07808581e-05,\n",
            "          -1.03072182e-03,  8.77236482e-04, -1.13419797e-02,\n",
            "          -6.27007976e-04,  7.25265185e-04, -8.97241849e-03,\n",
            "          -7.89124635e-04,  3.89413908e-03,  0.00000000e+00,\n",
            "          -1.67742255e-03,  0.00000000e+00,  5.90949145e-04,\n",
            "          -1.28957297e-04,  1.09397771e-03, -8.67975689e-03,\n",
            "          -1.46139192e-03,  1.40571210e-03,  5.02359995e-04,\n",
            "           7.64166907e-06, -3.91438790e-03, -2.36179421e-05,\n",
            "          -3.15676181e-04,  2.17329129e-03,  9.90544795e-04,\n",
            "           6.63095096e-04,  4.89487825e-03, -1.49872081e-04,\n",
            "          -1.29493745e-03,  3.79044184e-04],\n",
            "         [ 1.36784827e-02,  1.61660137e-03, -5.72586250e-05,\n",
            "          -1.31130277e-03,  6.36782672e-04, -1.18839871e-02,\n",
            "          -8.13012302e-04,  6.95118448e-04, -8.77605844e-03,\n",
            "          -1.04908366e-03,  3.47571727e-03,  0.00000000e+00,\n",
            "          -1.14644575e-03,  0.00000000e+00,  9.58041346e-04,\n",
            "          -2.14928237e-04,  1.32906064e-03, -8.38301145e-03,\n",
            "          -1.67752197e-03,  8.20409448e-04,  4.49759071e-04,\n",
            "           1.27361373e-05, -3.87094659e-03, -4.03222912e-05,\n",
            "          -6.71575544e-04,  1.74760399e-03,  8.19333538e-04,\n",
            "           1.08808652e-03,  4.83563216e-03, -1.25544408e-04,\n",
            "          -1.14531640e-03,  2.58969143e-04]],\n",
            "\n",
            "        [[ 9.68714058e-03, -2.54475075e-04, -3.92586837e-04,\n",
            "           1.85298501e-04,  4.22494690e-04, -1.01835635e-02,\n",
            "          -7.68336031e-05, -2.75842147e-04, -6.08684449e-03,\n",
            "           1.04429282e-03,  4.65082889e-03,  0.00000000e+00,\n",
            "          -1.81312882e-03,  0.00000000e+00,  1.44468955e-04,\n",
            "           0.00000000e+00, -1.79249851e-04, -6.84855506e-03,\n",
            "          -1.55528891e-03,  1.95534155e-03, -5.62891614e-07,\n",
            "           0.00000000e+00, -2.14211177e-03, -1.65660604e-04,\n",
            "           1.41413533e-03,  2.37036706e-03,  9.25662578e-04,\n",
            "           9.30217095e-04,  3.18199326e-03,  2.94593917e-07,\n",
            "           6.86904299e-04,  1.30580296e-03],\n",
            "         [ 1.13794496e-02,  6.40093203e-05, -7.35845897e-05,\n",
            "          -4.32281813e-04,  3.69508984e-04, -1.16514498e-02,\n",
            "          -4.14614216e-04, -1.17640171e-04, -7.00934511e-03,\n",
            "           3.10899952e-04,  4.29344550e-03,  0.00000000e+00,\n",
            "          -1.13852578e-03,  0.00000000e+00,  6.66741107e-04,\n",
            "          -1.28957283e-04,  5.30978490e-04, -7.47300917e-03,\n",
            "          -1.66653539e-03,  1.52705074e-03,  7.05615676e-05,\n",
            "           7.64166907e-06, -2.66150804e-03, -1.12084294e-04,\n",
            "           8.60122324e-04,  1.79432379e-03,  8.22788454e-04,\n",
            "           1.51472294e-03,  3.36336484e-03,  1.24934477e-05,\n",
            "           6.04182889e-04,  9.54150164e-04],\n",
            "         [ 1.20505858e-02,  3.13967874e-04,  1.61841846e-04,\n",
            "          -7.82247982e-04,  3.22306470e-04, -1.20277246e-02,\n",
            "          -5.79372456e-04,  2.86362374e-05, -7.27014756e-03,\n",
            "          -2.40976515e-04,  3.79100745e-03,  0.00000000e+00,\n",
            "          -6.65999542e-04,  0.00000000e+00,  9.90626635e-04,\n",
            "          -2.14928237e-04,  9.91226989e-04, -7.62234908e-03,\n",
            "          -1.62474078e-03,  1.10126578e-03,  1.17153453e-04,\n",
            "           1.27361373e-05, -2.94004520e-03, -6.04953275e-05,\n",
            "           3.99746321e-04,  1.43223989e-03,  6.76279014e-04,\n",
            "           1.83743355e-03,  3.30934650e-03,  3.20381805e-05,\n",
            "           4.40635864e-04,  6.61000202e-04]]],\n",
            "\n",
            "\n",
            "       [[[ 1.32075632e-02,  5.21805661e-04, -8.62149114e-04,\n",
            "          -3.73100222e-04,  1.14453502e-03, -1.05503984e-02,\n",
            "           9.71611007e-04,  9.16002260e-04, -9.40262340e-03,\n",
            "          -9.49489302e-04,  4.44016466e-03,  0.00000000e+00,\n",
            "          -2.11665034e-03,  0.00000000e+00,  1.07153726e-03,\n",
            "          -4.92396241e-04, -2.74739112e-04, -6.89279987e-03,\n",
            "          -4.07249638e-04,  1.03279145e-03,  6.41595921e-04,\n",
            "           0.00000000e+00, -3.94805055e-03,  0.00000000e+00,\n",
            "          -1.16438244e-03,  9.52872972e-04,  1.05364271e-03,\n",
            "           0.00000000e+00,  5.70030184e-03, -1.24551170e-03,\n",
            "          -1.53769250e-03, -7.75743276e-04],\n",
            "         [ 1.41620543e-02,  1.25799479e-03, -7.72725558e-04,\n",
            "          -4.29558335e-04,  8.14534607e-04, -1.15180537e-02,\n",
            "           7.36105954e-04,  8.82626162e-04, -9.04400460e-03,\n",
            "          -9.58242570e-04,  4.47778311e-03,  0.00000000e+00,\n",
            "          -1.60094223e-03,  0.00000000e+00,  1.34629186e-03,\n",
            "          -5.69217023e-04,  3.02183442e-04, -7.29948143e-03,\n",
            "          -1.12821232e-03,  7.51000480e-04,  5.41831541e-04,\n",
            "           7.64166907e-06, -4.10316605e-03, -2.57513511e-05,\n",
            "          -1.20008015e-03,  9.22600855e-04,  9.94402915e-04,\n",
            "           6.66642678e-04,  5.60710579e-03, -1.10894046e-03,\n",
            "          -1.03540043e-03, -1.05281547e-03],\n",
            "         [ 1.42088402e-02,  1.76327967e-03, -6.76103344e-04,\n",
            "          -4.41823417e-04,  5.68168762e-04, -1.16967028e-02,\n",
            "           6.08141883e-04,  7.83913187e-04, -8.28624051e-03,\n",
            "          -8.91304575e-04,  4.24864376e-03,  0.00000000e+00,\n",
            "          -1.30681368e-03,  0.00000000e+00,  1.38975575e-03,\n",
            "          -6.20431907e-04,  7.04123115e-04, -7.27288518e-03,\n",
            "          -1.57064106e-03,  4.28988860e-04,  4.15202434e-04,\n",
            "           1.27361373e-05, -3.98091041e-03, -4.29189895e-05,\n",
            "          -1.19520177e-03,  8.10659432e-04,  8.46375653e-04,\n",
            "           1.09316944e-03,  5.29010314e-03, -8.59238731e-04,\n",
            "          -6.41616760e-04, -1.14058668e-03]],\n",
            "\n",
            "        [[ 1.22130411e-02, -3.22502456e-04,  2.52868544e-04,\n",
            "          -1.60720816e-03,  1.43827940e-03, -1.08915884e-02,\n",
            "          -2.44440947e-04, -1.98463822e-04, -8.57584644e-03,\n",
            "           3.28876660e-04,  4.51407349e-03,  0.00000000e+00,\n",
            "          -1.50378537e-03,  0.00000000e+00,  1.33720008e-04,\n",
            "           0.00000000e+00,  3.40639876e-04, -8.37013125e-03,\n",
            "          -9.30834562e-04,  1.59052049e-03,  9.41503531e-05,\n",
            "           0.00000000e+00, -3.39983241e-03,  0.00000000e+00,\n",
            "          -3.22644279e-04,  2.48895679e-03,  8.72706238e-04,\n",
            "          -3.49798256e-06,  5.41366497e-03, -8.21509981e-04,\n",
            "          -1.30904280e-03,  5.23929193e-05],\n",
            "         [ 1.34978043e-02,  4.03479789e-04,  2.09409787e-04,\n",
            "          -1.20657880e-03,  1.21494150e-03, -1.20536163e-02,\n",
            "          -5.51234058e-04, -8.37839025e-08, -8.41499772e-03,\n",
            "          -9.63038256e-05,  4.59594699e-03,  0.00000000e+00,\n",
            "          -9.47996159e-04,  0.00000000e+00,  6.31320232e-04,\n",
            "          -1.28957297e-04,  8.91061325e-04, -8.67205579e-03,\n",
            "          -1.26341230e-03,  1.04306836e-03,  1.23083926e-04,\n",
            "           7.64166907e-06, -3.96358641e-03, -2.70264773e-05,\n",
            "          -7.16927869e-04,  1.70881941e-03,  9.86148021e-04,\n",
            "           6.63997140e-04,  5.51878335e-03, -5.97001519e-04,\n",
            "          -1.01167359e-03, -5.47083153e-04],\n",
            "         [ 1.38236359e-02,  9.87518579e-04,  1.32474015e-04,\n",
            "          -8.13785009e-04,  1.02947315e-03, -1.22017302e-02,\n",
            "          -7.12936802e-04,  1.67289298e-04, -7.85352197e-03,\n",
            "          -4.69065038e-04,  4.42961650e-03,  0.00000000e+00,\n",
            "          -5.43018046e-04,  0.00000000e+00,  9.22943349e-04,\n",
            "          -2.14928237e-04,  1.21998892e-03, -8.46652221e-03,\n",
            "          -1.37168285e-03,  4.87710786e-04,  1.46069840e-04,\n",
            "           1.27361373e-05, -4.15331498e-03, -4.45584374e-05,\n",
            "          -1.02418521e-03,  1.07729423e-03,  1.08984916e-03,\n",
            "           1.09440472e-03,  5.44705568e-03, -3.26353271e-04,\n",
            "          -7.93809420e-04, -8.77866230e-04]],\n",
            "\n",
            "        [[ 1.03634167e-02, -1.15372040e-05,  1.18530828e-04,\n",
            "          -1.38874538e-03,  1.40270113e-03, -1.10588688e-02,\n",
            "           3.91462148e-04, -2.05514643e-05, -6.46688603e-03,\n",
            "           9.11555428e-04,  4.31210268e-03,  0.00000000e+00,\n",
            "          -2.02895142e-03,  0.00000000e+00, -5.67543680e-07,\n",
            "          -1.54645517e-04, -1.97228626e-04, -6.60602376e-03,\n",
            "          -1.02672668e-03,  1.31175260e-03, -2.66248503e-06,\n",
            "           0.00000000e+00, -1.08042161e-03, -1.64251280e-04,\n",
            "           8.17100110e-04,  1.36525370e-03,  5.94871875e-04,\n",
            "           7.42707052e-04,  3.63172940e-03, -6.69369008e-04,\n",
            "           3.73493094e-04,  1.78090180e-03],\n",
            "         [ 1.17070572e-02,  1.40143500e-04,  1.38003699e-04,\n",
            "          -8.61330424e-04,  1.11925171e-03, -1.22480961e-02,\n",
            "          -6.79758814e-05,  1.13913316e-04, -6.91694347e-03,\n",
            "           3.19693616e-04,  4.32594167e-03,  0.00000000e+00,\n",
            "          -1.50342274e-03,  0.00000000e+00,  5.44443028e-04,\n",
            "          -2.62855814e-04,  3.30051465e-04, -7.01375818e-03,\n",
            "          -1.40421628e-03,  9.91957844e-04,  7.22497498e-05,\n",
            "           7.64166907e-06, -2.05661217e-03, -1.11448891e-04,\n",
            "           3.58448888e-04,  8.81589309e-04,  6.88907632e-04,\n",
            "           1.39390968e-03,  3.96605069e-03, -3.90093919e-04,\n",
            "           4.71640524e-04,  8.72311881e-04],\n",
            "         [ 1.21414885e-02,  1.59994874e-04,  1.14596223e-04,\n",
            "          -2.34625666e-04,  8.10836907e-04, -1.24484282e-02,\n",
            "          -3.92093498e-04,  2.11306047e-04, -6.85354462e-03,\n",
            "          -1.67110178e-04,  4.05687420e-03,  0.00000000e+00,\n",
            "          -1.07555266e-03,  0.00000000e+00,  8.99846142e-04,\n",
            "          -3.31848132e-04,  6.53555500e-04, -6.88232156e-03,\n",
            "          -1.60380756e-03,  6.80733938e-04,  1.19450655e-04,\n",
            "           1.27361373e-05, -2.66160886e-03, -6.06755493e-05,\n",
            "          -3.24868888e-05,  6.00170169e-04,  8.27960554e-04,\n",
            "           1.78141508e-03,  4.07762825e-03, -1.17022013e-04,\n",
            "           5.00927039e-04,  1.97777103e-04]]],\n",
            "\n",
            "\n",
            "       [[[ 1.22283986e-02,  1.75427960e-03,  5.73612633e-05,\n",
            "          -1.48339299e-04,  1.44487782e-03, -1.10347755e-02,\n",
            "           1.48594310e-03,  1.19178940e-03, -8.55099875e-03,\n",
            "           1.38820207e-04,  5.10241510e-03,  0.00000000e+00,\n",
            "          -3.26408842e-03,  0.00000000e+00,  9.34893731e-04,\n",
            "          -1.48684325e-04,  9.04305416e-05, -7.81765766e-03,\n",
            "          -1.04799902e-03,  1.79619936e-03,  4.98063746e-04,\n",
            "           0.00000000e+00, -4.32233885e-03,  0.00000000e+00,\n",
            "          -2.29095458e-03,  1.02311652e-03,  1.30628614e-04,\n",
            "           0.00000000e+00,  5.35130175e-03, -1.48500968e-03,\n",
            "          -7.83742347e-04,  1.62684984e-04],\n",
            "         [ 1.31288432e-02,  1.90558925e-03, -5.75540907e-05,\n",
            "          -3.69406771e-04,  1.04941975e-03, -1.20131737e-02,\n",
            "           1.14968733e-03,  1.06653466e-03, -8.52294732e-03,\n",
            "          -2.33647617e-04,  4.99305082e-03,  0.00000000e+00,\n",
            "          -2.42365082e-03,  0.00000000e+00,  1.24747900e-03,\n",
            "          -2.62193964e-04,  5.38706081e-04, -8.01481586e-03,\n",
            "          -1.46950572e-03,  1.25206239e-03,  4.10458131e-04,\n",
            "           7.64166907e-06, -4.32447437e-03, -2.57513511e-05,\n",
            "          -2.11840682e-03,  8.73371959e-04,  2.13932406e-04,\n",
            "           6.88638131e-04,  5.28241787e-03, -1.69546064e-03,\n",
            "          -8.94060300e-04, -3.22760839e-04],\n",
            "         [ 1.30910398e-02,  1.93220540e-03, -1.25509498e-04,\n",
            "          -4.22385463e-04,  5.93271980e-04, -1.20177977e-02,\n",
            "           8.82630702e-04,  8.60632630e-04, -8.09056684e-03,\n",
            "          -4.81542782e-04,  4.72641643e-03,  0.00000000e+00,\n",
            "          -1.75310334e-03,  0.00000000e+00,  1.31949037e-03,\n",
            "          -3.36579804e-04,  8.29640427e-04, -7.92915374e-03,\n",
            "          -1.76959846e-03,  6.87887194e-04,  2.85057758e-04,\n",
            "           1.27361373e-05, -4.06505959e-03, -4.29189895e-05,\n",
            "          -1.86060369e-03,  6.98046526e-04,  3.07068694e-04,\n",
            "           1.11870340e-03,  4.98796953e-03, -1.69313082e-03,\n",
            "          -9.56628006e-04, -6.12107397e-04]],\n",
            "\n",
            "        [[ 1.21589685e-02,  1.75097177e-03,  1.49404153e-03,\n",
            "          -8.28082091e-04,  1.64010248e-03, -1.20585170e-02,\n",
            "          -2.44440947e-04, -4.66072779e-05, -8.30893777e-03,\n",
            "           2.05444492e-04,  5.26197348e-03,  0.00000000e+00,\n",
            "          -3.05058714e-03,  0.00000000e+00,  3.36949102e-04,\n",
            "           0.00000000e+00,  1.27466372e-03, -8.58851336e-03,\n",
            "          -1.02966616e-03,  1.82120595e-03,  8.69989162e-05,\n",
            "           7.43436613e-05, -3.42623144e-03,  0.00000000e+00,\n",
            "          -1.03351870e-03,  1.79503520e-03,  1.10801577e-03,\n",
            "           0.00000000e+00,  5.50539838e-03, -1.07862195e-03,\n",
            "          -5.42328577e-04,  1.34367368e-03],\n",
            "         [ 1.32679176e-02,  1.75348541e-03,  1.26311905e-03,\n",
            "          -6.13763987e-04,  1.38247944e-03, -1.29677411e-02,\n",
            "          -5.52773592e-04,  1.40278295e-04, -8.40486959e-03,\n",
            "          -1.32387795e-04,  5.28464653e-03,  0.00000000e+00,\n",
            "          -2.16826424e-03,  0.00000000e+00,  6.52034360e-04,\n",
            "          -1.28957297e-04,  1.53177115e-03, -8.89093988e-03,\n",
            "          -1.38459867e-03,  1.25132722e-03,  1.15824812e-04,\n",
            "           6.06769172e-05, -3.71940155e-03, -2.70264791e-05,\n",
            "          -1.31929503e-03,  1.31319056e-03,  1.00466376e-03,\n",
            "           6.55937591e-04,  5.30096190e-03, -1.12175208e-03,\n",
            "          -9.12195304e-04,  5.52805024e-04],\n",
            "         [ 1.34001309e-02,  1.67794875e-03,  1.04980031e-03,\n",
            "          -3.42123036e-04,  1.14589883e-03, -1.28912423e-02,\n",
            "          -7.19374453e-04,  3.01597844e-04, -8.05305876e-03,\n",
            "          -4.13178321e-04,  4.98714205e-03,  0.00000000e+00,\n",
            "          -1.55159691e-03,  0.00000000e+00,  7.49235798e-04,\n",
            "          -2.14928237e-04,  1.66234933e-03, -8.66038352e-03,\n",
            "          -1.60440174e-03,  7.33296853e-04,  1.29749969e-04,\n",
            "           4.86670688e-05, -3.76142655e-03, -4.45584374e-05,\n",
            "          -1.51503296e-03,  9.25069849e-04,  8.00922164e-04,\n",
            "           1.08060008e-03,  4.96042939e-03, -9.87171894e-04,\n",
            "          -1.25517591e-03,  3.33915195e-05]],\n",
            "\n",
            "        [[ 9.74073913e-03, -3.41105100e-04,  2.33851979e-03,\n",
            "          -1.16154563e-03,  5.12167288e-04, -1.22270873e-02,\n",
            "           0.00000000e+00,  2.86716713e-05, -6.94270292e-03,\n",
            "           8.96276848e-04,  4.35594004e-03,  0.00000000e+00,\n",
            "          -2.68626423e-03,  0.00000000e+00,  5.84254856e-04,\n",
            "          -6.11660187e-04,  2.77719955e-04, -6.65388256e-03,\n",
            "          -1.94838340e-03,  1.82989717e-03, -6.90098386e-05,\n",
            "           7.43436613e-05, -1.57025596e-03, -2.07000194e-04,\n",
            "           9.81588615e-04,  6.69870642e-04,  5.24089439e-04,\n",
            "           8.22527392e-04,  4.40831808e-03, -3.43780121e-04,\n",
            "           2.01242953e-03,  2.32475204e-03],\n",
            "         [ 1.10980924e-02, -2.00649301e-04,  1.93240924e-03,\n",
            "          -8.77589395e-04,  5.91151067e-04, -1.32420119e-02,\n",
            "          -3.45240027e-04,  1.34932299e-04, -7.49628153e-03,\n",
            "           4.19997319e-04,  4.54183435e-03,  0.00000000e+00,\n",
            "          -1.72005233e-03,  0.00000000e+00,  9.02491738e-04,\n",
            "          -6.71967980e-04,  6.43467938e-04, -7.10328575e-03,\n",
            "          -2.07084091e-03,  1.45804347e-03,  2.95391947e-05,\n",
            "           6.06769172e-05, -2.30444921e-03, -1.33339854e-04,\n",
            "           5.10880083e-04,  5.23098337e-04,  5.18507790e-04,\n",
            "           1.48586254e-03,  4.35554842e-03, -3.72666313e-04,\n",
            "           1.46236259e-03,  1.13547000e-03],\n",
            "         [ 1.15410360e-02, -3.36543199e-05,  1.53641833e-03,\n",
            "          -4.85900091e-04,  6.32117386e-04, -1.32848425e-02,\n",
            "          -5.60432556e-04,  2.12665647e-04, -7.53573468e-03,\n",
            "           4.98838708e-05,  4.43580840e-03,  0.00000000e+00,\n",
            "          -9.47134860e-04,  0.00000000e+00,  1.07214786e-03,\n",
            "          -7.11042318e-04,  9.74539784e-04, -7.10990606e-03,\n",
            "          -2.04971945e-03,  1.06995669e-03,  9.90722692e-05,\n",
            "           4.86670688e-05, -2.69019092e-03, -6.49364010e-05,\n",
            "           1.13196023e-04,  4.52707463e-04,  4.45485639e-04,\n",
            "           1.85519236e-03,  4.17971890e-03, -3.35653836e-04,\n",
            "           9.44935018e-04,  1.35057780e-04]]]], dtype=float32)>, <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
            "array([ 6.8761452e-05,  7.8873654e-06, -2.7555607e-06, -1.5214784e-05,\n",
            "        4.9194277e-06, -7.0485861e-05, -1.1847430e-05,  3.9768856e-06,\n",
            "       -5.4033757e-05,  1.2726423e-06,  2.2896691e-05,  0.0000000e+00,\n",
            "        9.4696697e-06,  0.0000000e+00,  2.0250818e-05, -4.7761914e-06,\n",
            "        1.7735674e-05, -2.9362025e-05, -8.7367771e-06,  1.2791335e-05,\n",
            "        2.5637937e-06,  2.8302520e-07, -1.8587602e-05, -9.5375344e-07,\n",
            "       -2.3885698e-06, -1.0175972e-06,  4.0842524e-06,  2.2893473e-05,\n",
            "        1.9501418e-05,  1.1998518e-06,  7.2434013e-06, -5.7623583e-06],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(3, 3, 32, 64), dtype=float32, numpy=\n",
            "array([[[[ 2.93921097e-04,  2.87092989e-04, -2.73735670e-04, ...,\n",
            "           2.70149176e-04, -7.42527889e-04,  2.95602076e-04],\n",
            "         [ 1.69884646e-04,  3.12891352e-04, -2.22415969e-04, ...,\n",
            "          -5.45778312e-06, -1.04170502e-03,  1.57661183e-04],\n",
            "         [-1.51795597e-04,  2.06807046e-04, -2.19669892e-04, ...,\n",
            "           1.22695972e-04,  1.58383817e-04,  2.69351120e-04],\n",
            "         ...,\n",
            "         [-8.44892347e-05, -6.89929730e-05,  4.54885885e-05, ...,\n",
            "           4.88313963e-05, -4.15041897e-04,  1.44693971e-04],\n",
            "         [ 1.69527528e-04, -9.50483809e-06, -3.27414251e-04, ...,\n",
            "           6.90694797e-05, -1.43789919e-04,  2.44277588e-04],\n",
            "         [ 8.45147297e-05,  1.22839585e-03, -5.86070120e-04, ...,\n",
            "           4.38170420e-04, -4.00920445e-03,  5.73427416e-04]],\n",
            "\n",
            "        [[ 4.03711456e-05,  3.55945172e-04, -8.04599113e-05, ...,\n",
            "           1.52629305e-04, -9.88032087e-04,  6.68597204e-05],\n",
            "         [ 1.96479537e-04,  2.45815900e-04, -2.45117117e-04, ...,\n",
            "           1.46660212e-04, -7.65298784e-04,  9.91656561e-05],\n",
            "         [-2.51323276e-04,  2.67167954e-04, -1.20860161e-04, ...,\n",
            "           1.78900125e-04,  4.00513163e-05,  1.62295895e-04],\n",
            "         ...,\n",
            "         [-4.27758314e-05,  1.26190731e-04, -1.85666853e-04, ...,\n",
            "          -9.69702523e-06, -4.91703213e-05,  3.17607919e-04],\n",
            "         [ 1.04912651e-04,  2.49321922e-04, -1.81626674e-04, ...,\n",
            "           8.83210814e-05,  6.03367400e-04,  3.21437605e-04],\n",
            "         [-1.03502942e-04,  1.24111003e-03, -1.42348974e-04, ...,\n",
            "           2.35250569e-04, -4.37124353e-03,  4.29019914e-04]],\n",
            "\n",
            "        [[-2.94765865e-04,  6.25265180e-04,  1.43607220e-04, ...,\n",
            "           1.00892285e-04, -1.65273121e-03,  7.25908685e-05],\n",
            "         [ 1.41784461e-04,  2.88873794e-04, -3.48778485e-06, ...,\n",
            "           1.49321742e-04, -7.12675333e-04,  6.73385512e-06],\n",
            "         [-4.41394106e-04,  2.76103296e-04,  7.53817658e-05, ...,\n",
            "           9.80274344e-05, -2.58597371e-04,  1.33473455e-04],\n",
            "         ...,\n",
            "         [ 3.25962028e-05,  1.23632810e-04, -1.38965925e-05, ...,\n",
            "           5.78918844e-06, -5.29704157e-05,  5.78003455e-05],\n",
            "         [-6.16366888e-05,  5.74109377e-04, -9.60551261e-05, ...,\n",
            "           4.83549520e-05, -9.34355194e-05,  3.54960393e-05],\n",
            "         [-6.83715683e-04,  1.54663762e-03,  1.29369088e-04, ...,\n",
            "           3.02491186e-04, -4.54236893e-03,  3.30923765e-04]]],\n",
            "\n",
            "\n",
            "       [[[ 3.19001527e-04,  5.38440887e-04, -3.21181840e-04, ...,\n",
            "           2.22130897e-04, -7.07753701e-04,  3.57646379e-04],\n",
            "         [ 1.65379475e-04,  5.55043625e-05, -8.35203828e-05, ...,\n",
            "          -2.80291424e-05, -9.80514917e-04,  2.56594038e-04],\n",
            "         [-2.07482779e-04,  2.43773888e-04, -1.27298146e-04, ...,\n",
            "           2.32140301e-05, -2.16368513e-04,  1.96105117e-04],\n",
            "         ...,\n",
            "         [-1.22680489e-04, -1.52836103e-04,  1.17396652e-04, ...,\n",
            "          -1.54835972e-04, -2.77093059e-04,  1.70673942e-04],\n",
            "         [ 1.14999377e-04,  2.11467646e-04, -2.82409805e-04, ...,\n",
            "          -3.11268494e-04,  1.03224447e-04,  2.83240312e-04],\n",
            "         [ 9.14493576e-05,  1.13281538e-03, -3.99766141e-04, ...,\n",
            "           3.95504496e-04, -3.69090121e-03,  7.68897764e-04]],\n",
            "\n",
            "        [[ 6.59414363e-05,  4.83351207e-04, -1.03178172e-04, ...,\n",
            "           9.33968258e-05, -1.26098737e-03,  9.29120579e-05],\n",
            "         [ 1.72297805e-04,  2.29355355e-04, -5.66436647e-05, ...,\n",
            "           6.79057703e-05, -6.73900126e-04,  1.69756575e-04],\n",
            "         [-2.31737489e-04,  1.78674061e-04,  1.73436292e-05, ...,\n",
            "           6.12903386e-06, -4.51837550e-04,  1.66997925e-04],\n",
            "         ...,\n",
            "         [-8.62471716e-05,  1.46389881e-04, -1.06415173e-04, ...,\n",
            "          -1.40534219e-04, -9.18560399e-05,  2.12877014e-04],\n",
            "         [ 1.54906902e-05,  1.74031287e-04, -7.70624247e-05, ...,\n",
            "           1.11742658e-04, -4.80587129e-04,  4.28263666e-05],\n",
            "         [ 1.10497931e-05,  1.52139226e-03, -1.81638607e-04, ...,\n",
            "           2.03776697e-04, -4.33578668e-03,  4.26529034e-04]],\n",
            "\n",
            "        [[-3.16106889e-04,  9.20959807e-04,  1.10572451e-04, ...,\n",
            "           1.50165230e-04, -1.49325794e-03,  7.48676830e-05],\n",
            "         [ 4.13122907e-05,  4.48153354e-04,  8.11057689e-05, ...,\n",
            "           6.90949601e-05, -8.55803024e-04,  3.66200147e-05],\n",
            "         [-2.40910318e-04,  1.62649056e-04,  4.10851717e-05, ...,\n",
            "           7.34173664e-05, -3.10410454e-04,  1.53056782e-04],\n",
            "         ...,\n",
            "         [ 6.39983991e-05,  4.07603584e-05,  8.17814507e-05, ...,\n",
            "          -4.32456000e-05, -4.71462379e-04,  5.78497384e-05],\n",
            "         [-4.93720072e-05,  2.73004669e-04,  6.29264250e-05, ...,\n",
            "           1.72062631e-04, -7.45112426e-04,  1.73239459e-06],\n",
            "         [-4.47890430e-04,  1.62361423e-03,  1.35824142e-04, ...,\n",
            "           3.65430111e-04, -4.64863190e-03,  4.71915759e-04]]],\n",
            "\n",
            "\n",
            "       [[[ 2.81256129e-04,  3.87574022e-04, -4.80427443e-05, ...,\n",
            "           1.08778855e-04, -1.03173603e-03,  3.12695105e-04],\n",
            "         [ 1.54762136e-04,  1.78932649e-04, -1.57107395e-04, ...,\n",
            "          -1.14420909e-04, -8.08610988e-04,  2.77871062e-04],\n",
            "         [-2.08112091e-04,  1.17618372e-04, -4.84927077e-05, ...,\n",
            "           1.93455038e-04, -2.29462166e-04,  5.14760432e-05],\n",
            "         ...,\n",
            "         [-6.83505277e-05, -1.50618289e-04,  1.06063701e-04, ...,\n",
            "          -3.65269661e-04, -2.52232770e-04,  4.18814780e-05],\n",
            "         [ 1.17980468e-04,  6.01628388e-04, -1.86954174e-04, ...,\n",
            "          -3.25737201e-04, -5.97121951e-04,  1.52899374e-04],\n",
            "         [ 1.23555947e-04,  1.24981150e-03, -3.97701864e-04, ...,\n",
            "           1.13488175e-04, -3.96234356e-03,  6.26817229e-04]],\n",
            "\n",
            "        [[ 6.10911520e-05,  5.59448265e-04,  6.69718283e-05, ...,\n",
            "           3.55288968e-04, -1.50547572e-03,  3.20195366e-04],\n",
            "         [ 1.83931174e-04,  2.42056121e-04, -2.10579165e-04, ...,\n",
            "           8.85023619e-05, -9.51616326e-04,  9.28163499e-05],\n",
            "         [-2.58599524e-04,  8.14077139e-05,  8.94973346e-05, ...,\n",
            "           3.42540530e-04, -3.98486329e-04,  3.22343694e-04],\n",
            "         ...,\n",
            "         [-1.16555391e-04,  1.61834632e-05, -1.85853278e-05, ...,\n",
            "          -7.63124408e-05, -7.92794090e-05,  3.00584434e-05],\n",
            "         [-3.10324685e-06,  5.39176166e-04, -2.20025941e-05, ...,\n",
            "           4.53383487e-04, -5.05218224e-04,  1.52819965e-04],\n",
            "         [ 1.85535755e-05,  1.43257319e-03, -8.91472882e-05, ...,\n",
            "           2.11712904e-04, -4.90261894e-03,  3.58026417e-04]],\n",
            "\n",
            "        [[-2.18472414e-05,  5.25534328e-04, -1.01175567e-04, ...,\n",
            "           4.42370976e-04, -1.56216894e-03,  3.53102514e-04],\n",
            "         [-2.43148825e-05,  5.76694147e-04,  9.06018395e-05, ...,\n",
            "           2.20737275e-04, -1.10006682e-03,  1.85337922e-05],\n",
            "         [-3.21942905e-04, -9.07016074e-05,  1.11981135e-05, ...,\n",
            "           3.49567854e-04, -2.17671753e-04,  3.50820133e-04],\n",
            "         ...,\n",
            "         [ 2.14161228e-05,  3.30088951e-05,  8.13684310e-05, ...,\n",
            "           3.02425906e-05, -2.68062198e-04,  1.73948705e-04],\n",
            "         [-3.46699962e-04,  5.30781748e-04, -3.83530569e-05, ...,\n",
            "           9.34373762e-04, -4.73087362e-04,  8.12298036e-04],\n",
            "         [-2.16683839e-04,  1.39972626e-03, -6.66135311e-05, ...,\n",
            "           5.16862376e-04, -4.41379892e-03,  5.27160941e-04]]]],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
            "array([-1.73475728e-05,  1.06040443e-05, -3.52407665e-06, -1.20590348e-05,\n",
            "        2.09885934e-06,  5.27530137e-05, -6.44436805e-05, -1.09232587e-06,\n",
            "        3.78459981e-06, -5.36825937e-05, -1.09638759e-05,  2.51985202e-05,\n",
            "       -3.97007861e-05, -1.16229721e-05, -4.22727180e-06,  1.21789299e-08,\n",
            "        2.81426401e-05,  3.33306780e-05, -1.94962413e-05, -9.52897062e-06,\n",
            "        9.84627832e-07, -3.51295421e-05, -2.76414130e-06, -6.17498445e-05,\n",
            "       -4.85213204e-05, -2.63820766e-05,  1.48959662e-05,  1.85611589e-06,\n",
            "       -3.03928573e-05,  1.67267226e-05, -3.76845819e-05,  2.34092331e-05,\n",
            "       -6.18260674e-05,  1.70391668e-05,  3.00424858e-08,  1.17586160e-06,\n",
            "        5.55945235e-06,  1.86803879e-06,  1.83811535e-05,  3.91836656e-05,\n",
            "        7.19291165e-07, -1.18086859e-06,  1.59629217e-05, -4.71117801e-07,\n",
            "       -4.16405646e-06, -7.95727610e-05, -3.62452738e-07, -1.05529057e-06,\n",
            "        1.97085901e-05, -2.66826942e-06,  3.20726002e-07,  1.87859296e-05,\n",
            "       -1.87737078e-05,  4.51827873e-06,  2.14888951e-06,  3.12190059e-06,\n",
            "       -4.64188633e-05, -3.97732430e-10,  2.68773254e-10,  5.56574814e-05,\n",
            "       -5.28826513e-06,  4.45081560e-06, -5.31952755e-05,  1.48382405e-05],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(3, 3, 64, 128), dtype=float32, numpy=\n",
            "array([[[[-1.19577824e-04,  5.43837086e-05, -1.41434808e-04, ...,\n",
            "           1.13132883e-05, -2.15417007e-04,  5.35200525e-05],\n",
            "         [ 1.60752737e-04,  2.80020409e-04, -1.48727733e-04, ...,\n",
            "          -3.44349683e-05,  1.78328104e-04,  9.39715537e-05],\n",
            "         [ 3.27779562e-05,  3.70367779e-05, -1.06222433e-05, ...,\n",
            "           4.23016900e-06,  8.87603965e-05,  8.85016925e-05],\n",
            "         ...,\n",
            "         [ 1.12748879e-04, -4.46302729e-05, -4.82296222e-04, ...,\n",
            "           1.35451846e-05, -2.11023143e-04,  1.30012922e-05],\n",
            "         [ 2.75342434e-04, -2.26851553e-05, -2.02757539e-03, ...,\n",
            "          -7.76322850e-06,  2.90022406e-04,  2.36415304e-04],\n",
            "         [ 8.03562580e-06,  1.71685388e-05, -9.77134914e-06, ...,\n",
            "          -4.73236469e-06, -4.59329531e-05,  3.88322514e-05]],\n",
            "\n",
            "        [[-1.00187957e-04, -1.27174244e-05,  7.97794928e-06, ...,\n",
            "          -4.40378608e-06, -1.96245761e-04,  4.48934516e-05],\n",
            "         [ 3.98294447e-04,  7.52466512e-05,  6.75846095e-05, ...,\n",
            "           5.33303464e-05,  3.78687400e-06,  1.85068144e-04],\n",
            "         [ 1.01343670e-04,  2.11280334e-04, -2.33575105e-04, ...,\n",
            "           8.64596950e-06, -6.90407032e-05,  1.33755457e-04],\n",
            "         ...,\n",
            "         [-1.12430222e-04,  1.15320843e-04, -2.94586876e-04, ...,\n",
            "           1.38109954e-05,  1.53687841e-04,  1.93308908e-04],\n",
            "         [ 2.24885182e-04,  1.37748051e-04, -1.69850141e-03, ...,\n",
            "          -2.65895178e-05,  6.10836491e-04,  1.97672329e-04],\n",
            "         [ 2.39865767e-05,  2.31977974e-05, -1.55514150e-04, ...,\n",
            "           1.24254962e-06, -1.03352191e-04,  7.92068749e-05]],\n",
            "\n",
            "        [[ 4.49533472e-05, -3.81497339e-05, -1.49180327e-04, ...,\n",
            "           4.16099374e-06, -3.84214101e-04, -1.71782449e-06],\n",
            "         [ 3.41740088e-04,  1.29414519e-04,  6.58155477e-06, ...,\n",
            "           4.11515066e-06, -3.18381761e-04,  6.96515053e-05],\n",
            "         [ 4.35048132e-05,  1.77552472e-04, -1.65970167e-04, ...,\n",
            "           4.60875854e-05,  1.45463186e-04,  4.87420039e-06],\n",
            "         ...,\n",
            "         [ 4.40417061e-05,  6.76449199e-05, -3.19923856e-04, ...,\n",
            "           1.47793471e-05,  7.51072948e-05,  1.56682479e-04],\n",
            "         [ 4.87001962e-04,  2.44663155e-04, -1.26825774e-03, ...,\n",
            "           2.31303966e-05, -2.02142401e-05,  8.77994462e-05],\n",
            "         [ 3.02567787e-06,  6.32723677e-05, -1.60458105e-04, ...,\n",
            "           2.03152013e-05,  4.68360668e-06,  3.05777721e-05]]],\n",
            "\n",
            "\n",
            "       [[[-1.23107398e-04,  2.11191073e-04, -3.43949214e-05, ...,\n",
            "           1.52030698e-05, -1.57077331e-04,  3.36086450e-05],\n",
            "         [-1.57945324e-05,  2.31255544e-04, -2.77284824e-04, ...,\n",
            "          -4.19458302e-05,  2.31504906e-04,  1.61260541e-05],\n",
            "         [ 5.91489370e-05,  7.77098030e-05,  2.33296378e-05, ...,\n",
            "          -5.97348844e-07, -1.99057919e-04,  2.14345637e-05],\n",
            "         ...,\n",
            "         [ 4.21524746e-04, -2.15450709e-05, -1.98149122e-04, ...,\n",
            "          -1.64565281e-05, -1.28335669e-04,  7.34780042e-06],\n",
            "         [ 1.18397584e-04,  5.76831400e-04, -1.95191510e-03, ...,\n",
            "          -7.91627099e-06,  5.92406956e-04,  2.40077527e-04],\n",
            "         [ 3.27963498e-05, -9.70831024e-07, -4.03306694e-06, ...,\n",
            "           1.04116552e-05, -8.26650648e-05,  4.45225232e-05]],\n",
            "\n",
            "        [[-7.71159102e-05,  1.03396131e-04, -1.97692338e-04, ...,\n",
            "           5.14231651e-05, -6.91985333e-05,  1.22431025e-04],\n",
            "         [ 6.90445886e-05,  1.98718364e-04, -2.23943527e-04, ...,\n",
            "           5.79069711e-05,  1.74381814e-04,  3.20083316e-04],\n",
            "         [ 7.93117215e-06,  1.69540421e-04, -1.43355646e-04, ...,\n",
            "           5.42303678e-06, -8.27097028e-05,  9.30427923e-05],\n",
            "         ...,\n",
            "         [-1.12439084e-04,  3.97557713e-04, -2.77516694e-04, ...,\n",
            "          -4.56428825e-06,  4.17844458e-05, -4.61905984e-06],\n",
            "         [ 1.38943258e-04,  8.04201700e-04, -1.74204120e-03, ...,\n",
            "           1.16068994e-04,  8.54917162e-04,  3.59538477e-04],\n",
            "         [ 4.02450678e-06,  1.49345899e-04, -6.77601201e-05, ...,\n",
            "           1.21770427e-07, -1.39156080e-04, -2.12497434e-06]],\n",
            "\n",
            "        [[ 6.09761191e-05, -2.21970549e-05, -3.89457506e-04, ...,\n",
            "          -9.53358813e-06, -3.38548736e-04, -2.34073650e-05],\n",
            "         [ 1.16655137e-04,  5.40825189e-04, -4.04070190e-04, ...,\n",
            "           8.74386096e-05,  5.48064127e-05,  2.87727977e-04],\n",
            "         [ 1.46984035e-04,  8.38594133e-05, -1.27077597e-04, ...,\n",
            "           5.77652136e-05,  1.61399803e-04,  5.36846346e-05],\n",
            "         ...,\n",
            "         [ 1.09517190e-04,  3.61460203e-04, -2.67585361e-04, ...,\n",
            "           3.37413876e-05,  1.69526320e-05,  7.31985783e-05],\n",
            "         [ 5.07498393e-04,  5.62319765e-04, -1.97795429e-03, ...,\n",
            "           8.19311099e-05, -3.17585014e-04,  2.66102725e-04],\n",
            "         [ 1.81400537e-06,  6.41430815e-05, -1.15376635e-04, ...,\n",
            "           3.59198093e-05,  6.06123795e-05,  1.90914761e-05]]],\n",
            "\n",
            "\n",
            "       [[[-1.20518089e-04,  1.87480182e-05,  1.91305298e-06, ...,\n",
            "           8.70925578e-06, -2.26498407e-04, -5.79602784e-07],\n",
            "         [ 8.72399542e-05,  1.24503684e-04, -4.35289330e-05, ...,\n",
            "           1.16340816e-05, -1.60393713e-04, -9.64058054e-05],\n",
            "         [ 2.19487643e-04, -9.46076470e-05,  3.21647021e-05, ...,\n",
            "           3.65692904e-06, -2.99276988e-04, -4.04399907e-06],\n",
            "         ...,\n",
            "         [ 2.21940631e-04, -1.09867018e-04, -1.66757789e-04, ...,\n",
            "           1.99281076e-05, -5.33657905e-04,  7.03368860e-05],\n",
            "         [ 2.18379690e-04,  5.43583534e-04, -1.66045723e-03, ...,\n",
            "          -3.44857654e-05, -8.21932044e-05,  1.28359796e-04],\n",
            "         [ 7.06959836e-05, -4.00042918e-05,  1.54748050e-05, ...,\n",
            "           2.45024148e-06, -2.48906552e-04,  4.11174515e-05]],\n",
            "\n",
            "        [[-2.34586059e-05, -6.13784432e-05, -1.72367101e-04, ...,\n",
            "           3.64563020e-05, -2.75332641e-05,  9.84974540e-05],\n",
            "         [ 2.48012017e-04,  2.61085515e-04, -2.41936359e-05, ...,\n",
            "          -9.64360152e-06, -4.30136133e-04,  9.85496154e-05],\n",
            "         [-5.51578705e-06, -6.78207798e-05,  4.19233984e-05, ...,\n",
            "           5.05015123e-08, -2.28225690e-05, -9.08847214e-05],\n",
            "         ...,\n",
            "         [-9.24250126e-05, -9.26687499e-05, -2.17039982e-04, ...,\n",
            "           3.54365657e-05, -1.47747589e-04, -5.19078021e-05],\n",
            "         [ 3.46858986e-04,  2.24512594e-04, -1.67237537e-03, ...,\n",
            "           1.02981925e-04,  5.14749088e-04,  3.79774894e-04],\n",
            "         [ 1.65207748e-05, -6.30092909e-05,  2.00433969e-05, ...,\n",
            "           3.66974575e-07, -7.19437085e-05, -5.41019108e-05]],\n",
            "\n",
            "        [[-1.03703496e-05, -9.95752998e-07, -2.04004988e-04, ...,\n",
            "           2.36028609e-05, -2.12863000e-04,  5.50731238e-05],\n",
            "         [ 9.45340144e-05,  1.51611544e-04, -1.09840192e-04, ...,\n",
            "           2.07218691e-05, -2.89163261e-04,  8.07170145e-05],\n",
            "         [ 1.23757345e-05, -1.21345511e-04,  4.00398130e-05, ...,\n",
            "           2.42624610e-05,  1.94166263e-04,  1.45744751e-04],\n",
            "         ...,\n",
            "         [ 4.33726818e-05, -3.47384484e-04, -2.38219058e-04, ...,\n",
            "           2.32778457e-05,  1.77818962e-04,  2.34241743e-04],\n",
            "         [ 3.41027742e-04,  3.81273596e-04, -1.70297443e-03, ...,\n",
            "           6.90189918e-05, -4.54050692e-04,  5.34564839e-04],\n",
            "         [ 7.25041536e-06, -1.40841847e-04,  8.78967512e-06, ...,\n",
            "           1.80351378e-08,  1.12130525e-04,  3.37961683e-05]]]],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([ 7.62661193e-06,  7.31820319e-06, -7.00255696e-05, -2.37426902e-05,\n",
            "        8.42124973e-06,  1.80232055e-05, -1.83408747e-06, -5.67101824e-06,\n",
            "       -9.45254214e-05,  3.92051224e-05, -1.36848994e-05,  2.68947497e-05,\n",
            "        4.12848749e-05,  2.05531906e-05, -1.14841450e-05, -6.03374428e-06,\n",
            "        3.69099530e-06, -1.09075590e-05, -4.10435241e-05, -2.12391860e-05,\n",
            "        8.61485432e-07, -4.93485118e-07,  2.28327317e-05, -9.44920600e-07,\n",
            "        2.14751799e-05,  1.56001879e-05,  1.12923572e-05, -3.56545861e-05,\n",
            "       -3.00758620e-06, -6.11771029e-05,  8.41365400e-05, -2.35030657e-05,\n",
            "       -3.25048168e-05,  1.55715747e-06,  1.80678016e-05, -1.41725104e-05,\n",
            "       -1.81532403e-06,  3.13639248e-06, -2.07038720e-05,  1.03285265e-05,\n",
            "       -8.43276757e-06, -2.76002856e-06,  6.59549960e-06, -2.31827446e-08,\n",
            "       -6.26217434e-06,  4.55253648e-05, -4.74841363e-05, -6.03588924e-05,\n",
            "       -3.88753579e-05,  5.58788406e-06,  3.52224697e-06,  4.07819716e-05,\n",
            "       -2.68946606e-05,  2.26145421e-05, -2.04547541e-06,  1.78775335e-05,\n",
            "       -1.45741069e-05,  9.20857856e-05,  9.84558210e-06, -3.24655775e-05,\n",
            "       -6.12073563e-05,  1.64343476e-06, -1.08113927e-05, -1.08821109e-06,\n",
            "        9.82878555e-06, -2.62346248e-06,  2.02141237e-05, -8.08440927e-06,\n",
            "        4.81989082e-05,  8.09078847e-05,  3.94998460e-05,  5.49306133e-05,\n",
            "       -6.03532897e-07, -8.01681017e-06, -6.51477330e-06, -4.66765632e-05,\n",
            "        7.06647406e-05,  7.51299513e-06, -7.16192735e-05, -5.58849251e-06,\n",
            "        0.00000000e+00,  6.23779852e-05, -2.11382053e-06, -1.64871608e-07,\n",
            "       -2.01700459e-05, -5.59113460e-06, -2.83705594e-05, -3.04743844e-05,\n",
            "       -8.49143817e-06,  1.10982175e-04, -1.52592129e-05, -1.09538960e-05,\n",
            "       -9.24673532e-06,  5.00437854e-05, -3.49363763e-05, -6.69774454e-05,\n",
            "       -4.30128530e-05,  7.91760467e-05,  1.47491482e-05, -2.08039182e-06,\n",
            "       -4.10027169e-05, -1.05744868e-04,  0.00000000e+00, -5.11015314e-05,\n",
            "       -1.08909380e-05, -5.13878986e-05,  2.84333801e-05,  1.50337637e-05,\n",
            "        1.63506502e-05, -1.69355133e-06,  0.00000000e+00,  2.08621113e-05,\n",
            "       -1.73112330e-05, -2.73346745e-06, -2.84195194e-05, -6.84344268e-05,\n",
            "        7.75221706e-05,  1.03355378e-05,  1.39258680e-06,  6.14822648e-06,\n",
            "       -1.28751144e-05,  2.61598070e-05,  9.54493862e-06, -1.91098334e-05,\n",
            "        3.66422755e-05,  2.34985600e-06, -1.80549214e-05,  1.31094366e-05],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(4608, 128), dtype=float32, numpy=\n",
            "array([[ 0.0000000e+00, -5.9189362e-05,  0.0000000e+00, ...,\n",
            "        -1.0030538e-04,  0.0000000e+00,  5.1378161e-06],\n",
            "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
            "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
            "       [ 0.0000000e+00, -1.7515890e-04,  0.0000000e+00, ...,\n",
            "        -2.6427762e-04,  0.0000000e+00,  1.7297281e-05],\n",
            "       ...,\n",
            "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
            "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
            "       [ 0.0000000e+00, -5.4639590e-04,  0.0000000e+00, ...,\n",
            "        -8.6248649e-04,  0.0000000e+00,  5.2108800e-05],\n",
            "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
            "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([ 0.00000000e+00, -2.68930671e-05,  0.00000000e+00, -2.50091434e-05,\n",
            "        6.29780970e-06, -1.11860572e-06, -7.51324624e-05,  2.88217889e-05,\n",
            "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.93458436e-05,\n",
            "        0.00000000e+00,  3.04559890e-05, -2.95140671e-05,  1.78875380e-05,\n",
            "        6.96647203e-06,  8.01101487e-06, -7.47508466e-07,  0.00000000e+00,\n",
            "        0.00000000e+00, -2.48508277e-05,  0.00000000e+00,  4.29468372e-09,\n",
            "        4.75480483e-05,  8.25514289e-05, -1.87113110e-05,  0.00000000e+00,\n",
            "       -4.60904357e-06,  3.28680153e-06,  4.29764805e-06, -3.25937799e-06,\n",
            "       -1.61770868e-06,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
            "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -5.07266705e-05,\n",
            "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  7.92159597e-07,\n",
            "        0.00000000e+00,  4.29240026e-05, -2.56175408e-05, -2.68256244e-05,\n",
            "        3.94254057e-05, -9.85726729e-05,  0.00000000e+00, -6.11788892e-07,\n",
            "        2.53232400e-07, -7.89795231e-05,  1.90058399e-05,  4.80116214e-05,\n",
            "       -2.92724199e-05,  2.04248154e-05,  0.00000000e+00,  3.60740887e-05,\n",
            "        7.61889314e-05, -2.11243355e-06,  0.00000000e+00,  3.05735921e-05,\n",
            "        2.12530358e-06,  1.85135450e-05, -3.49109650e-05,  0.00000000e+00,\n",
            "        2.23152547e-05, -2.76936021e-07,  3.83108018e-05,  3.79481298e-06,\n",
            "       -6.19181810e-05, -2.86202740e-05,  8.44066108e-06,  1.98008929e-05,\n",
            "        4.27423292e-05, -1.02922051e-04, -1.37322959e-05, -2.55568666e-05,\n",
            "        5.68794894e-05, -9.22729942e-06,  2.58695727e-05,  9.37087350e-08,\n",
            "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  3.04206351e-05,\n",
            "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
            "       -9.46182045e-05, -5.70464954e-06,  0.00000000e+00,  0.00000000e+00,\n",
            "       -1.41337032e-05,  0.00000000e+00, -1.39661715e-05,  1.96654346e-05,\n",
            "        0.00000000e+00, -3.55948650e-05,  0.00000000e+00,  0.00000000e+00,\n",
            "        0.00000000e+00, -1.97849367e-05, -1.82314743e-05, -3.37911115e-05,\n",
            "       -4.91878154e-06,  0.00000000e+00,  0.00000000e+00,  8.12645521e-06,\n",
            "       -2.02495667e-05,  4.94044762e-05,  0.00000000e+00,  0.00000000e+00,\n",
            "       -1.98344078e-05,  3.62698993e-05, -4.47853672e-05, -2.31258650e-06,\n",
            "        6.92592403e-06,  1.90714727e-05, -2.07272788e-05, -1.20521890e-05,\n",
            "        0.00000000e+00, -4.28216190e-05,  0.00000000e+00,  2.51879464e-06],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(128, 15), dtype=float32, numpy=\n",
            "array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
            "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
            "       [ 3.5710273e-30,  2.7929361e-06,  6.5200916e-04, ...,\n",
            "         3.3075176e-10,  4.1162943e-15, -4.9485676e-15],\n",
            "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
            "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
            "       ...,\n",
            "       [ 2.1958518e-30,  2.9602575e-07,  3.3615076e-04, ...,\n",
            "         7.2319088e-11,  1.0674700e-15,  6.6940141e-15],\n",
            "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
            "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
            "       [ 4.6328125e-32, -7.8832400e-08,  2.5531595e-05, ...,\n",
            "         9.3229112e-11,  3.4904658e-16, -1.1302181e-14]], dtype=float32)>, <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
            "array([ 9.0630538e-32,  1.4945697e-07,  2.5918471e-05,  7.1264498e-05,\n",
            "        1.1025040e-22,  1.8693951e-04, -2.8605433e-04,  6.0091239e-09,\n",
            "        3.7791918e-18,  1.7739955e-06,  2.4178155e-09,  8.5230800e-14,\n",
            "        1.7575124e-11,  1.6297341e-16, -8.1437623e-16], dtype=float32)>]\n",
            "Gradients for value: [None, None, None, None, None, None, None, None, None, None]\n",
            "Value gradients are None!\n",
            "EPOCA: 0  POLICY LOSS: tf.Tensor(7.4505806e-08, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(85.39233, shape=(), dtype=float32)\n",
            "Batches\n",
            "Advantages calculated\n",
            "Mean and std of advantages: -1.0468066e-06 1.0\n",
            "log_probs is tensor: True\n",
            "advantage is tensor: True\n",
            "rewards_to_go is tensor: True\n",
            "V is tensor: True\n",
            "Policy Loss: tf.Tensor(-1.3671815e-06, shape=(), dtype=float32)\n",
            "Value Loss: tf.Tensor(65.48688, shape=(), dtype=float32)\n",
            "Gradients for policy: [<tf.Tensor: shape=(3, 3, 3, 32), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(3, 3, 32, 64), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
            "array([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "       -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,   0., -inf,\n",
            "       -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "       -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "       -inf, -inf, -inf,   0., -inf, -inf,   0., -inf, -inf, -inf, -inf,\n",
            "       -inf, -inf, -inf,   0., -inf, -inf, -inf, -inf, -inf],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(3, 3, 64, 128), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(4608, 128), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(128, 15), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan], dtype=float32)>]\n",
            "Gradients for value: [None, None, None, None, None, None, None, None, None, None]\n",
            "Value gradients are None!\n",
            "EPOCA: 0  POLICY LOSS: tf.Tensor(-1.3671815e-06, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(65.48688, shape=(), dtype=float32)\n",
            "Batches\n",
            "Advantages calculated\n",
            "Mean and std of advantages: -4.4330955e-07 1.0\n",
            "log_probs is tensor: True\n",
            "advantage is tensor: True\n",
            "rewards_to_go is tensor: True\n",
            "V is tensor: True\n",
            "Policy Loss: tf.Tensor(nan, shape=(), dtype=float32)\n",
            "Value Loss: tf.Tensor(65.22948, shape=(), dtype=float32)\n",
            "Gradients for policy: [<tf.Tensor: shape=(3, 3, 3, 32), dtype=float32, numpy=\n",
            "array([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]]], dtype=float32)>, <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(3, 3, 32, 64), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Tensor: shape=(3, 3, 64, 128), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Tensor: shape=(4608, 128), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(128, 15), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan], dtype=float32)>]\n",
            "Gradients for value: [None, None, None, None, None, None, None, None, None, None]\n",
            "Value gradients are None!\n",
            "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(65.22948, shape=(), dtype=float32)\n",
            "Batches\n",
            "Advantages calculated\n",
            "Mean and std of advantages: -5.1409006e-07 1.0\n",
            "log_probs is tensor: True\n",
            "advantage is tensor: True\n",
            "rewards_to_go is tensor: True\n",
            "V is tensor: True\n",
            "Policy Loss: tf.Tensor(nan, shape=(), dtype=float32)\n",
            "Value Loss: tf.Tensor(64.96915, shape=(), dtype=float32)\n",
            "Gradients for policy: [<tf.Tensor: shape=(3, 3, 3, 32), dtype=float32, numpy=\n",
            "array([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]]], dtype=float32)>, <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(3, 3, 32, 64), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Tensor: shape=(3, 3, 64, 128), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Tensor: shape=(4608, 128), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(128, 15), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan], dtype=float32)>]\n",
            "Gradients for value: [None, None, None, None, None, None, None, None, None, None]\n",
            "Value gradients are None!\n",
            "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(64.96915, shape=(), dtype=float32)\n",
            "Batches\n",
            "Advantages calculated\n",
            "Mean and std of advantages: -9.4622374e-07 1.0\n",
            "log_probs is tensor: True\n",
            "advantage is tensor: True\n",
            "rewards_to_go is tensor: True\n",
            "V is tensor: True\n",
            "Policy Loss: tf.Tensor(nan, shape=(), dtype=float32)\n",
            "Value Loss: tf.Tensor(65.74721, shape=(), dtype=float32)\n",
            "Gradients for policy: [<tf.Tensor: shape=(3, 3, 3, 32), dtype=float32, numpy=\n",
            "array([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]]], dtype=float32)>, <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(3, 3, 32, 64), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Tensor: shape=(3, 3, 64, 128), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Tensor: shape=(4608, 128), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(128, 15), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan], dtype=float32)>]\n",
            "Gradients for value: [None, None, None, None, None, None, None, None, None, None]\n",
            "Value gradients are None!\n",
            "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(65.74721, shape=(), dtype=float32)\n",
            "Batches\n",
            "Advantages calculated\n",
            "Mean and std of advantages: 9.685755e-08 1.0\n",
            "log_probs is tensor: True\n",
            "advantage is tensor: True\n",
            "rewards_to_go is tensor: True\n",
            "V is tensor: True\n",
            "Policy Loss: tf.Tensor(nan, shape=(), dtype=float32)\n",
            "Value Loss: tf.Tensor(64.71175, shape=(), dtype=float32)\n",
            "Gradients for policy: [<tf.Tensor: shape=(3, 3, 3, 32), dtype=float32, numpy=\n",
            "array([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]]], dtype=float32)>, <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(3, 3, 32, 64), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Tensor: shape=(3, 3, 64, 128), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Tensor: shape=(4608, 128), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(128, 15), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan], dtype=float32)>]\n",
            "Gradients for value: [None, None, None, None, None, None, None, None, None, None]\n",
            "Value gradients are None!\n",
            "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(64.71175, shape=(), dtype=float32)\n",
            "Batches\n",
            "Advantages calculated\n",
            "Mean and std of advantages: -1.0468066e-06 1.0\n",
            "log_probs is tensor: True\n",
            "advantage is tensor: True\n",
            "rewards_to_go is tensor: True\n",
            "V is tensor: True\n",
            "Policy Loss: tf.Tensor(nan, shape=(), dtype=float32)\n",
            "Value Loss: tf.Tensor(65.48688, shape=(), dtype=float32)\n",
            "Gradients for policy: [<tf.Tensor: shape=(3, 3, 3, 32), dtype=float32, numpy=\n",
            "array([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]]], dtype=float32)>, <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(3, 3, 32, 64), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Tensor: shape=(3, 3, 64, 128), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Tensor: shape=(4608, 128), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(128, 15), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan], dtype=float32)>]\n",
            "Gradients for value: [None, None, None, None, None, None, None, None, None, None]\n",
            "Value gradients are None!\n",
            "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(65.48688, shape=(), dtype=float32)\n",
            "Batches\n",
            "Advantages calculated\n",
            "Mean and std of advantages: -4.4330955e-07 1.0\n",
            "log_probs is tensor: True\n",
            "advantage is tensor: True\n",
            "rewards_to_go is tensor: True\n",
            "V is tensor: True\n",
            "Policy Loss: tf.Tensor(nan, shape=(), dtype=float32)\n",
            "Value Loss: tf.Tensor(65.22948, shape=(), dtype=float32)\n",
            "Gradients for policy: [<tf.Tensor: shape=(3, 3, 3, 32), dtype=float32, numpy=\n",
            "array([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0.]]]], dtype=float32)>, <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(3, 3, 32, 64), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Tensor: shape=(3, 3, 64, 128), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Tensor: shape=(4608, 128), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(128, 15), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan], dtype=float32)>]\n",
            "Gradients for value: [None, None, None, None, None, None, None, None, None, None]\n",
            "Value gradients are None!\n",
            "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(65.22948, shape=(), dtype=float32)\n",
            "Warning: early reset ignored\n",
            "episode:  0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Total of weights must be finite",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a63c96248677>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'procgen:procgen-coinrun-v0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdistribution_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'easy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_levels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mppo_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mppo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-8d2614101be8>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#passo 2 ciclare per k iterazioni.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnEpoche\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_to_go\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trajectories collected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-8d2614101be8>\u001b[0m in \u001b[0;36mcollect_trajectories\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepsPerEpisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'states'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstato\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mazione\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstato\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0;31m#azione sarà un int, mentre log_prob sarà il logaritmo della probabilità dell'azione\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mazione\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-8d2614101be8>\u001b[0m in \u001b[0;36mgetAction\u001b[0;34m(self, stato)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m#dist=tfp.distributions.Categorical(probs=azione_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# azionePresa=dist.sample()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mazionePresa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistaAzioni\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mazione_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;31m#dist=tfp.distributions.MultivariateNormalTriL(loc=azione_prob, scale_tril=tf.linalg.cholesky(self.cov_mat))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m#azionePresa=dist.sample()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/random.py\u001b[0m in \u001b[0;36mchoices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total of weights must be greater than zero'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_isfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total of weights must be finite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0mbisect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bisect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Total of weights must be finite"
          ]
        }
      ],
      "source": [
        "# Configurazione ed esecuzione\n",
        "env = gym.make('procgen:procgen-coinrun-v0',distribution_mode='easy', start_level=0, num_levels=1)\n",
        "ppo_model=PPO()\n",
        "ppo_model.learn(env)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}