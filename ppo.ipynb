{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sd2DHK4QYaKH"
   },
   "source": [
    "This module contains the implementation of the PPO algorithm.\n",
    "Ci basiamo sullo pseudocodice presente sul sito di OpenAI per la realizzazione del ppo.\n",
    "https://spinningup.openai.com/en/latest/algorithms/ppo.html#id7\n",
    "Utilizzando un Actor-Critic Method.\n",
    "Ciò suddivide l'implementazione in 8 passi principali:\n",
    "1. Inizializzazione dell'ambiente con policy parameters theta_0, e l'inizial value function parameters w_0.\n",
    "2. Ciclare per k iterazioni\n",
    "3. Raccogliere un set di traiettorie D_k = {τ_i} con una policy pi_k = pi(theta_k)\n",
    "4. Calcolare i reward-to-go R_t\n",
    "5. Calcolare gli advantage estimates A_t basandoci sulla value function V_{w_k}\n",
    "6. Aggiornare la policy massimizzando la PPO-Clip objective (Gradient ascent con adam) . Non scriverò la formula che è complessa\n",
    "7. Aggiornare la value function minimizzando la MSE tra V_{w_k} e R_t (Gradient descent con adam)\n",
    "8. Fine ciclo.\n",
    "\n",
    "Implementiamo tutti i passi nella funzione learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pqY5vJI-YaKI",
    "outputId": "77884fa9-5efe-4955-a667-c9a425a073ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #ignora warnings\n",
    "#Check if colab is used:\n",
    "from rete import ReteNeurale\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glfw\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print(\"Not running on CoLab\")\n",
    "  #print list of GPUs\n",
    "  #tf. config. list_physical_devices('GPU')\n",
    "  print(\"Devices: \", tf.config.list_physical_devices())\n",
    "  print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "if IN_COLAB:\n",
    "  !pip install procgen\n",
    "  !pip install tensorflow_probability\n",
    "  !pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f9EeZ0iE56HQ",
    "outputId": "3798c572-d4c3-4c01-d797-ac029befc27e"
   },
   "outputs": [],
   "source": [
    "#Tutta questa parte è relativa alla visualizzazione del gioco tramite salvataggio degli stati di un episode in una specifica cartella.\n",
    "#Tutto questo viene fatto perchè non si riesce a visualizzare procgen in live per problemi con glfw e openGL su Ubuntu.\n",
    "from moviepy import ImageSequenceClip\n",
    "from IPython.display import Video\n",
    "import os\n",
    "from pyvirtualdisplay.smartdisplay import SmartDisplay\n",
    "display = SmartDisplay(visible=0, size=(1920,1080),fbdir='/tmp')\n",
    "display.start()\n",
    "glfw.init()\n",
    "available_fbconfigs = glfw.get_video_modes(glfw.get_primary_monitor())\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'osmesa'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohXWluKCYaKJ"
   },
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self,env,gameName,totalSteps=2000000):\n",
    "        self.env=env\n",
    "        self.gameName=gameName\n",
    "        self.nAzioni=env.action_space.n\n",
    "        self.nStati=env.observation_space.shape\n",
    "        self.listaAzioni=[i for i in range(self.nAzioni)]\n",
    "        self.nTimestampsPerBatch=8192\n",
    "        self.stepsPerEpisode=1000\n",
    "        self.nTotalTimestamps=totalSteps\n",
    "        self.episodesPerBatch=10\n",
    "        self.nEpoche=300\n",
    "        self.gamma=0.99\n",
    "        self.epsilon=0.2\n",
    "        self.learningRate=3e-4\n",
    "        self.policyNN=ReteNeurale(self.nStati,self.nAzioni) #Actor\n",
    "        self.policy_optimizer=keras.optimizers.Adam(learning_rate=self.learningRate, clipnorm=1.0)\n",
    "        self.policyNN.compile(optimizer=self.policy_optimizer)\n",
    "        self.entropyCoefficient=0.01 #Per invogliare l'esplorazione un po di più.\n",
    "        self.lambdaGAE=0.95\n",
    "        self.updateLearningRateEveryTimesteps=10000000 #Aggiorna il learning rate ogni x steps dove x è il valore della variabile\n",
    "        self.csvPath=\"./rewards/\"+self.gameName+\"_rewards.csv\"\n",
    "        self.offsetCsv=0 #Usato per capire da quale riga iniziare a scrivere i rewards. Per non sovrascrivere i vecchi rewards.\n",
    "        self.batchSize=64\n",
    "        #Creo il file csv per salvare i rewards\n",
    "        if not os.path.isfile(self.csvPath):\n",
    "            data = {\"Epoch\": [], \"Average reward\": [], \"Min reward\": [], \"Max reward\": []}\n",
    "            df=pd.DataFrame(data,columns=[\"Epoch\",\"Average reward\",\"Min reward\",\"Max reward\"])\n",
    "            df.to_csv(self.csvPath,index=False,header=True)\n",
    "        else:\n",
    "            df=pd.read_csv(self.csvPath)\n",
    "            self.offsetCsv=len(df.index)\n",
    "        \n",
    "\n",
    "    def learn(self):\n",
    "        #passo 2 ciclare per k iterazioni.\n",
    "        stepsTot=0\n",
    "        iterazioniTot=0\n",
    "        while stepsTot<self.nTotalTimestamps:\n",
    "            self.updateLearningRate(stepsTot) \n",
    "            states, actions, rewards_to_go, log_probs, dones,len_ep =self.collect_trajectories()\n",
    "            stepsTot+=np.sum(len_ep)\n",
    "            iterazioniTot+=1\n",
    "            num_samples=np.sum(len_ep)\n",
    "            print(\"NUM SAMPLES:\",num_samples)\n",
    "            batch_size=64 #Faccio calcoli con mini-batches perchè altrimenti vado in Run out of memory di continuo.\n",
    "            samplesInPiu=num_samples%batch_size\n",
    "\n",
    "            if samplesInPiu!=0:\n",
    "                print(\"Errore: il numero di samples non è divisibile per il batch size\")\n",
    "                print(\"Quindi il primo batch sarà più grande di tutti gli altri\")\n",
    "            batch_size=self.batchSize+samplesInPiu\n",
    "            i=0\n",
    "        \n",
    "            while i <num_samples:\n",
    "                batch_states=states[i:i+batch_size]\n",
    "                batch_actions=actions[i:i+batch_size]\n",
    "                batch_rewards_to_go=rewards_to_go[i:i+batch_size]\n",
    "                batch_log_probs=log_probs[i:i+batch_size]\n",
    "                batch_dones=dones[i:i+batch_size]\n",
    "\n",
    "                V,latest_log_probs,_=self.evaluate(batch_states,batch_actions)\n",
    "                advantage=self.calcAdvantages(batch_rewards_to_go,V)\n",
    "                advantage, targets =self.compute_advantages_and_targets(batch_rewards_to_go, V, batch_dones)\n",
    "                n_updates=30\n",
    "                n=0\n",
    "                if n==n_updates:\n",
    "                    break\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    _,latest_log_probs,probs=self.evaluate(batch_states,batch_actions)\n",
    "                    policy_loss = self.getPolicyLoss(batch_log_probs,latest_log_probs,advantage)\n",
    "                    value_loss=tf.reduce_mean(tf.square(V-targets)) #MSE tra rewards to go e V\n",
    "\n",
    "                    #Aggiungo entropia alla loss per incentivare l'esplorazione\n",
    "                    entropy = -tf.reduce_mean(probs * tf.math.log(probs + 1e-10))\n",
    "                    total_loss=policy_loss+ value_loss*0.5 - entropy*self.entropyCoefficient\n",
    "                n+=1\n",
    "                gradientsPolicy = tape.gradient(total_loss, self.policyNN.trainable_variables)\n",
    "                self.policy_optimizer.apply_gradients(zip(gradientsPolicy, self.policyNN.trainable_variables))\n",
    "                if iterazioniTot%10==0:\n",
    "                    self.saveModel(\"ppo_\"+self.gameName+\".weights.h5\")\n",
    "                i+=batch_size\n",
    "                if samplesInPiu!=0:\n",
    "                    #Il primo giro l'ha fatto con un batch più grande, quindi devo fare un break per evitare di fare un doppio giro.\n",
    "                    batch_size=self.batchSize\n",
    "            print(\"EPOCA:\",iterazioniTot,\" TOTAL LOSS:\",total_loss,\" POLICY LOSS:\",policy_loss,\" VALUE LOSS:\",value_loss,\" ENTROPY:\",entropy)\n",
    "            self.evaluate_policy(epoch=iterazioniTot)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, episodes=10,epoch=0):\n",
    "        total_rewards = []\n",
    "        for _ in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            cumulative_reward = 0\n",
    "            while not done:\n",
    "                state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "                state_tensor = tf.expand_dims(state_tensor, axis=0)\n",
    "                probs, _ = self.policyNN(state_tensor)\n",
    "                action = np.argmax(probs.numpy())\n",
    "                state, reward, done, _ =self.env.step(action)\n",
    "                cumulative_reward += reward\n",
    "            total_rewards.append(cumulative_reward)\n",
    "        print(f\"Average Reward: {np.mean(total_rewards):.2f}\")\n",
    "        self.saveReward(np.mean(total_rewards),np.min(total_rewards),np.max(total_rewards),epoch,\"./rewards/\"+self.gameName+\"_rewards.csv\")\n",
    "\n",
    "    def collect_trajectories(self):\n",
    "        #Passo 3 --> Raccogliere un set di traiettorie D_k = {τ_i} con una policy pi_k = pi(theta_k)\n",
    "        #Dobbiamo raccogliere un set di traiettorie e per fare ciò dobbiamo raccogliere: stati, azioni, rewards, rewards to go, log_prob delle azioni.\n",
    "        batch={\n",
    "            'states':[],\n",
    "            'actions':[],\n",
    "            'rewards':[],\n",
    "            'rewards_to_go':[],\n",
    "            'log_probs':[],\n",
    "            'done':[],\n",
    "            'lengths':[]\n",
    "        }\n",
    "\n",
    "        t = 0 # Keeps track of how many timesteps we've run so far this batch\n",
    "        nEpisodes=0\n",
    "        while t < self.nTimestampsPerBatch:\n",
    "            rewardPerEpisode=[]\n",
    "            stato = self.env.reset()\n",
    "            done = False\n",
    "            frames=[]\n",
    "            for i in range(self.stepsPerEpisode):\n",
    "                t+=1\n",
    "                batch['states'].append(stato)\n",
    "                azione,log_prob=self.getAction(stato)\n",
    "                batch['actions'].append(azione)\n",
    "                batch['log_probs'].append(log_prob)\n",
    "                stato, reward, done ,_= self.env.step(azione)  #al posto di _ ci sarebbe info ma non ci serve\n",
    "                rewardPerEpisode.append(reward)\n",
    "                frames.append(stato)\n",
    "                batch['done'].append(done)\n",
    "                if done :\n",
    "                    break #Ha raggiunto il termine dell'episodio.\n",
    "            batch['rewards'].append(rewardPerEpisode)\n",
    "            batch['lengths'].append(i+1)\n",
    "            nEpisodes+=1\n",
    "            self.saveClip(frames,nEpisodes)\n",
    "            frames=[]\n",
    "        #Calcoliamo i rewards to go --> PASSO 4\n",
    "        batch['rewards_to_go']=self.calcRTG(batch['rewards'])\n",
    "        batch_statiTensor=tf.convert_to_tensor(batch['states'],dtype=tf.uint8)\n",
    "        batch_azioniTensor=tf.convert_to_tensor(batch['actions'],dtype=tf.int32)\n",
    "        batch_rewards_to_goTensor=tf.convert_to_tensor(batch['rewards_to_go'],dtype=tf.float32)\n",
    "        batch_log_probsTensor=tf.convert_to_tensor(batch['log_probs'],dtype=tf.float32)\n",
    "        batch_len=tf.convert_to_tensor(batch['lengths'],dtype=tf.int32)\n",
    "\n",
    "\n",
    "        return batch_statiTensor, batch_azioniTensor,batch_rewards_to_goTensor,batch_log_probsTensor, batch['done'],batch_len\n",
    "\n",
    "    def getAction(self,stato):\n",
    "        stato=tf.convert_to_tensor(np.expand_dims(stato, axis=0) ,dtype=tf.float32)# Diventa (1, 64, 64, 3)\n",
    "        azione_pred,_=self.policyNN(stato)\n",
    "        #Somma probabilità\n",
    "        dist=tfp.distributions.Categorical(probs=tf.squeeze(azione_pred))\n",
    "        azionePresa=dist.sample()\n",
    "        log_prob=dist.log_prob(azionePresa)\n",
    "        return azionePresa, tf.stop_gradient(log_prob)\n",
    "\n",
    "    def calcRTG(self,rewards):\n",
    "        #Prendo la formula per calcolare i rewards to go e richiede i cumulative rewards e un fattore di sconto.\n",
    "        rtg=[]\n",
    "        for episode_reward in reversed(rewards):\n",
    "            cumulative_reward=0\n",
    "            totalRewardPerEpisode=0\n",
    "            for single_reward in reversed(episode_reward):\n",
    "                cumulative_reward=single_reward+cumulative_reward*self.gamma\n",
    "                totalRewardPerEpisode+=single_reward\n",
    "                rtg.append(cumulative_reward)\n",
    "            print(\"Total reward per episode:\",totalRewardPerEpisode)\n",
    "        return tf.convert_to_tensor(rtg,dtype=tf.float32)\n",
    "\n",
    "    def calcAdvantages(self, rtg,values):\n",
    "        advantages=rtg-tf.stop_gradient(values)\n",
    "        return (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-10)\n",
    "    \n",
    "    def calcGAE(self, rewards, values):\n",
    "      gae = 0\n",
    "      advantages = []\n",
    "      for t in reversed(range(len(rewards))):\n",
    "        if t+1 < len(rewards):\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] - values[t]\n",
    "        else:\n",
    "            delta = rewards[t] - values[t]\n",
    "        gae = delta + self.gamma * self.lambdaGAE * gae\n",
    "        advantages.insert(0, gae) \n",
    "        return tf.convert_to_tensor(advantages, dtype=tf.float32)       \n",
    "       \n",
    "    def compute_advantages_and_targets(self,rewards:list, values:list, dones:list):\n",
    "        advantages = []\n",
    "        targets = []\n",
    "        advantage = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            #Se una delle variabili è solo un valore, allora non posso fare slicing e devo fare un controllo.\n",
    "            try:\n",
    "                if len(rewards) == 1:\n",
    "                    singleReward=rewards\n",
    "                else:\n",
    "                    singleReward=rewards[t]\n",
    "                \n",
    "                if len(dones) == 1:\n",
    "                    singleDone=dones\n",
    "                else:\n",
    "                    singleDone=dones[t]\n",
    "                \n",
    "                if len(values) == 1:\n",
    "                    singleValue=values\n",
    "                    nextValue=0\n",
    "                else:\n",
    "                    singleValue=values[t]\n",
    "                    if t +1 == len(rewards):\n",
    "                        nextValue=0\n",
    "                    else:\n",
    "                        nextValue=values[t+1]\n",
    "\n",
    "                delta = singleReward + (1 - singleDone) * self.gamma * nextValue - singleValue\n",
    "                advantage = delta + self.gamma * self.lambdaGAE * (1 - singleDone) * advantage\n",
    "                advantages.insert(0, advantage)\n",
    "                targets.insert(0, advantage + singleValue)\n",
    "\n",
    "            except:\n",
    "                print(\"Errore nel calcolo delle advantage e targets\")\n",
    "                print(\"INPUT RICEVUTI:\")\n",
    "                print(\"REWARDS:\",rewards)\n",
    "                print(\"VALUES:\",values)\n",
    "                print(\"DONES:\",dones)\n",
    "                print(\"T:\",t)\n",
    "        return tf.convert_to_tensor(advantages, dtype=tf.float32), tf.convert_to_tensor(targets, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    def getPolicyLoss(self,log_probs_old, log_probs_new, advantages):\n",
    "        advantages = tf.stop_gradient(advantages)\n",
    "        policy_ratio = tf.exp(log_probs_new-log_probs_old)\n",
    "        surrogated_loss_1 = policy_ratio * advantages\n",
    "        clipped_policy_ratio=tf.clip_by_value(policy_ratio, clip_value_min=1.0-self.epsilon, clip_value_max=1.0+self.epsilon)\n",
    "        surrogated_loss_2 = clipped_policy_ratio * advantages\n",
    "        clip_loss=tf.minimum(surrogated_loss_1,surrogated_loss_2)\n",
    "        return -tf.reduce_mean(clip_loss)\n",
    "\n",
    "    def evaluate(self, batch_states,batch_actions):\n",
    "        batch_states=tf.cast(batch_states, tf.float32)\n",
    "        mean,retVal=self.policyNN(batch_states)\n",
    "        V= tf.squeeze(retVal)\n",
    "        dist=tfp.distributions.Categorical(probs=mean)\n",
    "        log_probs=dist.log_prob(batch_actions)\n",
    "        return V, log_probs, mean\n",
    "\n",
    "    def loadModel(self, path):\n",
    "        if path is \"\":\n",
    "            return\n",
    "        self.policyNN.build(self.nStati)\n",
    "        try:\n",
    "            self.policyNN.load_weights(path)\n",
    "        except:\n",
    "            print(\"Errore nel caricamento del modello\")\n",
    "\n",
    "    def saveModel(self, path):\n",
    "        self.policyNN.save_weights(path)\n",
    "    \n",
    "    def updateLearningRate(self, epoch):\n",
    "      if epoch % self.updateLearningRateEveryTimesteps == 0 and epoch > 0:\n",
    "        self.learningRate *= 0.9  # Riduci il learning rate del 10%\n",
    "        self.policy_optimizer.learning_rate = self.learningRate #Aggiorno solo dentro l'if che tanto è uguale per tutte le altre volte.   \n",
    "\n",
    "    def saveReward(self,reward,minReward,maxReward,epoch,path):\n",
    "        #Devo controllare se c'è davvero il file o meno. In caso affermativo conto quante righe ci sono.Da li ci sarà un offset così da incrementare correttamente\n",
    "        epoch+=self.offsetCsv        \n",
    "        data = {\"Epoch\": [epoch], \"Average reward\": [reward], \"Min reward\": [minReward], \"Max reward\": [maxReward]}\n",
    "        df=pd.DataFrame(data,columns=[\"Epoch\",\"Average reward\",\"Min reward\",\"Max reward\"])\n",
    "        df.to_csv(path,mode='a',index=False,header=False)\n",
    "\n",
    "    def showGraph(self):\n",
    "        rewards=pd.read_csv(\"./rewards/\"+self.gameName+\"_rewards.csv\")\n",
    "        rewards.plot(x='Epoch',y='Average reward',kind='line',title=\"Average reward per epoch\")\n",
    "        plt.show()\n",
    "    \n",
    "    def saveClip(self,frames,i):\n",
    "        clip = ImageSequenceClip(list(frames), fps=15)\n",
    "        nameVideo=\"./clip/\"+self.gameName+\"/\"+self.gameName+\"_video\"+str(i)+\".mp4\"\n",
    "        clip.write_videofile(nameVideo, fps=15,logger=None)\n",
    "        Video(nameVideo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "MRQw7a59YaKK",
    "outputId": "9f1bdaeb-13fd-483f-a5ae-331656852210"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore nel caricamento del modello\n",
      "Total reward per episode: 4.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 2.0\n",
      "Total reward per episode: 2.0\n",
      "Total reward per episode: 1.0\n",
      "Total reward per episode: 4.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 4.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 2.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 1.0\n",
      "Total reward per episode: 4.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 5.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 3.0\n",
      "Total reward per episode: 2.0\n",
      "Total reward per episode: 2.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 1.0\n",
      "Total reward per episode: 1.0\n",
      "Total reward per episode: 2.0\n",
      "Total reward per episode: 2.0\n",
      "Total reward per episode: 3.0\n",
      "Total reward per episode: 6.0\n",
      "Total reward per episode: 1.0\n",
      "Total reward per episode: 7.0\n",
      "Total reward per episode: 3.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 7.0\n",
      "Total reward per episode: 3.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 1.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 4.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 1.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 3.0\n",
      "Total reward per episode: 3.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 2.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 3.0\n",
      "Total reward per episode: 3.0\n",
      "Total reward per episode: 1.0\n",
      "Total reward per episode: 2.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 7.0\n",
      "Total reward per episode: 3.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 2.0\n",
      "Total reward per episode: 1.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 1.0\n",
      "Total reward per episode: 1.0\n",
      "Total reward per episode: 2.0\n",
      "Total reward per episode: 2.0\n",
      "Total reward per episode: 3.0\n",
      "Total reward per episode: 6.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 2.0\n",
      "Total reward per episode: 5.0\n",
      "Total reward per episode: 4.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 1.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 1.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 2.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 1.0\n",
      "Total reward per episode: 1.0\n",
      "Total reward per episode: 7.0\n",
      "Total reward per episode: 3.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 2.0\n",
      "NUM SAMPLES: 8228\n",
      "Errore: il numero di samples non è divisibile per il batch size\n",
      "Quindi il primo batch sarà più grande di tutti gli altri\n",
      "N: 1\n",
      "I: 0\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(403.722, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-25.276304, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(858.0002, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17967397, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 100\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.12145083, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.13267714, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.026030906, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17891407, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 164\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(78.07083, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-11.429928, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(179.00508, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17867437, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 228\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(111.24769, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-13.159784, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(248.81854, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18026882, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 292\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(25.5567, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-7.127123, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(65.3712, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17781548, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 356\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(196.65514, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-16.238853, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(425.79144, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.1731341, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 420\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(92.11518, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-7.605743, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(199.44542, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17873693, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 484\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(258.04953, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-22.745064, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(561.5928, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17972404, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 548\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(321.95157, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-24.227783, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(692.3623, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17891303, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 612\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(15.396313, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-3.1746306, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(37.145496, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18040293, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 676\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.05493237, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.055551164, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.0048385123, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18004636, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 740\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(21.016275, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-4.92809, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(51.892338, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18032923, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 804\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(66.33607, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-8.593984, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(149.86371, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.1803363, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 868\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(20.840502, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-6.143299, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(53.971203, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18011777, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 932\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(6.785261, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-3.4877405, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(20.549608, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18022293, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 996\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(85.15009, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-9.104617, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(188.51299, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17858925, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 1060\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(216.36505, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-19.720644, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(472.175, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17929369, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 1124\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(165.14598, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-15.657202, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(361.60995, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17996684, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 1188\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.09316245, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.09943755, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.016137512, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17936517, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 1252\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.032819644, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.031842396, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.0016467365, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18006137, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 1316\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(151.49036, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-17.08319, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(337.1507, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17981645, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 1380\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(74.519295, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-7.153182, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(163.34854, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.1791148, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 1444\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.06780149, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.06893979, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.0058557326, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17895718, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 1508\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.06005743, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.060861476, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.005193594, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17927508, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 1572\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.05162351, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.05224652, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.004832489, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17932345, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 1636\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.049928866, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.05002957, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.003786872, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17927323, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 1700\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.048054907, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.04854443, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.004564417, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17926878, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 1764\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(162.75238, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-15.055504, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(355.6194, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18004483, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 1828\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(42.991158, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-8.894189, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(103.77429, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18001226, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 1892\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(95.39823, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-13.411916, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(217.6239, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18000087, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 1956\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(4.769373, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-2.3767562, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(14.295856, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17992261, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2020\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(91.649765, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-12.257185, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(207.8175, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18027191, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2084\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.014829787, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.0135804145, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.0011007573, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17997523, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2148\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(34.387363, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-8.289661, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(85.35765, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18020725, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2212\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(38.605194, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-9.388905, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(95.9918, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18009265, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2276\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(36.080414, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-8.030781, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(88.22598, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17965446, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2340\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(108.66648, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-14.155967, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(245.6485, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.1799446, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2404\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(72.83497, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-12.3680525, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(170.40964, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18009481, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2468\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(190.92964, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-18.761356, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(419.3856, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17986505, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2532\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(740.91595, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-32.998993, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(1547.8334, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17995773, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2596\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(218.63675, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-12.093171, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(461.46344, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17997943, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2660\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(17.752645, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-5.8351436, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(47.179176, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17988342, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2724\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(257.3594, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-21.927784, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(558.578, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18009853, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2788\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(1216.4672, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-47.566406, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(2528.0708, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17991851, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2852\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(224.46956, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-19.06316, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(487.06903, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17990007, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2916\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(59.009293, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-7.458923, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(132.94003, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17992458, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 2980\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.0125441365, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.0109437, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00040301814, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18019459, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 3044\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(23.656559, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-5.6173654, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(58.55145, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17997544, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 3108\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(715.94147, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-37.66194, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(1507.2104, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18011172, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 3172\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(573.76324, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-26.817871, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(1201.1658, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18004212, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 3236\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(187.66962, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-17.909203, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(411.16122, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17994004, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 3300\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(9.137628, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-4.101411, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(26.481676, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17998461, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 3364\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(16.260912, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-6.198991, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(44.9234, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17982903, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 3428\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(1.5352341, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-2.1636124, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(7.40129, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17984553, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 3492\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(273.00964, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-21.074146, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(588.1712, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.1792819, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 3556\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(89.38404, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-11.071782, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(200.91524, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17904176, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 3620\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(1.3123461, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-1.9670846, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(6.5624404, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17894633, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 3684\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(17.587435, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-5.0464334, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(45.271336, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17988274, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 3748\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.0007050083, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(0.002224803, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00055896444, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17992769, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 3812\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.00029336626, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(0.0018876414, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00040432738, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17964387, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 3876\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.0049910964, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(0.0064823655, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00060613523, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17943366, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 3940\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.009326889, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(0.010810178, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.0006161537, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17913653, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4004\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.006526356, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(0.008116022, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00041060318, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17949675, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4068\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(1.1923503, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-1.9889479, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(6.3661866, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17951782, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4132\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(148.90733, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-17.775312, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(333.3689, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17961034, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4196\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(201.86163, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-18.679787, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(441.08646, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.1801502, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4260\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(30.236412, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-4.742209, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(69.96084, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.1797897, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4324\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.0091090845, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.0074023074, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00019014353, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18018487, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4388\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.0058684684, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(0.0075304895, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00026361254, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17938274, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4452\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(66.14558, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-11.079222, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(154.45319, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17967138, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4516\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(39.2116, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-6.5528975, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(91.53258, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17895794, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4580\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.009043092, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.0073035844, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00011859301, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17988038, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4644\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.0027802018, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.0011897858, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00042158537, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18012089, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4708\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.0008359679, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(0.00058219605, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.000756035, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17961815, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4772\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(238.25432, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-18.02897, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(512.5702, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18024705, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4836\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(262.65744, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-22.23333, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(569.78516, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.1799948, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4900\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(103.78877, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-13.110394, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(233.80193, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.1803159, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 4964\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(15.591568, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-5.2763805, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(41.7395, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18006541, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5028\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.00985898, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.008130716, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.0001474419, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18019848, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5092\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(103.54702, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-14.167833, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(235.4333, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18019086, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5156\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(5.2497845, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-1.541552, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(13.586277, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18021296, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5220\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(565.17633, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-32.42173, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(1195.1997, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17999958, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5284\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(1126.2003, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-45.621086, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(2343.6465, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17975573, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5348\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(167.2393, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-16.044191, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(366.5706, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17988028, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5412\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(148.67406, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-13.873651, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(325.09903, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.1800291, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5476\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(54.948315, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-7.3300753, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(124.56038, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18006642, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5540\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(45.65145, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-6.7879124, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(104.882324, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18008938, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5604\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(37.063694, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-8.505583, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(91.14216, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18032089, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5668\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.20335215, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.39897633, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(1.2082583, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18006559, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5732\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.004210435, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.0024909012, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00016504142, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18020548, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5796\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.008753955, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.0070041763, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00010769773, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18036287, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5860\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.0034902897, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.001799918, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00022519694, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18029703, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5924\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.0071898913, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.0054897433, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00020767709, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18039867, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 5988\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(7.382754, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-3.6589487, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(22.087008, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18011345, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 6052\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(22.506876, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-6.6795287, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(58.376404, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17963189, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 6116\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(14.034082, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-5.7805605, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(39.63288, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17972545, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 6180\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(83.52917, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-9.781129, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(186.62418, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17966631, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 6244\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(140.96776, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-15.349201, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(312.6375, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18005067, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 6308\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(47.03362, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-9.89196, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(113.85475, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17969708, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 6372\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(121.91369, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-14.699514, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(273.23, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17973499, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 6436\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(4.8317237, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-1.1124653, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(11.891973, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17971012, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 6500\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(559.7541, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-33.472504, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(1186.4567, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17961258, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 6564\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(548.28107, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-25.499584, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(1147.5648, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17838962, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 6628\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(-0.009327376, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.9833671, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(1.9516488, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17847067, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 6692\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(91.03935, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-12.7436695, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(207.56961, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17878474, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 6756\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(450.5021, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-30.351566, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(961.71094, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18023802, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 6820\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(182.01837, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-18.645943, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(401.3322, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17946917, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 6884\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(225.18654, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-17.967104, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(486.31088, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18016349, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 6948\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.012859162, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(0.014395327, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.0005255792, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.1798955, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7012\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.010459994, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(0.012133521, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00024447698, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17957662, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7076\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(13.623988, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-5.248813, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(37.749195, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.1796208, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7140\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.0031604024, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(2.0861626e-05, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.009872049, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17964838, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7204\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.021939788, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(0.02334796, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.0007798158, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17980811, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7268\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.010922734, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(0.012406526, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00063261564, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18001004, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7332\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.023741325, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(0.025110902, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.000859826, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17994896, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7396\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(15.667876, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-5.4544992, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(42.248344, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17970824, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7460\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.020503635, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(0.021762952, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.0010760837, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17973591, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7524\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(103.88927, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-13.747768, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(235.27768, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17996602, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7588\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(8.495109, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-2.6217434, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(22.2373, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17986968, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7652\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.018840361, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(0.020413376, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.00045010448, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17980677, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7716\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(15.095781, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-4.7532263, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(39.701614, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17992124, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7780\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.0050919782, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-1.0333171, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(2.0804217, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18017758, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7844\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(101.6667, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-14.000607, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(231.33823, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18013586, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7908\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(815.3954, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-37.345444, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(1705.4854, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18005723, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 7972\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(325.8026, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-22.39211, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(696.39307, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18020065, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 8036\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(4.590631, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-1.1232094, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(11.431284, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18015073, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 8100\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(0.0075987773, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-0.4625677, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.9439321, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17995937, shape=(), dtype=float32)\n",
      "N: 1\n",
      "I: 8164\n",
      "EPOCA: 1  TOTAL LOSS: tf.Tensor(166.62, shape=(), dtype=float32)  POLICY LOSS: tf.Tensor(-18.046516, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(369.33664, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.18020079, shape=(), dtype=float32)\n",
      "Average Reward: 0.80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#load model weights if available \u001b[39;00m\n\u001b[1;32m     15\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39mloadModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mgameName\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#save model weights\u001b[39;00m\n\u001b[1;32m     19\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39msaveModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mgameName\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 41\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m stepsTot\u001b[38;5;241m<\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnTotalTimestamps:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdateLearningRate(stepsTot) \n\u001b[0;32m---> 41\u001b[0m     states, actions, rewards_to_go, log_probs, dones,len_ep \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     stepsTot\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(len_ep)\n\u001b[1;32m     43\u001b[0m     iterazioniTot\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[11], line 135\u001b[0m, in \u001b[0;36mPPO.collect_trajectories\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(stato)\n\u001b[0;32m--> 135\u001b[0m azione,log_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstato\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(azione)\n\u001b[1;32m    137\u001b[0m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_probs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(log_prob)\n",
      "Cell \u001b[0;32mIn[11], line 164\u001b[0m, in \u001b[0;36mPPO.getAction\u001b[0;34m(self, stato)\u001b[0m\n\u001b[1;32m    162\u001b[0m azione_pred,_\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicyNN(stato)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m#Somma probabilità\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m dist\u001b[38;5;241m=\u001b[39m\u001b[43mtfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mazione_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m azionePresa\u001b[38;5;241m=\u001b[39mdist\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m    166\u001b[0m log_prob\u001b[38;5;241m=\u001b[39mdist\u001b[38;5;241m.\u001b[39mlog_prob(azionePresa)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py:342\u001b[0m, in \u001b[0;36m_DistributionMeta.__new__.<locals>.wrapped_init\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Note: if we ever want to have things set in `self` before `__init__` is\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# called, here is the place to do it.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m self_\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m \u001b[43mdefault_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Note: if we ever want to override things set in `self` by subclass\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# `__init__`, here is the place to do it.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m   \u001b[38;5;66;03m# We prefer subclasses will set `parameters = dict(locals())` because\u001b[39;00m\n\u001b[1;32m    347\u001b[0m   \u001b[38;5;66;03m# this has nearly zero overhead. However, failing to do this, we will\u001b[39;00m\n\u001b[1;32m    348\u001b[0m   \u001b[38;5;66;03m# resolve the input arguments dynamically and only when needed.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow_probability/python/distributions/categorical.py:198\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, logits, probs, dtype, force_probs_to_zero_outside_support, validate_args, allow_nan_stats, name)\u001b[0m\n\u001b[1;32m    196\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMust pass probs or logits, but not both.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(name) \u001b[38;5;28;01mas\u001b[39;00m name:\n\u001b[0;32m--> 198\u001b[0m   prob_logit_dtype \u001b[38;5;241m=\u001b[39m \u001b[43mdtype_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommon_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probs \u001b[38;5;241m=\u001b[39m tensor_util\u001b[38;5;241m.\u001b[39mconvert_nonref_to_tensor(\n\u001b[1;32m    200\u001b[0m       probs, dtype_hint\u001b[38;5;241m=\u001b[39mprob_logit_dtype, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logits \u001b[38;5;241m=\u001b[39m tensor_util\u001b[38;5;241m.\u001b[39mconvert_nonref_to_tensor(\n\u001b[1;32m    202\u001b[0m       logits, dtype_hint\u001b[38;5;241m=\u001b[39mprob_logit_dtype, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow_probability/python/internal/dtype_util.py:184\u001b[0m, in \u001b[0;36mcommon_dtype\u001b[0;34m(args, dtype_hint)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mis_nested(dtype_hint):\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;66;03m# Flatten only the top layer of the `args` structure.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m   shallow_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mget_traverse_shallow_structure(\n\u001b[1;32m    183\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mis\u001b[39;00m args, args)\n\u001b[0;32m--> 184\u001b[0m flattened_args \u001b[38;5;241m=\u001b[39m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_up_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshallow_structure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m seen \u001b[38;5;241m=\u001b[39m [_NOT_YET_SEEN] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(flattened_args)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(flattened_args):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/nest.py:843\u001b[0m, in \u001b[0;36mflatten_up_to\u001b[0;34m(shallow_tree, input_tree, check_types, expand_composites)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__internal__.nest.flatten_up_to\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflatten_up_to\u001b[39m(shallow_tree, input_tree, check_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    766\u001b[0m                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    767\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Flattens `input_tree` up to `shallow_tree`.\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \n\u001b[1;32m    769\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;124;03m      `input_tree`.\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 843\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_up_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCORE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshallow_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcheck_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1537\u001b[0m, in \u001b[0;36mflatten_up_to\u001b[0;34m(modality, shallow_tree, input_tree, check_types, expand_composites)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Flattens `input_tree` up to `shallow_tree`.\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m \n\u001b[1;32m   1459\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;124;03m    `input_tree`.\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[0;32m-> 1537\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_flatten_up_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshallow_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[1;32m   1541\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_flatten_up_to(shallow_tree, input_tree)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1554\u001b[0m, in \u001b[0;36m_tf_core_flatten_up_to\u001b[0;34m(shallow_tree, input_tree, check_types, expand_composites)\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tf_core_flatten_up_to\u001b[39m(\n\u001b[1;32m   1549\u001b[0m     shallow_tree, input_tree, check_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m ):\n\u001b[1;32m   1551\u001b[0m   is_nested_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1552\u001b[0m       _is_nested_or_composite \u001b[38;5;28;01mif\u001b[39;00m expand_composites \u001b[38;5;28;01melse\u001b[39;00m _tf_core_is_nested\n\u001b[1;32m   1553\u001b[0m   )\n\u001b[0;32m-> 1554\u001b[0m   \u001b[43m_tf_core_assert_shallow_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshallow_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcheck_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_composites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1560\u001b[0m   \u001b[38;5;66;03m# Discard paths returned by nest_util._tf_core_yield_flat_up_to.\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m   1562\u001b[0m       v\n\u001b[1;32m   1563\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m _tf_core_yield_flat_up_to(\n\u001b[1;32m   1564\u001b[0m           shallow_tree, input_tree, is_nested_fn\n\u001b[1;32m   1565\u001b[0m       )\n\u001b[1;32m   1566\u001b[0m   ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1400\u001b[0m, in \u001b[0;36m_tf_core_assert_shallow_structure\u001b[0;34m(shallow_tree, input_tree, check_types, expand_composites)\u001b[0m\n\u001b[1;32m   1392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1393\u001b[0m         SHALLOW_TREE_HAS_INVALID_KEYS\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28msorted\u001b[39m(absent_keys))\n\u001b[1;32m   1394\u001b[0m     )\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shallow_branch, input_branch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m   1397\u001b[0m     _tf_core_yield_value(shallow_tree),\n\u001b[1;32m   1398\u001b[0m     _tf_core_yield_value(input_tree),\n\u001b[1;32m   1399\u001b[0m ):\n\u001b[0;32m-> 1400\u001b[0m   \u001b[43m_tf_core_assert_shallow_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshallow_branch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_branch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcheck_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_composites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1284\u001b[0m, in \u001b[0;36m_tf_core_assert_shallow_structure\u001b[0;34m(shallow_tree, input_tree, check_types, expand_composites)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tf_core_assert_shallow_structure\u001b[39m(\n\u001b[1;32m   1279\u001b[0m     shallow_tree, input_tree, check_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m ):\n\u001b[1;32m   1281\u001b[0m   is_nested_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1282\u001b[0m       _is_nested_or_composite \u001b[38;5;28;01mif\u001b[39;00m expand_composites \u001b[38;5;28;01melse\u001b[39;00m _tf_core_is_nested\n\u001b[1;32m   1283\u001b[0m   )\n\u001b[0;32m-> 1284\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_nested_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshallow_tree\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_nested_fn(input_tree):\n\u001b[1;32m   1286\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1287\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf shallow structure is a sequence, input must also be a sequence. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1288\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput has type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1289\u001b[0m           \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(input_tree)\n\u001b[1;32m   1290\u001b[0m       )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configurazione ed esecuzione\n",
    "#Lista di giochi a disposizione di Procgen:\n",
    "\"\"\" \n",
    "    bigfish, bossfight, caveflyer, chaser, climber\n",
    "    coinrun, dodgeball, fruitbot, heist, jumper\n",
    "    leaper, maze, miner, ninja, plumber, starpilot\n",
    "\"\"\"\n",
    "gameName=\"starpilot\" #Scelto starpilot perchè è un gioco che ha episode corti, quindi allenamenti più rapidi.\n",
    "env = gym.make('procgen:procgen-'+gameName+'-v0',distribution_mode='easy', num_levels=100)\n",
    "\n",
    "#Creo l'oggetto PPO\n",
    "ppo_model=PPO(env,gameName)\n",
    "\n",
    "#load model weights if available \n",
    "ppo_model.loadModel(\"ppo_\"+gameName+\".weights.h5\")\n",
    "ppo_model.learn()\n",
    "\n",
    "#save model weights\n",
    "ppo_model.saveModel(\"ppo_\"+gameName+\".weights.h5\")\n",
    "\n",
    "ppo_model.showGraph()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
