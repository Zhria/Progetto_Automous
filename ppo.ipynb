{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sd2DHK4QYaKH"
   },
   "source": [
    "This module contains the implementation of the PPO algorithm.\n",
    "Ci basiamo sullo pseudocodice presente sul sito di OpenAI per la realizzazione del ppo.\n",
    "https://spinningup.openai.com/en/latest/algorithms/ppo.html#id7\n",
    "Utilizzando un Actor-Critic Method.\n",
    "Ciò suddivide l'implementazione in 8 passi principali:\n",
    "1. Inizializzazione dell'ambiente con policy parameters theta_0, e l'inizial value function parameters w_0.\n",
    "2. Ciclare per k iterazioni\n",
    "3. Raccogliere un set di traiettorie D_k = {τ_i} con una policy pi_k = pi(theta_k)\n",
    "4. Calcolare i reward-to-go R_t\n",
    "5. Calcolare gli advantage estimates A_t basandoci sulla value function V_{w_k}\n",
    "6. Aggiornare la policy massimizzando la PPO-Clip objective (Gradient ascent con adam) . Non scriverò la formula che è complessa\n",
    "7. Aggiornare la value function minimizzando la MSE tra V_{w_k} e R_t (Gradient descent con adam)\n",
    "8. Fine ciclo.\n",
    "\n",
    "Implementiamo tutti i passi nella funzione learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pqY5vJI-YaKI",
    "outputId": "77884fa9-5efe-4955-a667-c9a425a073ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-27 19:32:38.435201: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735324358.450842  253168 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735324358.455292  253168 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-27 19:32:38.471065: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #ignora warnings\n",
    "#Check if colab is used:\n",
    "from rete import ReteNeurale\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glfw\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print(\"Not running on CoLab\")\n",
    "  #print list of GPUs\n",
    "  #tf. config. list_physical_devices('GPU')\n",
    "  print(\"Devices: \", tf.config.list_physical_devices())\n",
    "  print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "if IN_COLAB:\n",
    "  !pip install procgen\n",
    "  !pip install tensorflow_probability\n",
    "  !pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f9EeZ0iE56HQ",
    "outputId": "3798c572-d4c3-4c01-d797-ac029befc27e"
   },
   "outputs": [],
   "source": [
    "#Tutta questa parte è relativa alla visualizzazione del gioco tramite salvataggio degli stati di un episode in una specifica cartella.\n",
    "#Tutto questo viene fatto perchè non si riesce a visualizzare procgen in live per problemi con glfw e openGL su Ubuntu.\n",
    "from moviepy import ImageSequenceClip\n",
    "from IPython.display import Video\n",
    "import os\n",
    "from pyvirtualdisplay.smartdisplay import SmartDisplay\n",
    "display = SmartDisplay(visible=0, size=(1920,1080),fbdir='/tmp')\n",
    "display.start()\n",
    "glfw.init()\n",
    "available_fbconfigs = glfw.get_video_modes(glfw.get_primary_monitor())\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'osmesa'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohXWluKCYaKJ"
   },
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self,env,gameName,totalSteps=2000000):\n",
    "        self.env=env\n",
    "        self.gameName=gameName\n",
    "        self.nAzioni=env.action_space.n\n",
    "        self.nStati=env.observation_space.shape\n",
    "        self.listaAzioni=[i for i in range(self.nAzioni)]\n",
    "        self.nTimestampsPerBatch=8192\n",
    "        #self.nTimestampsPerBatch=4096\n",
    "        self.stepsPerEpisode=512\n",
    "        self.nTotalTimestamps=totalSteps\n",
    "        self.episodesPerBatch=10\n",
    "        self.nEpoche=300\n",
    "        self.gamma=0.99\n",
    "        self.epsilon=0.2\n",
    "        self.learningRate=1e-4\n",
    "        self.policyNN=ReteNeurale(self.nStati,self.nAzioni) #Actor\n",
    "        self.policy_optimizer=keras.optimizers.Adam(learning_rate=self.learningRate, clipnorm=1.0)\n",
    "        self.policyNN.compile(optimizer=self.policy_optimizer)\n",
    "        self.entropyCoefficient=0.01 #Per invogliare l'esplorazione un po di più.\n",
    "        self.lambdaGAE=0.95\n",
    "        self.updateLearningRateEveryTimesteps=10000000 #Aggiorna il learning rate ogni x steps dove x è il valore della variabile\n",
    "        self.csvPath=\"./rewards/\"+self.gameName+\"_rewards.csv\"\n",
    "        self.offsetCsv=0 #Usato per capire da quale riga iniziare a scrivere i rewards. Per non sovrascrivere i vecchi rewards.\n",
    "        self.batchSize=512\n",
    "        #Creo il file csv per salvare i rewards\n",
    "        if not os.path.isfile(self.csvPath):\n",
    "            data = {\"Epoch\": [], \"Average reward\": [], \"Min reward\": [], \"Max reward\": []}\n",
    "            df=pd.DataFrame(data,columns=[\"Epoch\",\"Average reward\",\"Min reward\",\"Max reward\"])\n",
    "            df.to_csv(self.csvPath,index=False,header=True)\n",
    "        else:\n",
    "            df=pd.read_csv(self.csvPath)\n",
    "            self.offsetCsv=len(df.index)\n",
    "        \n",
    "\n",
    "    def learn(self):\n",
    "        #passo 2 ciclare per k iterazioni.\n",
    "        stepsTot=0\n",
    "        iterazioniTot=0\n",
    "        while stepsTot<self.nTotalTimestamps:\n",
    "            print(\"Step totali eseguiti: {}\".format(stepsTot),\" Step totali rimasti:\",self.nTotalTimestamps-stepsTot)\n",
    "            self.updateLearningRate(stepsTot) \n",
    "            states, actions, rewards_to_go, log_probs, dones,len_ep =self.collect_trajectories()\n",
    "            stepsTot+=np.sum(len_ep)\n",
    "            iterazioniTot+=1\n",
    "            num_samples=np.sum(len_ep)\n",
    "            print(\"NUM SAMPLES:\",num_samples)\n",
    "\n",
    "            samplesInPiu=num_samples%self.batchSize\n",
    "            batch_size=self.batchSize+samplesInPiu\n",
    "\n",
    "            i=0\n",
    "            while i <num_samples:\n",
    "                batch_states=states[i:i+batch_size]\n",
    "                batch_actions=actions[i:i+batch_size]\n",
    "                batch_rewards_to_go=rewards_to_go[i:i+batch_size]\n",
    "                batch_log_probs=log_probs[i:i+batch_size]\n",
    "                batch_dones=dones[i:i+batch_size]\n",
    "\n",
    "                v,latest_log_probs,_=self.evaluate(batch_states,batch_actions)\n",
    "                #advantage=self.calcAdvantages(batch_rewards_to_go,V)\n",
    "                advantage, targets =self.calcGaeAndTargets(batch_rewards_to_go, v, batch_dones)\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    _,latest_log_probs,probs=self.evaluate(batch_states,batch_actions)\n",
    "                    policy_loss = self.getPolicyLoss(batch_log_probs,latest_log_probs,advantage)\n",
    "                    #MSE valueLoss\n",
    "                    value_loss = tf.reduce_mean(tf.square(targets - v))\n",
    "                    \n",
    "                    #Aggiungo entropia alla loss per incentivare l'esplorazione\n",
    "                    entropy = -tf.reduce_mean(probs * tf.math.log(probs + 1e-10))\n",
    "                    total_loss=policy_loss+ value_loss*0.5 - entropy*self.entropyCoefficient\n",
    "                gradientsPolicy = tape.gradient(total_loss, self.policyNN.trainable_variables)\n",
    "                self.policy_optimizer.apply_gradients(zip(gradientsPolicy, self.policyNN.trainable_variables))\n",
    "                i+=batch_size\n",
    "                batch_size=self.batchSize\n",
    "                print(\"EPOCA:\",iterazioniTot,\" TOTAL LOSS:\",total_loss.numpy(),\" POLICY LOSS:\",policy_loss.numpy(),\" VALUE LOSS:\",value_loss.numpy(),\" ENTROPY:\",entropy.numpy())\n",
    "            self.evaluate_policy(epoch=iterazioniTot)\n",
    "            if iterazioniTot%10==0:\n",
    "                self.saveModel(\"ppo_\"+self.gameName+\".weights.h5\")\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, episodes=10,epoch=0):\n",
    "        total_rewards = []\n",
    "        for i in range(episodes):\n",
    "            frames=[]\n",
    "\n",
    "            state = self.env.reset()\n",
    "            frames.append(state)\n",
    "            done = False\n",
    "            cumulative_reward = 0\n",
    "            while not done:\n",
    "                state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "                state_tensor = tf.expand_dims(state_tensor, axis=0)\n",
    "                probs, _ = self.policyNN(state_tensor)\n",
    "                action = np.argmax(probs.numpy())\n",
    "                state, reward, done, _ =self.env.step(action)\n",
    "                frames.append(state)\n",
    "\n",
    "                cumulative_reward += reward\n",
    "            total_rewards.append(cumulative_reward)\n",
    "            self.saveClip(frames,i)\n",
    "            \n",
    "            print(\"Episode reward:\", cumulative_reward)\n",
    "        print(f\"Average Reward: {np.mean(total_rewards):.2f}\")\n",
    "        self.saveReward(np.mean(total_rewards),np.min(total_rewards),np.max(total_rewards),epoch,\"./rewards/\"+self.gameName+\"_rewards.csv\")\n",
    "\n",
    "    def collect_trajectories(self):\n",
    "        #Passo 3 --> Raccogliere un set di traiettorie D_k = {τ_i} con una policy pi_k = pi(theta_k)\n",
    "        #Dobbiamo raccogliere un set di traiettorie e per fare ciò dobbiamo raccogliere: stati, azioni, rewards, rewards to go, log_prob delle azioni.\n",
    "        batch={\n",
    "            'states':[],\n",
    "            'actions':[],\n",
    "            'rewards':[],\n",
    "            'rewards_to_go':[],\n",
    "            'log_probs':[],\n",
    "            'done':[],\n",
    "            'lengths':[]\n",
    "        }\n",
    "\n",
    "        t = 0 # Keeps track of how many timesteps we've run so far this batch\n",
    "        nEpisodes=0\n",
    "        while t < self.nTimestampsPerBatch:\n",
    "            rewardPerEpisode=[]\n",
    "            stato = self.env.reset()\n",
    "            done = False\n",
    "            frames=[]\n",
    "            for i in range(self.stepsPerEpisode):\n",
    "                t+=1\n",
    "                batch['states'].append(stato)\n",
    "                azione,log_prob=self.getAction(stato)\n",
    "                batch['actions'].append(azione)\n",
    "                batch['log_probs'].append(log_prob)\n",
    "                stato, reward, done ,_= self.env.step(azione)  #al posto di _ ci sarebbe info ma non ci serve\n",
    "                rewardPerEpisode.append(reward)\n",
    "                frames.append(stato)\n",
    "                batch['done'].append(done)\n",
    "                if done :\n",
    "                    break #Ha raggiunto il termine dell'episodio.\n",
    "            batch['rewards'].append(rewardPerEpisode)\n",
    "            batch['lengths'].append(i+1)\n",
    "            nEpisodes+=1\n",
    "            #self.saveClip(frames,nEpisodes)\n",
    "            frames=[]\n",
    "        #Calcoliamo i rewards to go --> PASSO 4\n",
    "        batch['rewards_to_go']=self.calcRTG(batch['rewards'])\n",
    "        batch_statiTensor=tf.convert_to_tensor(batch['states'],dtype=tf.uint8)\n",
    "        batch_azioniTensor=tf.convert_to_tensor(batch['actions'],dtype=tf.int32)\n",
    "        batch_rewards_to_goTensor=tf.convert_to_tensor(batch['rewards_to_go'],dtype=tf.float32)\n",
    "        batch_log_probsTensor=tf.convert_to_tensor(batch['log_probs'],dtype=tf.float32)\n",
    "        batch_len=tf.convert_to_tensor(batch['lengths'],dtype=tf.int32)\n",
    "        batch_dones=tf.convert_to_tensor(batch['done'],dtype=tf.bool)\n",
    "\n",
    "\n",
    "        return batch_statiTensor, batch_azioniTensor,batch_rewards_to_goTensor,batch_log_probsTensor, batch_dones,batch_len\n",
    "\n",
    "    def getAction(self,stato):\n",
    "        stato=tf.convert_to_tensor(np.expand_dims(stato, axis=0) ,dtype=tf.float32)# Diventa (1, 64, 64, 3)\n",
    "        azione_pred,_=self.policyNN(stato)\n",
    "        #Somma probabilità\n",
    "        dist=tfp.distributions.Categorical(probs=tf.squeeze(azione_pred))\n",
    "        azionePresa=dist.sample()\n",
    "        log_prob=dist.log_prob(azionePresa)\n",
    "        return azionePresa, tf.stop_gradient(log_prob)\n",
    "\n",
    "    def calcRTG(self,rewards):\n",
    "        #Prendo la formula per calcolare i rewards to go e richiede i cumulative rewards e un fattore di sconto.\n",
    "        rtg=[]\n",
    "        for episode_reward in reversed(rewards):\n",
    "            cumulative_reward=0\n",
    "            totalRewardPerEpisode=0\n",
    "            for single_reward in reversed(episode_reward):\n",
    "                cumulative_reward=single_reward+cumulative_reward*self.gamma\n",
    "                totalRewardPerEpisode+=single_reward\n",
    "                rtg.append(cumulative_reward)\n",
    "            print(\"Total reward per episode RTG:\",totalRewardPerEpisode)\n",
    "        return tf.convert_to_tensor(rtg,dtype=tf.float32)\n",
    "\n",
    "   \n",
    "    def calcGaeAndTargets(self,rewards,values,dones):\n",
    "        advantages = []\n",
    "        targets = []\n",
    "        advantage = 0\n",
    "        try:\n",
    "            tf.debugging.check_numerics(rewards, \"Ricompense non valide\")\n",
    "            tf.debugging.check_numerics(values, \"Valori non validi\")\n",
    "            tf.debugging.assert_type(dones, tf.bool, \"La variabile dones deve essere booleana.\")\n",
    "            \n",
    "        except:\n",
    "            print(\"Errore: \",rewards , values, dones)\n",
    "        for t in reversed(range(len(rewards))):\n",
    "\n",
    "            #Se una delle variabili è solo un valore, allora non posso fare slicing e devo fare un controllo.\n",
    "            if t+1<len(rewards):\n",
    "                delta=rewards[t]+ (1-dones[t])*self.gamma*values[t+1]-values[t]\n",
    "            else:\n",
    "                delta=rewards[t]-values[t]\n",
    "            advantage=delta+self.gamma*self.lambdaGAE*(1-dones[t])*advantage\n",
    "            advantages.insert(0,advantage)\n",
    "            targets.insert(0, advantage + values[t])\n",
    "        return tf.convert_to_tensor(advantages, dtype=tf.float32), tf.convert_to_tensor(targets, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    def getPolicyLoss(self,log_probs_old, log_probs_new, advantages):\n",
    "        advantages = tf.stop_gradient(advantages)\n",
    "        policy_ratio = tf.exp(log_probs_new-log_probs_old)\n",
    "        surrogated_loss_1 = policy_ratio * advantages\n",
    "        clipped_policy_ratio=tf.clip_by_value(policy_ratio, clip_value_min=1.0-self.epsilon, clip_value_max=1.0+self.epsilon)\n",
    "        surrogated_loss_2 = clipped_policy_ratio * advantages\n",
    "        clip_loss=tf.minimum(surrogated_loss_1,surrogated_loss_2)\n",
    "        return -tf.reduce_mean(clip_loss)\n",
    "\n",
    "    def evaluate(self, batch_states,batch_actions):\n",
    "        batch_states=tf.cast(batch_states, tf.float32)\n",
    "        mean,v=self.policyNN(batch_states)\n",
    "        mean = tf.clip_by_value(mean, 1e-10, 1.0)  # Evita valori molto bassi\n",
    "        mean /= tf.reduce_sum(mean, axis=-1, keepdims=True)  # Normalizza\n",
    "        v= tf.squeeze(v)\n",
    "        dist=tfp.distributions.Categorical(probs=mean)\n",
    "        log_probs=dist.log_prob(batch_actions)\n",
    "        return v, log_probs, mean\n",
    "\n",
    "    def loadModel(self, path):\n",
    "        if path is \"\":\n",
    "            return\n",
    "        self.policyNN.build(self.nStati)\n",
    "        try:\n",
    "            #Check if weights contains Nan or Inf\n",
    "            self.policyNN.load_weights(path)\n",
    "            for var in self.policyNN.trainable_variables:\n",
    "                tf.debugging.check_numerics(var, \"LOAD Contiene NAN o INF\")\n",
    "\n",
    "        except:\n",
    "            print(\"Errore nel caricamento del modello\")\n",
    "\n",
    "\n",
    "    def saveModel(self, path):\n",
    "        #Controllo che la cartella esista e che non ci siano NaN nei pesi\n",
    "        if not os.path.exists(\"weights\"):\n",
    "            os.makedirs(\"weights\")\n",
    "        try:\n",
    "            for var in self.policyNN.trainable_variables:\n",
    "                tf.debugging.check_numerics(var, \"SAVE Contiene NAN o INF\")\n",
    "\n",
    "        except: \n",
    "            print(\"Errore: i pesi contengono NaN o Inf. Non verrà salvato\")\n",
    "            return\n",
    "        \n",
    "        self.policyNN.save_weights(path)\n",
    "    \n",
    "    def updateLearningRate(self, epoch):\n",
    "      if epoch % self.updateLearningRateEveryTimesteps == 0 and epoch > 0:\n",
    "        self.learningRate *= 0.9  # Riduci il learning rate del 10%\n",
    "        self.policy_optimizer.learning_rate = self.learningRate #Aggiorno solo dentro l'if che tanto è uguale per tutte le altre volte.   \n",
    "\n",
    "    def saveReward(self,reward,minReward,maxReward,epoch,path):\n",
    "        #Devo controllare se c'è davvero il file o meno. In caso affermativo conto quante righe ci sono.Da li ci sarà un offset così da incrementare correttamente\n",
    "        epoch+=self.offsetCsv        \n",
    "        data = {\"Epoch\": [epoch], \"Average reward\": [reward], \"Min reward\": [minReward], \"Max reward\": [maxReward]}\n",
    "        df=pd.DataFrame(data,columns=[\"Epoch\",\"Average reward\",\"Min reward\",\"Max reward\"])\n",
    "        df.to_csv(path,mode='a',index=False,header=False)\n",
    "\n",
    "    def showGraph(self):\n",
    "        rewards=pd.read_csv(\"./rewards/\"+self.gameName+\"_rewards.csv\")\n",
    "        rewards.plot(x='Epoch',y='Average reward',kind='line',title=\"Average reward per epoch\")\n",
    "        plt.show()\n",
    "    \n",
    "    def saveClip(self,frames,i):\n",
    "        clip = ImageSequenceClip(list(frames), fps=15)\n",
    "        nameVideo=\"./clip/\"+self.gameName+\"/\"+self.gameName+\"_video\"+str(i)+\".mp4\"\n",
    "        clip.write_videofile(nameVideo, fps=15,logger=None)\n",
    "        Video(nameVideo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "MRQw7a59YaKK",
    "outputId": "9f1bdaeb-13fd-483f-a5ae-331656852210"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1735324362.590036  253168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2784 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step totali eseguiti: 0  Step totali rimasti: 2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1735324363.221143  253168 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "/home/zakaria/anaconda3/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/zakaria/anaconda3/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 3.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 2.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 10.0\n",
      "Total reward per episode RTG: 5.0\n",
      "Total reward per episode RTG: 3.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 2.0\n",
      "Total reward per episode RTG: 2.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 4.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 6.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 3.0\n",
      "Total reward per episode RTG: 6.0\n",
      "Total reward per episode RTG: 4.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 2.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 8.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 3.0\n",
      "Total reward per episode RTG: 5.0\n",
      "Total reward per episode RTG: 5.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 2.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 2.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 4.0\n",
      "Total reward per episode RTG: 5.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 3.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 7.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 4.0\n",
      "Total reward per episode RTG: 5.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 2.0\n",
      "Total reward per episode RTG: 2.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 10.0\n",
      "Total reward per episode RTG: 2.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 3.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 5.0\n",
      "Total reward per episode RTG: 3.0\n",
      "Total reward per episode RTG: 5.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 6.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 2.0\n",
      "Total reward per episode RTG: 5.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 2.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 5.0\n",
      "Total reward per episode RTG: 11.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 2.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 2.0\n",
      "Total reward per episode RTG: 0.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 1.0\n",
      "Total reward per episode RTG: 1.0\n",
      "NUM SAMPLES: 8228\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'batch_size' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#load model weights if available \u001b[39;00m\n\u001b[1;32m     16\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39mloadModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./weights/ppo_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mgameName\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#save model weights\u001b[39;00m\n\u001b[1;32m     20\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39msaveModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./weights/ppo_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mgameName\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m num_samples\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(len_ep)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM SAMPLES:\u001b[39m\u001b[38;5;124m\"\u001b[39m,num_samples)\n\u001b[0;32m---> 49\u001b[0m samplesInPiu\u001b[38;5;241m=\u001b[39mnum_samples\u001b[38;5;241m%\u001b[39m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m     50\u001b[0m batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchSize\u001b[38;5;241m+\u001b[39msamplesInPiu\n\u001b[1;32m     52\u001b[0m i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'batch_size' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Configurazione ed esecuzione\n",
    "#Lista di giochi a disposizione di Procgen:\n",
    "\"\"\" \n",
    "    bigfish, bossfight, caveflyer, chaser, climber\n",
    "    coinrun, dodgeball, fruitbot, heist, jumper\n",
    "    leaper, maze, miner, ninja, plumber, starpilot\n",
    "\"\"\"\n",
    "seed=42\n",
    "gameName=\"starpilot\" #Scelto starpilot perchè è un gioco che ha episode corti, quindi allenamenti più rapidi.\n",
    "env = gym.make('procgen:procgen-'+gameName+'-v0',distribution_mode='easy',start_level=seed,rand_seed=seed, num_levels=100, use_backgrounds=False)\n",
    "\n",
    "#Creo l'oggetto PPO\n",
    "ppo_model=PPO(env,gameName)\n",
    "\n",
    "#load model weights if available \n",
    "ppo_model.loadModel(\"./weights/ppo_\"+gameName+\".weights.h5\")\n",
    "ppo_model.learn()\n",
    "\n",
    "#save model weights\n",
    "ppo_model.saveModel(\"./weights/ppo_\"+gameName+\".weights.h5\")\n",
    "\n",
    "ppo_model.showGraph()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
