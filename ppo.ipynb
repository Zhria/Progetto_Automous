{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sd2DHK4QYaKH"
   },
   "source": [
    "This module contains the implementation of the PPO algorithm.\n",
    "Ci basiamo sullo pseudocodice presente sul sito di OpenAI per la realizzazione del ppo.\n",
    "https://spinningup.openai.com/en/latest/algorithms/ppo.html#id7\n",
    "Utilizzando un Actor-Critic Method.\n",
    "Ciò suddivide l'implementazione in 8 passi principali:\n",
    "1. Inizializzazione dell'ambiente con policy parameters theta_0, e l'inizial value function parameters w_0.\n",
    "2. Ciclare per k iterazioni\n",
    "3. Raccogliere un set di traiettorie D_k = {τ_i} con una policy pi_k = pi(theta_k)\n",
    "4. Calcolare i reward-to-go R_t\n",
    "5. Calcolare gli advantage estimates A_t basandoci sulla value function V_{w_k}\n",
    "6. Aggiornare la policy massimizzando la PPO-Clip objective (Gradient ascent con adam) . Non scriverò la formula che è complessa\n",
    "7. Aggiornare la value function minimizzando la MSE tra V_{w_k} e R_t (Gradient descent con adam)\n",
    "8. Fine ciclo.\n",
    "\n",
    "Implementiamo tutti i passi nella funzione learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pqY5vJI-YaKI",
    "outputId": "77884fa9-5efe-4955-a667-c9a425a073ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #ignora warnings\n",
    "#Check if colab is used:\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print(\"Not running on CoLab\")\n",
    "if IN_COLAB:\n",
    "  !pip install procgen\n",
    "  !pip install tensorflow_probability\n",
    "  !pip install numpy\n",
    "from rete import ReteNeurale\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import math\n",
    "import glfw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f9EeZ0iE56HQ",
    "outputId": "3798c572-d4c3-4c01-d797-ac029befc27e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GLFWvidmode(size=Size(width=1400, height=900), bits=Bits(red=8, green=8, blue=8), refresh_rate=0)]\n"
     ]
    }
   ],
   "source": [
    "#!pip install moviepy\n",
    "#!pip install IPython\n",
    "#!pip install pyvirtualdisplay\n",
    "from moviepy import ImageSequenceClip\n",
    "from IPython.display import Video\n",
    "import os\n",
    "from pyvirtualdisplay.smartdisplay import SmartDisplay\n",
    "display = SmartDisplay(visible=0, size=(1400, 900),fbdir='/tmp')\n",
    "display.start()\n",
    "glfw.init()\n",
    "available_fbconfigs = glfw.get_video_modes(glfw.get_primary_monitor())\n",
    "print(available_fbconfigs)\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'osmesa'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohXWluKCYaKJ"
   },
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self,env):\n",
    "        self.env=env\n",
    "        self.nAzioni=env.action_space.n\n",
    "        self.nStati=env.observation_space.shape\n",
    "        self.listaAzioni=[i for i in range(self.nAzioni)]\n",
    "        self.episodesPerBatch=10\n",
    "        self.nEpoche=4\n",
    "        self.stepsPerEpisode=512        \n",
    "        self.gamma=0.99\n",
    "        self.epsilon=0.2\n",
    "        self.learningRate=5e-3\n",
    "        self.policyNN=ReteNeurale(self.nStati,self.nAzioni) #Actor\n",
    "        self.policy_optimizer=keras.optimizers.Adam(learning_rate=self.learningRate)\n",
    "        self.policyNN.compile(optimizer=self.policy_optimizer)\n",
    "        self.entropyCoefficient=0.01 #Per invogliare l'esplorazione un po di più.\n",
    "        self.lambdaGAE=0.95\n",
    "        \n",
    "        \n",
    "\n",
    "    def learn(self,env):\n",
    "        #passo 2 ciclare per k iterazioni.\n",
    "        for k in range(self.nEpoche):\n",
    "            states, actions, rewards_to_go, log_probs, epLen =self.collect_trajectories(k)\n",
    "            #print(\"Trajectories collected\")\n",
    "\n",
    "            num_samples=states.shape[0]\n",
    "            batch_size=32 #Faccio calcoli con mini-batches perchè altrimenti vado in Run out of memory di continuo.\n",
    "            for i in range(0,num_samples, batch_size):\n",
    "              batch_states=states[i:i+batch_size]\n",
    "              batch_actions=actions[i:i+batch_size]\n",
    "              batch_rewards_to_go=rewards_to_go[i:i+batch_size]\n",
    "              batch_log_probs=log_probs[i:i+batch_size]\n",
    "\n",
    "              V,latest_log_probs,_=self.evaluate(batch_states,batch_actions)\n",
    "              advantage=self.calcAdvantages(batch_rewards_to_go,V)\n",
    "\n",
    "              with tf.GradientTape() as tape:\n",
    "                _,latest_log_probs,probs=self.evaluate(batch_states,batch_actions)\n",
    "                policy_loss = self.getPolicyLoss(batch_log_probs,latest_log_probs,advantage)\n",
    "                value_loss=tf.reduce_mean(tf.square(batch_rewards_to_go-V)) #MSE tra rewards to go e V\n",
    "                #Aggiungo entropia alla loss per incentivare l'esplorazione\n",
    "                entropy = -tf.reduce_mean(probs * tf.math.log(probs + 1e-10))\n",
    "                total_loss=policy_loss+ value_loss*0.5 - entropy*self.entropyCoefficient\n",
    "              gradientsPolicy = tape.gradient(total_loss, self.policyNN.trainable_variables)\n",
    "              self.policy_optimizer.apply_gradients(zip(gradientsPolicy, self.policyNN.trainable_variables))\n",
    "              print(\"EPOCA:\",k,\" TOTAL LOSS:\",total_loss,\" POLICY LOSS:\",policy_loss,\" VALUE LOSS:\",value_loss,\" ENTROPY:\",entropy,)\n",
    "            self.evaluate_policy()\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, episodes=10):\n",
    "        total_rewards = []\n",
    "        for _ in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            cumulative_reward = 0\n",
    "            while not done:\n",
    "                state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "                state_tensor = tf.expand_dims(state_tensor, axis=0)\n",
    "                probs, _ = self.policyNN(state_tensor)\n",
    "                action = np.argmax(probs.numpy())\n",
    "                state, reward, done, info =self.env.step(action)\n",
    "                cumulative_reward += reward\n",
    "            total_rewards.append(cumulative_reward)\n",
    "        print(f\"Average Reward: {np.mean(total_rewards):.2f}\")\n",
    "\n",
    "    def collect_trajectories(self,epoca):\n",
    "        #Passo 3 --> Raccogliere un set di traiettorie D_k = {τ_i} con una policy pi_k = pi(theta_k)\n",
    "        #Dobbiamo raccogliere un set di traiettorie e per fare ciò dobbiamo raccogliere: stati, azioni, rewards, rewards to go, log_prob delle azioni.\n",
    "        batch={\n",
    "            'states':[],\n",
    "            'actions':[],\n",
    "            'rewards':[],\n",
    "            'rewards_to_go':[],\n",
    "            'log_probs':[],\n",
    "            'epLen':[]\n",
    "        }\n",
    "        done = False\n",
    "        stato = self.env.reset()\n",
    "        frames=[]\n",
    "        for i in range(self.episodesPerBatch):\n",
    "            if done == True:\n",
    "                stato = self.env.reset()\n",
    "                done=False\n",
    "            rewardPerEpisode=[]\n",
    "            print(\"----------------------------\\nEpisode: \",i)\n",
    "            for j in range(self.stepsPerEpisode):\n",
    "                batch['states'].append(stato)\n",
    "                azione,log_prob=self.getAction(stato)\n",
    "                #azione sarà un int, mentre log_prob sarà il logaritmo della probabilità dell'azione\n",
    "                batch['actions'].append(azione)\n",
    "                batch['log_probs'].append(log_prob)\n",
    "                stato, reward, done,info = self.env.step(azione)\n",
    "                rewardPerEpisode.append(reward)\n",
    "                #info non usata.\n",
    "                frames.append(stato)\n",
    "\n",
    "                if done:\n",
    "                    break #Ha raggiunto il termine dell'episodio.\n",
    "\n",
    "            batch['epLen'].append(j+1)\n",
    "            batch['rewards'].append(rewardPerEpisode)\n",
    "            clip = ImageSequenceClip(list(frames), fps=15)\n",
    "            nameVideo=\"coinrun_video\"+str(i)+\".mp4\"\n",
    "            clip.write_videofile(nameVideo, fps=15,logger=None)\n",
    "            Video(nameVideo)\n",
    "            frames=[]\n",
    "\n",
    "        #Calcoliamo i rewards to go --> PASSO 4\n",
    "        batch['rewards_to_go']=self.calcRTG(batch['rewards'])\n",
    "        #return batch states, actions, rewards, rewards to go, log_probs\n",
    "        #print(\"BATCH LOG PROBS:\",batch['log_probs'])\n",
    "        batch_statiTensor=tf.convert_to_tensor(batch['states'],dtype=tf.uint8)\n",
    "        batch_azioniTensor=tf.convert_to_tensor(batch['actions'],dtype=tf.int32)\n",
    "        batch_rewards_to_goTensor=tf.convert_to_tensor(batch['rewards_to_go'],dtype=tf.float32)\n",
    "        batch_log_probsTensor=tf.convert_to_tensor(batch['log_probs'],dtype=tf.float32)\n",
    "\n",
    "\n",
    "        return batch_statiTensor, batch_azioniTensor,batch_rewards_to_goTensor,batch_log_probsTensor, batch['epLen']\n",
    "\n",
    "    def getAction(self,stato):\n",
    "        stato=tf.convert_to_tensor(np.expand_dims(stato, axis=0) ,dtype=tf.float32)# Diventa (1, 64, 64, 3)\n",
    "        azione_pred,_=self.policyNN(stato)\n",
    "        #Somma probabilità\n",
    "        dist=tfp.distributions.Categorical(probs=tf.squeeze(azione_pred))\n",
    "        azionePresa=dist.sample()\n",
    "        log_prob=dist.log_prob(azionePresa)\n",
    "        return azionePresa, tf.stop_gradient(log_prob)\n",
    "\n",
    "    def calcRTG(self,rewards):\n",
    "        #Prendo la formula per calcolare i rewards to go e richiede i cumulative rewards e un fattore di sconto.\n",
    "        rtg=[]\n",
    "        for episode_reward in reversed(rewards):\n",
    "            cumulative_reward=0\n",
    "            totalRewardPerEpisode=0\n",
    "            for single_reward in reversed(episode_reward):\n",
    "                cumulative_reward=single_reward+cumulative_reward*self.gamma\n",
    "                totalRewardPerEpisode+=single_reward\n",
    "                rtg.append(cumulative_reward)\n",
    "            print(\"Total reward per episode:\",totalRewardPerEpisode)\n",
    "        return tf.convert_to_tensor(rtg,dtype=tf.float32)\n",
    "\n",
    "    def calcAdvantages(self, rtg,values):\n",
    "        advantages=rtg-tf.stop_gradient(values)\n",
    "        return (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-10)\n",
    "    \n",
    "    def calcGAE(self, rewards, values):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            delta = rewards[i] + self.gamma * values[i + 1] - values[i]\n",
    "            gae = delta + self.gamma * self.lambdaGAE * gae\n",
    "            returns.insert(0, gae + values[i])\n",
    "        return returns\n",
    "\n",
    "    def getPolicyLoss(self,log_probs_old, log_probs_new, advantages):\n",
    "        advantages = tf.stop_gradient(advantages)\n",
    "        #print(\"CALC SURROGATED LOSS, ADVANTAGES:\",advantages)\n",
    "        #print(\"CALC SURROGATED LOSS, Log probs old:\",log_probs_old)\n",
    "        #print(\"CALC SURROGATED LOSS, Log probs new:\",log_probs_new)\n",
    "        policy_ratio = tf.exp(log_probs_new-log_probs_old)\n",
    "        print(\"advantages\"  ,advantages)\n",
    "        #print(\"CALC SURROGATED LOSS, Policy ratio :\",policy_ratio)\n",
    "        surrogated_loss_1 = policy_ratio * advantages\n",
    "        clipped_policy_ratio=tf.clip_by_value(policy_ratio, clip_value_min=1.0-self.epsilon, clip_value_max=1.0+self.epsilon)\n",
    "        print(\"clipped policy ration\",clipped_policy_ratio)\n",
    "        surrogated_loss_2 = clipped_policy_ratio * advantages\n",
    "        clip_loss=tf.minimum(surrogated_loss_1,surrogated_loss_2)\n",
    "        return -tf.reduce_mean(clip_loss)\n",
    "\n",
    "    def evaluate(self, batch_states,batch_actions):\n",
    "        batch_states=tf.cast(batch_states, tf.float32)\n",
    "        #retVal=self.valueNN(batch_states)\n",
    "        mean,retVal=self.policyNN(batch_states)\n",
    "        V= tf.squeeze(retVal)\n",
    "        #print(\"V EVALUATE:\",V)\n",
    "        #print(\"MEAN EVALUATE:\",mean)\n",
    "        dist=tfp.distributions.Categorical(probs=mean)\n",
    "        log_probs=dist.log_prob(batch_actions)\n",
    "        #print(\"LOG PROBS EVALUATE:\",log_probs)\n",
    "        return V, log_probs, mean\n",
    "\n",
    "    def loadModel(self, path):\n",
    "        if path is \"\":\n",
    "            return\n",
    "        self.policyNN.build(self.nStati)\n",
    "        try:\n",
    "            self.policyNN.load_weights(path)\n",
    "        except:\n",
    "            print(\"Errore nel caricamento del modello\")\n",
    "\n",
    "    def saveModel(self, path):\n",
    "        self.policyNN.save_weights(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "MRQw7a59YaKK",
    "outputId": "9f1bdaeb-13fd-483f-a5ae-331656852210"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore nel caricamento del modello\n",
      "----------------------------\n",
      "Episode:  0\n",
      "----------------------------\n",
      "Episode:  1\n",
      "----------------------------\n",
      "Episode:  2\n",
      "----------------------------\n",
      "Episode:  3\n",
      "----------------------------\n",
      "Episode:  4\n",
      "----------------------------\n",
      "Episode:  5\n",
      "----------------------------\n",
      "Episode:  6\n",
      "----------------------------\n",
      "Episode:  7\n",
      "----------------------------\n",
      "Episode:  8\n",
      "----------------------------\n",
      "Episode:  9\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "Total reward per episode: 0.0\n",
      "advantages tf.Tensor(\n",
      "[-0.9931043  -0.53673786 -1.1529847   1.4524926  -1.0116411  -1.7550758\n",
      "  1.1977935  -0.39407426  1.1805238   0.31454524  1.0753247   0.5605353\n",
      "  1.4104245  -0.45841616  1.6663804  -0.69678426 -0.37813538  1.3131282\n",
      " -0.7306473  -0.53673786  1.5108099  -0.4555393  -0.69431853  1.1822293\n",
      " -0.6706945   1.4689515   0.7236546   0.8358716   0.3160205   0.9980598\n",
      " -0.49250516  1.5131316  -0.99551725 -0.87999254  1.09059    -0.7769968\n",
      " -0.9931043   0.15441571 -1.0769002  -0.6565676  -0.6565676  -0.6565676\n",
      " -0.6565676   1.3131282  -0.67518425 -0.8637429   1.1793929  -0.6297621\n",
      "  1.1676733   0.34391055  1.0819643   0.34391055  1.2964633  -1.1897694\n",
      "  1.6412474  -1.728276   -0.9702505   1.1536413  -0.68738693 -0.53673786\n",
      " -1.1529847  -0.9373373  -0.7306473  -1.0779488 ], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[1.0000002  0.99999976 1.0000002  1.         0.99999976 1.\n",
      " 1.0000007  0.99999976 1.0000002  1.         0.99999976 1.0000002\n",
      " 1.         0.9999999  1.0000002  1.0000002  1.         1.0000002\n",
      " 1.0000005  1.0000002  1.0000002  1.0000007  1.         1.\n",
      " 1.         1.         1.         1.0000002  1.0000005  1.\n",
      " 1.         1.         1.         1.0000002  0.99999976 1.\n",
      " 0.99999976 1.0000005  1.         1.         1.0000002  1.\n",
      " 1.         1.0000002  1.0000002  1.         1.         1.\n",
      " 0.9999995  1.0000002  1.         0.9999995  0.9999995  1.0000005\n",
      " 0.9999993  1.         1.         0.99999976 0.9999995  1.0000002\n",
      " 1.         1.         1.         1.0000005 ], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(-8.940697e-08, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(0.032385264, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.17485611, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(0.014443981, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[ 1.9006962   1.5866255   1.7040097   1.2093801   1.7040097   0.82018435\n",
      " -0.60672104 -0.88371956 -1.3232743  -0.7463524  -0.54160434  0.35822377\n",
      " -0.2916745   0.35822377 -0.54160434 -0.7463524  -1.3232743  -0.88371956\n",
      " -0.60672104  0.6941811   1.3690233   1.2989312   0.86012155 -0.69257075\n",
      " -0.8697357  -1.3607503  -0.5751193  -0.10198705  0.34396595 -0.31265742\n",
      "  0.22862482 -0.6300017  -0.5751193  -1.2683533  -0.85976106 -0.67278\n",
      "  0.71514     1.4086577   1.4338228   1.3186691   1.3438342   0.82018435\n",
      " -0.674299   -0.71483713 -1.2742614  -0.5436762  -0.54160434  0.35822377\n",
      " -0.2916745   0.35822377 -0.54160434 -0.7463524  -1.3232743  -0.91725856\n",
      " -0.55991447  0.81795865  1.2816787   2.017143    1.9258709  -0.6759046\n",
      " -1.0127801  -1.5125965  -0.40334302 -0.08849357], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[0.8 0.8 0.8 0.8 0.8 1.2 0.8 0.8 0.8 1.2 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\n",
      " 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 1.2 0.8 0.8 0.8 0.8\n",
      " 0.8 0.8 1.2 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 1.2 0.8 0.8\n",
      " 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 1.2 1.2], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(0.9262357, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(44.352432, shape=(), dtype=float32)  ENTROPY: tf.Tensor(0.0002537448, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(23.10245, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[-0.86124986 -0.93661934 -0.7841658  -0.9714668  -0.7338385  -0.98261034\n",
      " -0.44952953  0.6861526   1.2281353   1.7300584   1.7341793   1.7890838\n",
      "  1.7932302   1.3418303   0.6026316  -0.6352648  -0.68110496 -0.66390264\n",
      " -0.78524745 -0.8207944  -0.7625057  -0.8207944  -0.78524745 -0.66390264\n",
      " -0.69037527 -0.5972058   0.5349198   1.1418711   1.4376763   1.4162612\n",
      "  1.1973969   0.55893326 -0.6602286  -0.6995966  -0.6903204  -0.75936073\n",
      " -0.8668579  -0.6376613  -0.7583673  -0.64645153 -0.6903204  -0.6995966\n",
      " -0.6668674   0.55990905  1.1809902   1.7152467   1.6947936   1.1749727\n",
      "  0.71207845 -0.549757   -0.70683295 -0.78621155 -0.8109715  -0.7833585\n",
      " -0.83257675 -0.7833585  -0.8109715  -0.8045856  -0.75865144 -0.6678373\n",
      "  0.71207845  1.3440759   1.6947936   1.7152467 ], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[1.2 1.2 0.8 0.8 0.8 0.8 0.8 1.2 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\n",
      " 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 0.8 0.8 0.8 0.8 1.2 0.8 0.8 0.8 0.8 0.8\n",
      " 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\n",
      " 0.8 0.8 0.8 0.8 1.2 0.8 0.8 0.8 0.8 0.8], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(0.9864805, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(11.082075, shape=(), dtype=float32)  ENTROPY: tf.Tensor(1.0093315e-26, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(6.5275183, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n",
      "advantages tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "clipped policy ration tf.Tensor(\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan], shape=(64,), dtype=float32)\n",
      "EPOCA: 0  POLICY LOSS: tf.Tensor(nan, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(nan, shape=(), dtype=float32)  ENTROPY: tf.Tensor(nan, shape=(), dtype=float32)  TOTAL LOSS: tf.Tensor(nan, shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m ppo_model\u001b[38;5;241m=\u001b[39mPPO(env)\n\u001b[1;32m      8\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39mloadModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_coinrun.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39msaveModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_coinrun.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 33\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     29\u001b[0m batch_rewards_to_go\u001b[38;5;241m=\u001b[39mrewards_to_go[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     30\u001b[0m batch_log_probs\u001b[38;5;241m=\u001b[39mlog_probs[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m---> 33\u001b[0m V,latest_log_probs,_\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m advantage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalcAdvantages(batch_rewards_to_go,V)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n",
      "Cell \u001b[0;32mIn[11], line 166\u001b[0m, in \u001b[0;36mPPO.evaluate\u001b[0;34m(self, batch_states, batch_actions)\u001b[0m\n\u001b[1;32m    164\u001b[0m batch_states\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mcast(batch_states, tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m#retVal=self.valueNN(batch_states)\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m mean,retVal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicyNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m V\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(retVal)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m#print(\"V EVALUATE:\",V)\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m#print(\"MEAN EVALUATE:\",mean)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/layers/layer.py:899\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/ops/operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     43\u001b[0m         call_fn,\n\u001b[1;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/ProgettoAutonomous/ppo_implementation_strutturata/rete.py:28\u001b[0m, in \u001b[0;36mReteNeurale.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[1;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(x)\n\u001b[0;32m---> 28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool3(x)\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)        \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/layers/layer.py:899\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/ops/operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     43\u001b[0m         call_fn,\n\u001b[1;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:252\u001b[0m, in \u001b[0;36mBaseConv.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m         bias_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank\n\u001b[0;32m--> 252\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39madd(outputs, bias)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/ops/numpy.py:4761\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(x, newshape)\u001b[0m\n\u001b[1;32m   4759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[1;32m   4760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Reshape(newshape)\u001b[38;5;241m.\u001b[39msymbolic_call(x)\n\u001b[0;32m-> 4761\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py:1885\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(x, newshape)\u001b[0m\n\u001b[1;32m   1883\u001b[0m     output\u001b[38;5;241m.\u001b[39mset_shape(output_shape)\n\u001b[1;32m   1884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m-> 1885\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1254\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iterable_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m replace_iterable_params(args, kwargs, iterable_params)\n\u001b[0;32m-> 1254\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mapi_dispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1256\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configurazione ed esecuzione\n",
    "env = gym.make('procgen:procgen-coinrun-v0',distribution_mode='easy', start_level=0, num_levels=1)\n",
    "\n",
    "#load model weights \n",
    "\n",
    "ppo_model=PPO(env)\n",
    "\n",
    "ppo_model.loadModel(\"ppo_coinrun.weights.h5\")\n",
    "ppo_model.learn(env)\n",
    "ppo_model.saveModel(\"ppo_coinrun.weights.h5\")\n",
    "\n",
    "\n",
    "#save model weights\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
