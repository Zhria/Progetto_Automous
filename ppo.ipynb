{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sd2DHK4QYaKH"
   },
   "source": [
    "This module contains the implementation of the PPO algorithm.\n",
    "Ci basiamo sullo pseudocodice presente sul sito di OpenAI per la realizzazione del ppo.\n",
    "https://spinningup.openai.com/en/latest/algorithms/ppo.html#id7\n",
    "Utilizzando un Actor-Critic Method.\n",
    "Ciò suddivide l'implementazione in 8 passi principali:\n",
    "1. Inizializzazione dell'ambiente con policy parameters theta_0, e l'inizial value function parameters w_0.\n",
    "2. Ciclare per k iterazioni\n",
    "3. Raccogliere un set di traiettorie D_k = {τ_i} con una policy pi_k = pi(theta_k)\n",
    "4. Calcolare i reward-to-go R_t\n",
    "5. Calcolare gli advantage estimates A_t basandoci sulla value function V_{w_k}\n",
    "6. Aggiornare la policy massimizzando la PPO-Clip objective (Gradient ascent con adam) . Non scriverò la formula che è complessa\n",
    "7. Aggiornare la value function minimizzando la MSE tra V_{w_k} e R_t (Gradient descent con adam)\n",
    "8. Fine ciclo.\n",
    "\n",
    "Implementiamo tutti i passi nella funzione learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pqY5vJI-YaKI",
    "outputId": "1424a1ab-43fe-4315-8154-1bf6372c7ac0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 10:46:53.226229: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734256013.240915    6249 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734256013.245322    6249 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 10:46:53.259730: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #ignora warnings\n",
    "#Check if colab is used:\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print(\"Not running on CoLab\")\n",
    "if IN_COLAB:\n",
    "  !pip install procgen\n",
    "  !pip install tensorflow_probability\n",
    "  !pip install numpy\n",
    "from rete import ReteNeurale\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohXWluKCYaKJ"
   },
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def learn(self,env):\n",
    "        #Passo 1 --> Inizializzazione dell'ambiente con policy parameters theta_0, e l'inizial value function parameters w_0.\n",
    "        #Dobbiamo creare una rete neurale per la policy e per la value function.\n",
    "        self.env=env\n",
    "        self.nAzioni=env.action_space.n\n",
    "        self.nStati=env.observation_space.shape\n",
    "        self.listaAzioni=[i for i in range(self.nAzioni)]\n",
    "\n",
    "        #self.stepsPerEpisode=2048 #Per produzione\n",
    "        #self.episodesPerBatch=8 #per produzione\n",
    "        #self.nEpoche=200 #per produzione.\n",
    "        self.stepsPerEpisode=512\n",
    "        self.episodesPerBatch=1\n",
    "        self.nEpoche=5\n",
    "\n",
    "        self.gamma=0.99\n",
    "        self.epsilon=0.2\n",
    "        self.nUpdatesPerIteration=10\n",
    "        self.cov_mat=tf.linalg.diag(tf.fill([self.nAzioni], 0.5))\n",
    "        self.policyNN=ReteNeurale(self.nStati,self.nAzioni) #Actor\n",
    "        #self.valueNN=ReteNeurale(self.nStati,1,False) #Critic\n",
    "        self.policy_optimizer=keras.optimizers.Adam(learning_rate=5e-4)\n",
    "        \n",
    "        #self.value_optimizer=keras.optimizers.Adam(learning_rate=0.0005)\n",
    "        self.policyNN.compile(optimizer=self.policy_optimizer)\n",
    "        #self.valueNN.compile(optimizer=self.value_optimizer)\n",
    "        #passo 2 ciclare per k iterazioni.\n",
    "        for k in range(self.nEpoche):\n",
    "            states, actions, rewards_to_go, log_probs =self.collect_trajectories()\n",
    "            #print(\"Trajectories collected\")\n",
    "           \n",
    "            num_samples=states.shape[0]\n",
    "            batch_size=64 #Faccio calcoli con mini-batches perchè altrimenti vado in Run out of memory fisso.\n",
    "            for i in range(0,num_samples, batch_size):\n",
    "              batch_states=states[i:i+batch_size]\n",
    "              batch_actions=actions[i:i+batch_size]\n",
    "              batch_rewards_to_go=rewards_to_go[i:i+batch_size]\n",
    "              batch_log_probs=log_probs[i:i+batch_size]\n",
    "\n",
    "            \n",
    "              V,latest_log_probs,_=self.evaluate(batch_states,batch_actions)\n",
    "              advantage=self.calcAdvantages(batch_rewards_to_go,V)\n",
    "              \n",
    "              with tf.GradientTape(persistent=True) as tape:\n",
    "                  _,latest_log_probs,probs=self.evaluate(batch_states,batch_actions)\n",
    "                  surrogated_loss_1, surrogated_loss_2=self.calcSurrogatedLoss(batch_log_probs,latest_log_probs,advantage)\n",
    "                  policy_loss = -tf.reduce_mean(tf.minimum(surrogated_loss_1, surrogated_loss_2))\n",
    "                  value_loss=tf.reduce_mean(tf.square(batch_rewards_to_go-V)) #MSE tra rewards to go e V\n",
    "                  #print(\"Policy Loss:\", policy_loss)\n",
    "                  #print(\"Value Loss:\", value_loss)\n",
    "\n",
    "                  #Aggiungo entropia alla loss per incentivare l'esplorazione\n",
    "                  entropy = -tf.reduce_mean(probs * tf.math.log(probs + 1e-10))\n",
    "                  total_loss=policy_loss+ value_loss*0.5 - entropy*0.01\n",
    "              gradientsPolicy = tape.gradient(total_loss, self.policyNN.trainable_variables)\n",
    "              self.policy_optimizer.apply_gradients(zip(gradientsPolicy, self.policyNN.trainable_variables))\n",
    "\n",
    "              #gradientsValue = tape.gradient(value_loss, self.valueNN.trainable_variables)\n",
    "              #self.value_optimizer.apply_gradients(zip(gradientsValue, self.valueNN.trainable_variables))\n",
    "              #del tape\n",
    "              print(\"EPOCA:\",k,\" POLICY LOSS:\",policy_loss,\" VALUE LOSS:\",value_loss)\n",
    "            self.evaluate_policy()\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_policy(self, episodes=10):\n",
    "        total_rewards = []\n",
    "        for _ in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            cumulative_reward = 0\n",
    "            while not done:\n",
    "                state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "                state_tensor = tf.expand_dims(state_tensor, axis=0)\n",
    "                probs, _ = self.policyNN(state_tensor)\n",
    "                action = np.argmax(probs.numpy())\n",
    "                stato, reward, done, info =self.env.step(action)\n",
    "                cumulative_reward += reward\n",
    "            total_rewards.append(cumulative_reward)\n",
    "        print(f\"Average Reward: {np.mean(total_rewards):.2f}\")\n",
    "\n",
    "    def collect_trajectories(self):\n",
    "        #Passo 3 --> Raccogliere un set di traiettorie D_k = {τ_i} con una policy pi_k = pi(theta_k)\n",
    "        #Dobbiamo raccogliere un set di traiettorie e per fare ciò dobbiamo raccogliere: stati, azioni, rewards, rewards to go, log_prob delle azioni.\n",
    "        batch={\n",
    "            'states':[],\n",
    "            'actions':[],\n",
    "            'rewards':[],\n",
    "            'rewards_to_go':[],\n",
    "            'log_probs':[],\n",
    "        }\n",
    "        done = False\n",
    "        stato = self.env.reset()\n",
    "\n",
    "        #Abbiamo un fisso di 8 episodi per batch con 2048 steps per episodio\n",
    "        for i in range(self.episodesPerBatch):\n",
    "            if done == True:\n",
    "                stato = self.env.reset()\n",
    "                done=False\n",
    "            rewardPerEpisode=[]\n",
    "            print(\"Episode: \",i)\n",
    "            for j in range(self.stepsPerEpisode):\n",
    "                batch['states'].append(stato)\n",
    "                azione,log_prob=self.getAction(stato)\n",
    "                #azione sarà un int, mentre log_prob sarà il logaritmo della probabilità dell'azione\n",
    "                batch['actions'].append(azione)\n",
    "                batch['log_probs'].append(log_prob)\n",
    "                stato, reward, done, info = self.env.step(azione)\n",
    "                print(\"Info:\",info)\n",
    "                #info non usata.\n",
    "                rewardPerEpisode.append(reward)\n",
    "                if done:\n",
    "                    print(\"DONE EPISODE\")\n",
    "                    print(\"REWARD \",reward)\n",
    "                    print(\"STEPS : \",j)                    \n",
    "                    stato=self.env.reset()\n",
    "                    break #Ha raggiunto il termine dell'episodio.\n",
    "            batch['rewards'].append(rewardPerEpisode)\n",
    "        #Calcoliamo i rewards to go --> PASSO 4\n",
    "        batch['rewards_to_go']=self.calcRTG(batch['rewards'])\n",
    "        #return batch states, actions, rewards, rewards to go, log_probs\n",
    "        #print(\"BATCH LOG PROBS:\",batch['log_probs'])\n",
    "        batch_statiTensor=tf.convert_to_tensor(batch['states'],dtype=tf.uint8)\n",
    "        batch_azioniTensor=tf.convert_to_tensor(batch['actions'],dtype=tf.int32)\n",
    "        batch_rewards_to_goTensor=tf.convert_to_tensor(batch['rewards_to_go'],dtype=tf.float32)\n",
    "        batch_log_probsTensor=tf.convert_to_tensor(batch['log_probs'],dtype=tf.float32)\n",
    "\n",
    "\n",
    "        return batch_statiTensor, batch_azioniTensor,batch_rewards_to_goTensor,batch_log_probsTensor\n",
    "\n",
    "    def getAction(self,stato):\n",
    "        stato=tf.convert_to_tensor(np.expand_dims(stato, axis=0) ,dtype=tf.float32)# Diventa (1, 64, 64, 3)\n",
    "        azione_pred,_=self.policyNN(stato)\n",
    "        #Somma probabilità\n",
    "        dist=tfp.distributions.Categorical(probs=tf.squeeze(azione_pred))\n",
    "        azionePresa=dist.sample()\n",
    "        log_prob=dist.log_prob(azionePresa)\n",
    "        return azionePresa, tf.stop_gradient(log_prob)\n",
    "\n",
    "    def calcRTG(self,rewards):\n",
    "        #print(\"CALC REWARDS TO GO\")\n",
    "        #print(rewards)\n",
    "        #Prendo la formula per calcolare i rewards to go e richiede i cumulative rewards e un fattore di sconto.\n",
    "        rtg=[]\n",
    "        for episode_reward in reversed(rewards):\n",
    "            cumulative_reward=0\n",
    "            for single_reward in reversed(episode_reward):\n",
    "                cumulative_reward=single_reward+cumulative_reward*self.gamma\n",
    "                rtg.append(cumulative_reward)\n",
    "        return tf.convert_to_tensor(rtg,dtype=tf.float32)\n",
    "\n",
    "    def calcAdvantages(self, rtg,values):\n",
    "        advantages=rtg-tf.stop_gradient(values)\n",
    "        return (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-10)\n",
    "\n",
    "    def calcSurrogatedLoss(self,log_probs_old, log_probs_new, advantages):\n",
    "        advantages = tf.stop_gradient(advantages)\n",
    "        #print(\"CALC SURROGATED LOSS, ADVANTAGES:\",advantages)\n",
    "        #print(\"CALC SURROGATED LOSS, Log probs old:\",log_probs_old)\n",
    "        #print(\"CALC SURROGATED LOSS, Log probs new:\",log_probs_new)\n",
    "        policy_ratio = tf.exp(log_probs_old - log_probs_new)\n",
    "        #print(\"CALC SURROGATED LOSS, Policy ratio :\",policy_ratio)\n",
    "        surrogated_loss_1 = policy_ratio * advantages\n",
    "        surrogated_loss_2 = tf.clip_by_value(policy_ratio, clip_value_min=1.0-self.epsilon, clip_value_max=1.0+self.epsilon) * advantages\n",
    "        return surrogated_loss_1, surrogated_loss_2\n",
    "\n",
    "    def evaluate(self, batch_states,batch_actions):\n",
    "        batch_states=tf.cast(batch_states, tf.float32)\n",
    "        #retVal=self.valueNN(batch_states)\n",
    "        mean,retVal=self.policyNN(batch_states)\n",
    "        V= tf.squeeze(retVal)\n",
    "        #print(\"V EVALUATE:\",V)\n",
    "        #print(\"MEAN EVALUATE:\",mean)\n",
    "        dist=tfp.distributions.Categorical(probs=mean)\n",
    "        log_probs=dist.log_prob(batch_actions)\n",
    "        #print(\"LOG PROBS EVALUATE:\",log_probs)\n",
    "        return V, log_probs, mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUvSNGxTYaKK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MRQw7a59YaKK",
    "outputId": "14b44976-8c3c-4514-9d59-8209e0bab64c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734256017.281409    6249 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2735 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734256017.950099    6249 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "/home/zakaria/anaconda3/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/zakaria/anaconda3/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n",
      "Info: {'prev_level_seed': 0, 'prev_level_complete': 0, 'level_seed': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 10:47:18.761737: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:497] Allocator (GPU_0_bfc) ran out of memory trying to allocate 420.50MiB (rounded to 440926208)requested by op Mul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-12-15 10:47:18.761767: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1053] BFCAllocator dump for GPU_0_bfc\n",
      "2024-12-15 10:47:18.761775: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (256): \tTotal Chunks: 76, Chunks in use: 76. 19.0KiB allocated for chunks. 19.0KiB in use in bin. 5.3KiB client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761780: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (512): \tTotal Chunks: 6, Chunks in use: 5. 3.0KiB allocated for chunks. 2.5KiB in use in bin. 2.5KiB client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761785: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (1024): \tTotal Chunks: 9, Chunks in use: 9. 9.2KiB allocated for chunks. 9.2KiB in use in bin. 8.9KiB client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761791: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (2048): \tTotal Chunks: 13, Chunks in use: 12. 42.8KiB allocated for chunks. 40.8KiB in use in bin. 40.0KiB client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761796: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (4096): \tTotal Chunks: 2, Chunks in use: 2. 10.8KiB allocated for chunks. 10.8KiB in use in bin. 7.1KiB client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761801: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (8192): \tTotal Chunks: 4, Chunks in use: 4. 60.0KiB allocated for chunks. 60.0KiB in use in bin. 60.0KiB client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761805: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761811: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (32768): \tTotal Chunks: 1, Chunks in use: 0. 38.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761828: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (65536): \tTotal Chunks: 6, Chunks in use: 6. 457.0KiB allocated for chunks. 457.0KiB in use in bin. 424.0KiB client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761834: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761840: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (262144): \tTotal Chunks: 5, Chunks in use: 5. 1.41MiB allocated for chunks. 1.41MiB in use in bin. 1.41MiB client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761845: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761849: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761855: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 1. 3.00MiB allocated for chunks. 3.00MiB in use in bin. 3.00MiB client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761860: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 1. 6.00MiB allocated for chunks. 6.00MiB in use in bin. 6.00MiB client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761865: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761874: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (16777216): \tTotal Chunks: 1, Chunks in use: 1. 30.03MiB allocated for chunks. 30.03MiB in use in bin. 30.03MiB client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761880: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (33554432): \tTotal Chunks: 1, Chunks in use: 1. 56.25MiB allocated for chunks. 56.25MiB in use in bin. 56.25MiB client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761887: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (67108864): \tTotal Chunks: 2, Chunks in use: 1. 209.34MiB allocated for chunks. 105.12MiB in use in bin. 105.12MiB client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761893: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 0. 211.32MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761900: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (268435456): \tTotal Chunks: 5, Chunks in use: 5. 2.17GiB allocated for chunks. 2.17GiB in use in bin. 2.05GiB client-requested in use in bin.\n",
      "2024-12-15 10:47:18.761906: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1076] Bin for 420.50MiB was 256.00MiB, Chunk State: \n",
      "2024-12-15 10:47:18.761912: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1089] Next region of size 2868314112\n",
      "2024-12-15 10:47:18.761918: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a000000 of size 256 next 1\n",
      "2024-12-15 10:47:18.761923: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a000100 of size 1280 next 2\n",
      "2024-12-15 10:47:18.761927: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a000600 of size 256 next 3\n",
      "2024-12-15 10:47:18.761932: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a000700 of size 256 next 4\n",
      "2024-12-15 10:47:18.761937: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a000800 of size 1024 next 5\n",
      "2024-12-15 10:47:18.761942: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a000c00 of size 256 next 6\n",
      "2024-12-15 10:47:18.761948: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a000d00 of size 256 next 7\n",
      "2024-12-15 10:47:18.761953: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a000e00 of size 256 next 8\n",
      "2024-12-15 10:47:18.761958: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a000f00 of size 2048 next 44\n",
      "2024-12-15 10:47:18.761965: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a001700 of size 256 next 68\n",
      "2024-12-15 10:47:18.761971: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a001800 of size 256 next 69\n",
      "2024-12-15 10:47:18.761975: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a001900 of size 256 next 70\n",
      "2024-12-15 10:47:18.761980: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a001a00 of size 256 next 71\n",
      "2024-12-15 10:47:18.761984: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a001b00 of size 256 next 72\n",
      "2024-12-15 10:47:18.761988: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a001c00 of size 256 next 73\n",
      "2024-12-15 10:47:18.761993: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a001d00 of size 256 next 74\n",
      "2024-12-15 10:47:18.761998: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a001e00 of size 256 next 75\n",
      "2024-12-15 10:47:18.762003: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a001f00 of size 256 next 76\n",
      "2024-12-15 10:47:18.762008: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a002000 of size 256 next 82\n",
      "2024-12-15 10:47:18.762014: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a002100 of size 256 next 78\n",
      "2024-12-15 10:47:18.762023: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a002200 of size 256 next 79\n",
      "2024-12-15 10:47:18.762030: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a002300 of size 256 next 80\n",
      "2024-12-15 10:47:18.762036: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a002400 of size 1024 next 86\n",
      "2024-12-15 10:47:18.762042: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a002800 of size 512 next 83\n",
      "2024-12-15 10:47:18.762049: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a002a00 of size 256 next 90\n",
      "2024-12-15 10:47:18.762055: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a002b00 of size 5888 next 81\n",
      "2024-12-15 10:47:18.762062: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a004200 of size 15360 next 84\n",
      "2024-12-15 10:47:18.762068: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a007e00 of size 256 next 91\n",
      "2024-12-15 10:47:18.762077: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a007f00 of size 3584 next 88\n",
      "2024-12-15 10:47:18.762084: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a008d00 of size 3584 next 77\n",
      "2024-12-15 10:47:18.762091: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a009b00 of size 256 next 92\n",
      "2024-12-15 10:47:18.762098: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a009c00 of size 256 next 93\n",
      "2024-12-15 10:47:18.762104: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a009d00 of size 256 next 96\n",
      "2024-12-15 10:47:18.762111: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a009e00 of size 256 next 97\n",
      "2024-12-15 10:47:18.762117: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a009f00 of size 512 next 100\n",
      "2024-12-15 10:47:18.762123: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00a100 of size 512 next 101\n",
      "2024-12-15 10:47:18.762128: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00a300 of size 1024 next 104\n",
      "2024-12-15 10:47:18.762133: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00a700 of size 1024 next 105\n",
      "2024-12-15 10:47:18.762139: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00ab00 of size 256 next 108\n",
      "2024-12-15 10:47:18.762146: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00ac00 of size 256 next 109\n",
      "2024-12-15 10:47:18.762152: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00ad00 of size 1024 next 110\n",
      "2024-12-15 10:47:18.762158: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00b100 of size 1024 next 111\n",
      "2024-12-15 10:47:18.762163: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00b500 of size 256 next 112\n",
      "2024-12-15 10:47:18.762168: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00b600 of size 256 next 113\n",
      "2024-12-15 10:47:18.762173: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00b700 of size 256 next 114\n",
      "2024-12-15 10:47:18.762178: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00b800 of size 256 next 115\n",
      "2024-12-15 10:47:18.762184: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00b900 of size 256 next 116\n",
      "2024-12-15 10:47:18.762190: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00ba00 of size 256 next 117\n",
      "2024-12-15 10:47:18.762196: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00bb00 of size 256 next 118\n",
      "2024-12-15 10:47:18.762201: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00bc00 of size 256 next 119\n",
      "2024-12-15 10:47:18.762205: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00bd00 of size 256 next 120\n",
      "2024-12-15 10:47:18.762210: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00be00 of size 256 next 121\n",
      "2024-12-15 10:47:18.762216: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00bf00 of size 256 next 122\n",
      "2024-12-15 10:47:18.762223: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00c000 of size 3840 next 9\n",
      "2024-12-15 10:47:18.762230: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00cf00 of size 256 next 10\n",
      "2024-12-15 10:47:18.762235: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00d000 of size 256 next 11\n",
      "2024-12-15 10:47:18.762239: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00d100 of size 256 next 12\n",
      "2024-12-15 10:47:18.762244: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00d200 of size 256 next 13\n",
      "2024-12-15 10:47:18.762248: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00d300 of size 256 next 15\n",
      "2024-12-15 10:47:18.762253: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00d400 of size 256 next 16\n",
      "2024-12-15 10:47:18.762257: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00d500 of size 256 next 14\n",
      "2024-12-15 10:47:18.762262: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00d600 of size 256 next 21\n",
      "2024-12-15 10:47:18.762267: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00d700 of size 256 next 17\n",
      "2024-12-15 10:47:18.762273: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00d800 of size 512 next 20\n",
      "2024-12-15 10:47:18.762279: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00da00 of size 256 next 26\n",
      "2024-12-15 10:47:18.762285: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00db00 of size 256 next 24\n",
      "2024-12-15 10:47:18.762289: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00dc00 of size 1024 next 25\n",
      "2024-12-15 10:47:18.762294: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00e000 of size 256 next 31\n",
      "2024-12-15 10:47:18.762299: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00e100 of size 256 next 29\n",
      "2024-12-15 10:47:18.762304: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00e200 of size 256 next 30\n",
      "2024-12-15 10:47:18.762310: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00e300 of size 256 next 36\n",
      "2024-12-15 10:47:18.762316: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00e400 of size 256 next 34\n",
      "2024-12-15 10:47:18.762321: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00e500 of size 256 next 35\n",
      "2024-12-15 10:47:18.762325: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00e600 of size 256 next 47\n",
      "2024-12-15 10:47:18.762330: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00e700 of size 256 next 46\n",
      "2024-12-15 10:47:18.762335: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00e800 of size 256 next 45\n",
      "2024-12-15 10:47:18.762341: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00e900 of size 256 next 552\n",
      "2024-12-15 10:47:18.762347: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00ea00 of size 256 next 42\n",
      "2024-12-15 10:47:18.762353: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00eb00 of size 256 next 50\n",
      "2024-12-15 10:47:18.762358: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00ec00 of size 256 next 48\n",
      "2024-12-15 10:47:18.762363: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00ed00 of size 256 next 57\n",
      "2024-12-15 10:47:18.762368: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00ee00 of size 256 next 39\n",
      "2024-12-15 10:47:18.762372: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00ef00 of size 256 next 58\n",
      "2024-12-15 10:47:18.762378: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00f000 of size 256 next 60\n",
      "2024-12-15 10:47:18.762385: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00f100 of size 256 next 18\n",
      "2024-12-15 10:47:18.762390: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a00f200 of size 3584 next 19\n",
      "2024-12-15 10:47:18.762395: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a010000 of size 1024 next 43\n",
      "2024-12-15 10:47:18.762399: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a010400 of size 3840 next 67\n",
      "2024-12-15 10:47:18.762403: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a011300 of size 3840 next 41\n",
      "2024-12-15 10:47:18.762408: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a012200 of size 3840 next 54\n",
      "2024-12-15 10:47:18.762412: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a013100 of size 3840 next 55\n",
      "2024-12-15 10:47:18.762417: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a014000 of size 3840 next 56\n",
      "2024-12-15 10:47:18.762422: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a014f00 of size 3840 next 59\n",
      "2024-12-15 10:47:18.762426: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a015e00 of size 256 next 61\n",
      "2024-12-15 10:47:18.762430: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a015f00 of size 256 next 62\n",
      "2024-12-15 10:47:18.762434: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a016000 of size 256 next 63\n",
      "2024-12-15 10:47:18.762438: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a016100 of size 256 next 64\n",
      "2024-12-15 10:47:18.762443: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a016200 of size 256 next 65\n",
      "2024-12-15 10:47:18.762447: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a016300 of size 256 next 66\n",
      "2024-12-15 10:47:18.762452: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a016400 of size 5120 next 38\n",
      "2024-12-15 10:47:18.762458: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a017800 of size 15360 next 37\n",
      "2024-12-15 10:47:18.762463: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a01b400 of size 99328 next 553\n",
      "2024-12-15 10:47:18.762468: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a033800 of size 2048 next 23\n",
      "2024-12-15 10:47:18.762473: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a034000 of size 73728 next 22\n",
      "2024-12-15 10:47:18.762479: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a046000 of size 294912 next 87\n",
      "2024-12-15 10:47:18.762485: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a08e000 of size 73728 next 89\n",
      "2024-12-15 10:47:18.762490: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a0a0000 of size 73728 next 94\n",
      "2024-12-15 10:47:18.762495: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a0b2000 of size 73728 next 95\n",
      "2024-12-15 10:47:18.762500: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a0c4000 of size 15360 next 106\n",
      "2024-12-15 10:47:18.762504: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a0c7c00 of size 15360 next 107\n",
      "2024-12-15 10:47:18.762509: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a0cb800 of size 256 next 126\n",
      "2024-12-15 10:47:18.762515: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a0cb900 of size 512 next 129\n",
      "2024-12-15 10:47:18.762521: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 75708a0cbb00 of size 512 next 128\n",
      "2024-12-15 10:47:18.762527: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a0cbd00 of size 256 next 131\n",
      "2024-12-15 10:47:18.762534: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 75708a0cbe00 of size 2048 next 123\n",
      "2024-12-15 10:47:18.762541: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a0cc600 of size 256 next 124\n",
      "2024-12-15 10:47:18.762548: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 75708a0cc700 of size 39168 next 28\n",
      "2024-12-15 10:47:18.762555: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a0d6000 of size 294912 next 27\n",
      "2024-12-15 10:47:18.762563: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a11e000 of size 6291456 next 40\n",
      "2024-12-15 10:47:18.762570: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708a71e000 of size 3145728 next 51\n",
      "2024-12-15 10:47:18.762578: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708aa1e000 of size 31490048 next 49\n",
      "2024-12-15 10:47:18.762585: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75708c826000 of size 58982400 next 52\n",
      "2024-12-15 10:47:18.762592: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 757090066000 of size 110231552 next 53\n",
      "2024-12-15 10:47:18.762598: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 757096986000 of size 294912 next 98\n",
      "2024-12-15 10:47:18.762604: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7570969ce000 of size 294912 next 99\n",
      "2024-12-15 10:47:18.762612: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 757096a16000 of size 73728 next 125\n",
      "2024-12-15 10:47:18.762620: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 757096a28000 of size 294912 next 127\n",
      "2024-12-15 10:47:18.762626: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 757096a70000 of size 109273088 next 85\n",
      "2024-12-15 10:47:18.762632: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75709d2a6000 of size 561479680 next 33\n",
      "2024-12-15 10:47:18.762636: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7570bea1e000 of size 440926208 next 32\n",
      "2024-12-15 10:47:18.762641: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7570d8e9e000 of size 440926208 next 102\n",
      "2024-12-15 10:47:18.762646: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7570f331e000 of size 440926208 next 103\n",
      "2024-12-15 10:47:18.762651: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 75710d79e000 of size 440926208 next 130\n",
      "2024-12-15 10:47:18.762657: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 757127c1e000 of size 221585408 next 18446744073709551615\n",
      "2024-12-15 10:47:18.762663: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114]      Summary of in-use Chunks by size: \n",
      "2024-12-15 10:47:18.762673: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 76 Chunks of size 256 totalling 19.0KiB\n",
      "2024-12-15 10:47:18.762681: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 5 Chunks of size 512 totalling 2.5KiB\n",
      "2024-12-15 10:47:18.762688: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 8 Chunks of size 1024 totalling 8.0KiB\n",
      "2024-12-15 10:47:18.762693: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2024-12-15 10:47:18.762698: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 2 Chunks of size 2048 totalling 4.0KiB\n",
      "2024-12-15 10:47:18.762702: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 3 Chunks of size 3584 totalling 10.5KiB\n",
      "2024-12-15 10:47:18.762706: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 7 Chunks of size 3840 totalling 26.2KiB\n",
      "2024-12-15 10:47:18.762711: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 5120 totalling 5.0KiB\n",
      "2024-12-15 10:47:18.762715: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 5888 totalling 5.8KiB\n",
      "2024-12-15 10:47:18.762719: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 4 Chunks of size 15360 totalling 60.0KiB\n",
      "2024-12-15 10:47:18.762723: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 5 Chunks of size 73728 totalling 360.0KiB\n",
      "2024-12-15 10:47:18.762728: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 99328 totalling 97.0KiB\n",
      "2024-12-15 10:47:18.762732: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 5 Chunks of size 294912 totalling 1.41MiB\n",
      "2024-12-15 10:47:18.762736: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 3145728 totalling 3.00MiB\n",
      "2024-12-15 10:47:18.762740: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 6291456 totalling 6.00MiB\n",
      "2024-12-15 10:47:18.762745: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 31490048 totalling 30.03MiB\n",
      "2024-12-15 10:47:18.762749: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 58982400 totalling 56.25MiB\n",
      "2024-12-15 10:47:18.762753: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 110231552 totalling 105.12MiB\n",
      "2024-12-15 10:47:18.762758: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 4 Chunks of size 440926208 totalling 1.64GiB\n",
      "2024-12-15 10:47:18.762762: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 561479680 totalling 535.47MiB\n",
      "2024-12-15 10:47:18.762766: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1121] Sum Total of in-use chunks: 2.36GiB\n",
      "2024-12-15 10:47:18.762771: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1123] Total bytes in pool: 2868314112 memory_limit_: 2868314112 available bytes: 0 curr_region_allocation_bytes_: 5736628224\n",
      "2024-12-15 10:47:18.762777: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1128] Stats: \n",
      "Limit:                      2868314112\n",
      "InUse:                      2537413888\n",
      "MaxInUse:                   2537413888\n",
      "NumAllocs:                       22648\n",
      "MaxAllocSize:                570720256\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-12-15 10:47:18.762787: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:508] ********___****************xxx***************************************************************_______\n",
      "2024-12-15 10:47:18.762801: W tensorflow/core/framework/op_kernel.cc:1829] RESOURCE_EXHAUSTED: failed to allocate memory\n",
      "2024-12-15 10:47:18.762819: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocgen:procgen-coinrun-v0\u001b[39m\u001b[38;5;124m'\u001b[39m,distribution_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124measy\u001b[39m\u001b[38;5;124m'\u001b[39m, start_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, num_levels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m ppo_model\u001b[38;5;241m=\u001b[39mPPO()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 57\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     55\u001b[0m     total_loss\u001b[38;5;241m=\u001b[39mpolicy_loss\u001b[38;5;241m+\u001b[39m value_loss\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m-\u001b[39m entropy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m     56\u001b[0m gradientsPolicy \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(total_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicyNN\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradientsPolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicyNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#gradientsValue = tape.gradient(value_loss, self.valueNN.trainable_variables)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#self.value_optimizer.apply_gradients(zip(gradientsValue, self.valueNN.trainable_variables))\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#del tape\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCA:\u001b[39m\u001b[38;5;124m\"\u001b[39m,k,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m POLICY LOSS:\u001b[39m\u001b[38;5;124m\"\u001b[39m,policy_loss,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m VALUE LOSS:\u001b[39m\u001b[38;5;124m\"\u001b[39m,value_loss)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:344\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[1;32m    343\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterations\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:409\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    406\u001b[0m     grads \u001b[38;5;241m=\u001b[39m [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g \u001b[38;5;241m/\u001b[39m scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads]\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:472\u001b[0m, in \u001b[0;36mBaseOptimizer._backend_apply_gradients\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_update_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_variables_moving_average(\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables\n\u001b[1;32m    479\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py:120\u001b[0m, in \u001b[0;36mTFOptimizer._backend_update_step\u001b[0;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[1;32m    118\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[1;32m    119\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_reduce_sum_gradients(grads_and_vars)\n\u001b[0;32m--> 120\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_tf_update_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py:134\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step\u001b[0;34m(self, distribution, grads_and_vars, learning_rate)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(grad, var, learning_rate)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3005\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3002\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   3003\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3004\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 3005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3007\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   3008\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4075\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4072\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   4073\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   4074\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 4075\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4081\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4077\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   4078\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   4079\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   4080\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 4081\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4082\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   4083\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py:131\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad, learning_rate)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_grad_to_update_var\u001b[39m(var, grad, learning_rate):\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/adam.py:133\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[0;34m(self, gradient, variable, learning_rate)\u001b[0m\n\u001b[1;32m    128\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_velocities[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_variable_index(variable)]\n\u001b[1;32m    130\u001b[0m alpha \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m ops\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta_2_power) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta_1_power)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_add(\n\u001b[0;32m--> 133\u001b[0m     m, \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubtract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_add(\n\u001b[1;32m    136\u001b[0m     v,\n\u001b[1;32m    137\u001b[0m     ops\u001b[38;5;241m.\u001b[39mmultiply(\n\u001b[1;32m    138\u001b[0m         ops\u001b[38;5;241m.\u001b[39msubtract(ops\u001b[38;5;241m.\u001b[39msquare(gradient), v), \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2\n\u001b[1;32m    139\u001b[0m     ),\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamsgrad:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/ops/numpy.py:5909\u001b[0m, in \u001b[0;36mmultiply\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m   5907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x1, x2)):\n\u001b[1;32m   5908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Multiply()\u001b[38;5;241m.\u001b[39msymbolic_call(x1, x2)\n\u001b[0;32m-> 5909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/backend/tensorflow/sparse.py:627\u001b[0m, in \u001b[0;36melementwise_binary_intersection.<locals>.sparse_wrapper\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mIndexedSlices(\n\u001b[1;32m    622\u001b[0m             func(tf\u001b[38;5;241m.\u001b[39mgather(x1, x2\u001b[38;5;241m.\u001b[39mindices), x2\u001b[38;5;241m.\u001b[39mvalues),\n\u001b[1;32m    623\u001b[0m             x2\u001b[38;5;241m.\u001b[39mindices,\n\u001b[1;32m    624\u001b[0m             x2\u001b[38;5;241m.\u001b[39mdense_shape,\n\u001b[1;32m    625\u001b[0m         )\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# Default case, no SparseTensor and no IndexedSlices.\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py:523\u001b[0m, in \u001b[0;36mmultiply\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    521\u001b[0m x1 \u001b[38;5;241m=\u001b[39m convert_to_tensor(x1, dtype)\n\u001b[1;32m    522\u001b[0m x2 \u001b[38;5;241m=\u001b[39m convert_to_tensor(x2, dtype)\n\u001b[0;32m--> 523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:6002\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   6001\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 6002\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: "
     ]
    }
   ],
   "source": [
    "# Configurazione ed esecuzione\n",
    "env = gym.make('procgen:procgen-coinrun-v0',distribution_mode='easy', start_level=0, num_levels=1)\n",
    "ppo_model=PPO()\n",
    "ppo_model.learn(env)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
