{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd2DHK4QYaKH"
      },
      "source": [
        "This module contains the implementation of the PPO algorithm.\n",
        "Ci basiamo sullo pseudocodice presente sul sito di OpenAI per la realizzazione del ppo.\n",
        "https://spinningup.openai.com/en/latest/algorithms/ppo.html#id7\n",
        "Utilizzando un Actor-Critic Method.\n",
        "Ciò suddivide l'implementazione in 8 passi principali:\n",
        "1. Inizializzazione dell'ambiente con policy parameters theta_0, e l'inizial value function parameters w_0.\n",
        "2. Ciclare per k iterazioni\n",
        "3. Raccogliere un set di traiettorie D_k = {τ_i} con una policy pi_k = pi(theta_k)\n",
        "4. Calcolare i reward-to-go R_t\n",
        "5. Calcolare gli advantage estimates A_t basandoci sulla value function V_{w_k}\n",
        "6. Aggiornare la policy massimizzando la PPO-Clip objective (Gradient ascent con adam) . Non scriverò la formula che è complessa\n",
        "7. Aggiornare la value function minimizzando la MSE tra V_{w_k} e R_t (Gradient descent con adam)\n",
        "8. Fine ciclo.\n",
        "\n",
        "Implementiamo tutti i passi nella funzione learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pqY5vJI-YaKI",
        "outputId": "70cc34f4-9186-4a44-8096-f188df6bce1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting procgen\n",
            "  Downloading procgen-0.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from procgen) (1.26.4)\n",
            "Requirement already satisfied: gym<1.0.0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from procgen) (0.25.2)\n",
            "Collecting gym3<1.0.0,>=0.3.3 (from procgen)\n",
            "  Downloading gym3-0.3.3-py3-none-any.whl.metadata (835 bytes)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from procgen) (3.16.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (0.0.8)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.17.1)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (2.36.1)\n",
            "Collecting imageio-ffmpeg<0.4.0,>=0.3.0 (from gym3<1.0.0,>=0.3.3->procgen)\n",
            "  Downloading imageio_ffmpeg-0.3.0-py3-none-manylinux2010_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting glfw<2.0.0,>=1.8.6 (from gym3<1.0.0,>=0.3.3->procgen)\n",
            "  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting moderngl<6.0.0,>=5.5.4 (from gym3<1.0.0,>=0.3.3->procgen)\n",
            "  Downloading moderngl-5.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.13.0->gym3<1.0.0,>=0.3.3->procgen) (2.22)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0.0,>=2.6.0->gym3<1.0.0,>=0.3.3->procgen) (11.0.0)\n",
            "Collecting glcontext>=3.0.0 (from moderngl<6.0.0,>=5.5.4->gym3<1.0.0,>=0.3.3->procgen)\n",
            "  Downloading glcontext-3.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.0 kB)\n",
            "Downloading procgen-0.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gym3-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imageio_ffmpeg-0.3.0-py3-none-manylinux2010_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading moderngl-5.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.4/291.4 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glcontext-3.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imageio-ffmpeg, glfw, glcontext, moderngl, gym3, procgen\n",
            "  Attempting uninstall: imageio-ffmpeg\n",
            "    Found existing installation: imageio-ffmpeg 0.5.1\n",
            "    Uninstalling imageio-ffmpeg-0.5.1:\n",
            "      Successfully uninstalled imageio-ffmpeg-0.5.1\n",
            "Successfully installed glcontext-3.0.0 glfw-1.12.0 gym3-0.3.3 imageio-ffmpeg-0.3.0 moderngl-5.12.0 procgen-0.10.7\n",
            "Requirement already satisfied: tensorflow_probability in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability) (1.26.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability) (3.1.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability) (0.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore') #ignora warnings\n",
        "!pip install procgen\n",
        "!pip install tensorflow_probability\n",
        "!pip install numpy\n",
        "from rete import ReteNeurale\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import gym\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ohXWluKCYaKJ"
      },
      "outputs": [],
      "source": [
        "class PPO:\n",
        "    def learn(self,env):\n",
        "        #Passo 1 --> Inizializzazione dell'ambiente con policy parameters theta_0, e l'inizial value function parameters w_0.\n",
        "        #Dobbiamo creare una rete neurale per la policy e per la value function.\n",
        "        self.env=env\n",
        "        self.nAzioni=env.action_space.n\n",
        "        self.nStati=env.observation_space.shape\n",
        "        self.listaAzioni=list(range(env.action_space.n))\n",
        "\n",
        "        print(\"N STATI ENV:\",self.nStati)\n",
        "        print(\"N AZIONI ENV:\",self.nAzioni)\n",
        "        #self.stepsPerEpisode=2048 Per produzione\n",
        "        #self.episodesPerBatch=8 per produzione\n",
        "        #self.nEpoche=200 per produzione.\n",
        "        self.stepsPerEpisode=512\n",
        "        self.episodesPerBatch=4\n",
        "        self.nEpoche=10\n",
        "\n",
        "        self.gamma=0.95\n",
        "        self.epsilon=0.2\n",
        "        self.nUpdatesPerIteration=10\n",
        "        self.cov_mat=tf.linalg.diag(tf.fill([self.nAzioni], 0.5))\n",
        "        self.policyNN=ReteNeurale(self.nStati,self.nAzioni,softmax=True) #Actor\n",
        "        self.valueNN=ReteNeurale(self.nStati,1,False) #Critic\n",
        "        self.policy_optimizer=keras.optimizers.Adam(learning_rate=0.0005)\n",
        "        self.value_optimizer=keras.optimizers.Adam(learning_rate=0.0005)\n",
        "        self.policyNN.compile(optimizer=self.policy_optimizer)\n",
        "        self.valueNN.compile(optimizer=self.value_optimizer)\n",
        "        #passo 2 ciclare per k iterazioni.\n",
        "        for k in range(self.nEpoche):\n",
        "            states, actions, rewards, rewards_to_go, log_probs =self.collect_trajectories()\n",
        "            print(\"Trajectories collected\")\n",
        "            V,latest_log_probs=self.evaluate(states,actions)\n",
        "            advantage=self.calcAdvantages(rewards_to_go,V)\n",
        "            print(\"Advantages calculated\")\n",
        "            std_advantages=tf.math.reduce_std(advantage)\n",
        "            mean_advantages=tf.math.reduce_mean(advantage)\n",
        "            print(\"Mean and std of advantages:\",mean_advantages,std_advantages)\n",
        "\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                _,latest_log_probs=self.evaluate(states,actions)\n",
        "                print(\"log_probs is tensor:\", isinstance(log_probs, tf.Tensor))\n",
        "                print(\"advantage is tensor:\", isinstance(advantage, tf.Tensor))\n",
        "                print(\"rewards_to_go is tensor:\", isinstance(rewards_to_go, tf.Tensor))\n",
        "                print(\"V is tensor:\", isinstance(V, tf.Tensor))\n",
        "\n",
        "\n",
        "                surrogated_loss_1, surrogated_loss_2=self.calcSurrogatedLoss(log_probs,latest_log_probs,advantage)\n",
        "                policy_loss = -tf.reduce_mean(tf.minimum(surrogated_loss_1, surrogated_loss_2))\n",
        "                value_loss=tf.reduce_mean(tf.square(rewards_to_go-V)) #MSE tra rewards to go e V\n",
        "                print(\"Policy Loss:\", policy_loss)\n",
        "                print(\"Value Loss:\", value_loss)\n",
        "            gradientsPolicy = tape.gradient(policy_loss, self.policyNN.trainable_variables)\n",
        "\n",
        "\n",
        "                 # Debug: Controlla se i gradienti sono None\n",
        "            print(\"Gradients for policy:\", gradientsPolicy)\n",
        "\n",
        "                # Verifica che i gradienti non siano None\n",
        "            if gradientsPolicy and all(grad is not None for grad in gradientsPolicy):\n",
        "                self.policy_optimizer.apply_gradients(zip(gradientsPolicy, self.policyNN.trainable_variables))\n",
        "            else:\n",
        "                print(\"Policy gradients are None!\")\n",
        "\n",
        "            gradientsValue = tape.gradient(value_loss, self.valueNN.trainable_variables)\n",
        "            print(\"Gradients for value:\", gradientsValue)\n",
        "            # Verifica che i gradienti non siano None\n",
        "            if gradientsValue and all(grad is not None for grad in gradientsValue):\n",
        "                self.value_optimizer.apply_gradients(zip(gradientsValue, self.valueNN.trainable_variables))\n",
        "            else:\n",
        "                print(\"Value gradients are None!\")\n",
        "\n",
        "                print(\"EPOCA:\",k,\" POLICY LOSS:\",policy_loss,\" VALUE LOSS:\",value_loss)\n",
        "\n",
        "\n",
        "    def collect_trajectories(self):\n",
        "        #Passo 3 --> Raccogliere un set di traiettorie D_k = {τ_i} con una policy pi_k = pi(theta_k)\n",
        "        #Dobbiamo raccogliere un set di traiettorie e per fare ciò dobbiamo raccogliere: stati, azioni, rewards, rewards to go, log_prob delle azioni.\n",
        "        batch={\n",
        "            'states':[],\n",
        "            'actions':[],\n",
        "            'rewards':[],\n",
        "            'rewards_to_go':[],\n",
        "            'log_probs':[],\n",
        "        }\n",
        "        stato = self.env.reset()\n",
        "        done = False\n",
        "        #Abbiamo un fisso di 8 episodi per batch con 2048 steps per episodio\n",
        "        for i in range(self.episodesPerBatch):\n",
        "            rewardPerEpisode=[]\n",
        "            print(\"episode: \",i)\n",
        "            for j in range(self.stepsPerEpisode):\n",
        "                batch['states'].append(stato)\n",
        "                azione,log_prob=self.getAction(stato)\n",
        "                #azione sarà un int, mentre log_prob sarà il logaritmo della probabilità dell'azione\n",
        "                batch['actions'].append(azione)\n",
        "                batch['log_probs'].append(log_prob)\n",
        "                stato, reward, done, info = self.env.step(azione)\n",
        "                #info non usata.\n",
        "                rewardPerEpisode.append(reward)\n",
        "                #if done:\n",
        "                #    break #Ha raggiunto il termine dell'episodio.\n",
        "            batch['rewards'].append(rewardPerEpisode)\n",
        "        #Calcoliamo i rewards to go --> PASSO 4\n",
        "        batch['rewards_to_go']=self.calcRTG(batch['rewards'])\n",
        "        #return batch states, actions, rewards, rewards to go, log_probs\n",
        "\n",
        "        batch_statiTensor=tf.convert_to_tensor(batch['states'],dtype=tf.uint8)\n",
        "        batch_azioniTensor=tf.convert_to_tensor(batch['actions'],dtype=tf.int32)\n",
        "        batch_rewardsTensor=tf.convert_to_tensor(batch['rewards'],dtype=tf.float32)\n",
        "        batch_rewards_to_goTensor=tf.convert_to_tensor(batch['rewards_to_go'],dtype=tf.float32)\n",
        "        batch_log_probsTensor=tf.convert_to_tensor(batch['log_probs'],dtype=tf.float32)\n",
        "\n",
        "\n",
        "        return batch_statiTensor, batch_azioniTensor,batch_rewardsTensor,batch_rewards_to_goTensor,batch_log_probsTensor\n",
        "\n",
        "    def getAction(self,stato):\n",
        "        stato= np.expand_dims(stato, axis=0)  # Diventa (1, 64, 64, 3)\n",
        "        stato=tf.convert_to_tensor(stato,dtype=tf.float32)\n",
        "        azione_pred=self.policyNN(stato)\n",
        "        #print last column values softmax\n",
        "\n",
        "        #dist=tfp.distributions.Categorical(probs=azione_pred)\n",
        "        # azionePresa=dist.sample()\n",
        "        azionePresa=random.choices(self.listaAzioni, weights=tf.squeeze(azione_pred), k=1)[0]\n",
        "        #dist=tfp.distributions.MultivariateNormalTriL(loc=azione_prob, scale_tril=tf.linalg.cholesky(self.cov_mat))\n",
        "        #azionePresa=dist.sample()\n",
        "        #log_prob=dist.log_prob(azionePresa)\n",
        "\n",
        "        log_prob=tf.math.log(azione_pred[0][azionePresa]+ 1e-10) #Aggiungo un 1e-10 per evitare problemi nel calcolo del gradiente\n",
        "        return tf.squeeze(azionePresa), log_prob\n",
        "\n",
        "    def calcRTG(self,rewards):\n",
        "        print(\"CALC REWARDS TO GO\")\n",
        "        print(rewards)\n",
        "        #Prendo la formula per calcolare i rewards to go e richiede i cumulative rewards e un fattore di sconto.\n",
        "        rtg=[]\n",
        "        for episode_reward in reversed(rewards):\n",
        "            cumulative_reward=0\n",
        "            for single_reward in reversed(episode_reward):\n",
        "                cumulative_reward=single_reward+cumulative_reward*self.gamma\n",
        "                rtg.append(cumulative_reward)\n",
        "        return tf.convert_to_tensor(rtg,dtype=tf.float32)\n",
        "\n",
        "    def calcAdvantages(self, rtg,values):\n",
        "        advantages=rtg-tf.stop_gradient(values)\n",
        "        return (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-10)\n",
        "\n",
        "    def calcSurrogatedLoss(self,log_probs_old, log_probs_new, advantages):\n",
        "        advantages = tf.stop_gradient(advantages)\n",
        "        policy_ratio = tf.exp(log_probs_old - log_probs_new)\n",
        "        surrogated_loss_1 = policy_ratio * advantages\n",
        "        surrogated_loss_2 = tf.clip_by_value(policy_ratio, clip_value_min=1.0-self.epsilon, clip_value_max=1.0+self.epsilon) * advantages\n",
        "        return surrogated_loss_1, surrogated_loss_2\n",
        "\n",
        "    def evaluate(self, batch_states,batch_actions):\n",
        "        batch_states=tf.cast(batch_states, tf.float32)\n",
        "        retVal=self.valueNN(batch_states)\n",
        "        V= tf.squeeze(retVal)\n",
        "        mean=self.policyNN(batch_states)\n",
        "        dist=tfp.distributions.Categorical(probs=mean)\n",
        "        log_probs=dist.log_prob(batch_actions)\n",
        "        return V, log_probs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DUvSNGxTYaKK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MRQw7a59YaKK",
        "outputId": "8baf7386-6db9-45d8-ff44-94880d356a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N STATI ENV: (64, 64, 3)\n",
            "N AZIONI ENV: 15\n",
            "episode:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode:  1\n",
            "episode:  2\n",
            "episode:  3\n",
            "CALC REWARDS TO GO\n",
            "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "Trajectories collected\n",
            "log_probs is tensor: True\n",
            "advantage is tensor: True\n",
            "rewards_to_go is tensor: True\n",
            "V is tensor: True\n",
            "Policy Loss: tf.Tensor(1.0300428e-06, shape=(), dtype=float32)\n",
            "Value Loss: tf.Tensor(381.03235, shape=(), dtype=float32)\n",
            "Gradients for policy: [<tf.Tensor: shape=(3, 3, 3, 32), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "          nan, nan, nan, nan, nan, nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan], dtype=float32)>, <tf.Tensor: shape=(3, 3, 32, 64), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
            "array([-inf, -inf, -inf, -inf,   0., -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "       -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "       -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "       -inf,   0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "       -inf, -inf, -inf, -inf, -inf,   0., -inf, -inf, -inf, -inf, -inf,\n",
            "         0., -inf, -inf, -inf, -inf,   0., -inf, -inf, -inf],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(3, 3, 64, 128), dtype=float32, numpy=\n",
            "array([[[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "       [[[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]],\n",
            "\n",
            "        [[nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         ...,\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan],\n",
            "         [nan, nan, nan, ..., nan, nan, nan]]]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(4608, 128), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(128, 15), dtype=float32, numpy=\n",
            "array([[nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       ...,\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan],\n",
            "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
            "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "       nan, nan], dtype=float32)>]\n",
            "Gradients for value: [None, None, None, None, None, None, None, None, None, None]\n",
            "Value gradients are None!\n",
            "EPOCA: 0  POLICY LOSS: tf.Tensor(1.0300428e-06, shape=(), dtype=float32)  VALUE LOSS: tf.Tensor(381.03235, shape=(), dtype=float32)\n",
            "Warning: early reset ignored\n",
            "episode:  0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Total of weights must be finite",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a63c96248677>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'procgen:procgen-coinrun-v0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdistribution_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'easy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_levels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mppo_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mppo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-7183e8b8cc9d>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#passo 2 ciclare per k iterazioni.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnEpoche\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_to_go\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trajectories collected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatest_log_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7183e8b8cc9d>\u001b[0m in \u001b[0;36mcollect_trajectories\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepsPerEpisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'states'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstato\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mazione\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstato\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[0;31m#azione sarà un int, mentre log_prob sarà il logaritmo della probabilità dell'azione\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mazione\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7183e8b8cc9d>\u001b[0m in \u001b[0;36mgetAction\u001b[0;34m(self, stato)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m#dist=tfp.distributions.Categorical(probs=azione_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# azionePresa=dist.sample()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mazionePresa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistaAzioni\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mazione_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m#dist=tfp.distributions.MultivariateNormalTriL(loc=azione_prob, scale_tril=tf.linalg.cholesky(self.cov_mat))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m#azionePresa=dist.sample()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/random.py\u001b[0m in \u001b[0;36mchoices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total of weights must be greater than zero'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_isfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total of weights must be finite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0mbisect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bisect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Total of weights must be finite"
          ]
        }
      ],
      "source": [
        "# Configurazione ed esecuzione\n",
        "env = gym.make('procgen:procgen-coinrun-v0',distribution_mode='easy', start_level=0, num_levels=1)\n",
        "ppo_model=PPO()\n",
        "ppo_model.learn(env)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}